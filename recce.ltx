% Copyright 2013 Jeffrey Kegler
% This document is licensed under
% a Creative Commons Attribution-NoDerivs 3.0 United States License.
\documentclass[12pt]{amsart}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{url}
\usepackage{varioref}
\usepackage{hyperref}
\allowdisplaybreaks[3]

% Reference meta tags ("classifying prefixes") from fancyref:
% Note: I follow fancyref
% Chapter chap:
% Section sec:
% Equation eq:
% Figure fig:
% Table tab:
% Enumeration enum:
% Footnote fn:
% My classifying prefixes
% Definition def:
% Theorem th:
% Lemma lem:
% Observation obs:

\delimitershortfall=0pt
\delimiterfactor=1100

% Shorten the distane between equations.  This is not
% to save space for its own sake, but because
% we often equations into series, and these
% do not look good with too much space.
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
\setlength\abovedisplayshortskip{3pt}
\setlength\belowdisplayshortskip{3pt}

\newcommand{\TODO}[1]{\par{\bf TODO}: #1\par}
\newcommand{\mymathop}[1]{\mathop{\texttt{#1}}}

% For a type name, when it occurs in text
\newcommand{\type}[1]{\ensuremath{\var{#1}}}

\newcommand{\defined}{\underset{\text{def}}{\equiv}}
\newcommand{\dfn}[1]{{\bf #1}}
\newcommand{\sep}{\,\mid\,}
\newcommand{\mydot}{\raisebox{.05em}{$\,\bullet\,$}}
\newcommand{\cat}{\,.\,}
\newcommand{\size}[1]{\ensuremath{\left | {#1} \right |}}
\newcommand{\bigsize}[1]{\ensuremath{\bigl| {#1} \bigr|}}
\newcommand{\order}[1]{\ensuremath{{\mathcal O}(#1)}}
\newcommand{\overlap}{\mathrlap{\raisebox{.5ex}{$\vee$}}\cap}
\newcommand{\Oc}{\order{1}}
\newcommand{\On}{\order{\var{n}}}
\newcommand{\lastix}[1]{\ensuremath{\##1}}
\newcommand{\Vlastix}[1]{\lastix{\var{#1}}}

\newcommand{\colonpageref}[1]{%
\vrefpagenum{\temprefpage}{#1}%
 \ifthenelse{\equal{\temprefpage}{\thepage}}%
  {}{:\pageref{#1}}%
}

\newcommand{\Aref}[1]{(a\ref{#1}\colonpageref{#1})}
\newcommand{\Eref}[1]{(e\ref{#1}\colonpageref{#1})}
\newcommand{\Lref}[1]{(L\ref{#1}:\pageref{#1})} % always include page
\newcommand{\Sref}[1]{(s\ref{#1}:\pageref{#1})} % always include page
\newcommand{\Tref}[1]{(t\ref{#1}\colonpageref{#1})}
\newcommand{\Dfref}[1]{(\textrm{Df}\ref{#1}\colonpageref{#1})}
\newcommand{\longDfref}[2]{Def of ``#1'' \Dfref{#2}}
\newcommand{\Lmref}[1]{(\textrm{Lm}\ref{#1}\colonpageref{#1})}
\newcommand{\Obref}[1]{(\textrm{Ob}\ref{#1}\colonpageref{#1})}
\newcommand{\Thref}[1]{(\textrm{Th}\ref{#1}\colonpageref{#1})}

% I use hyphens in variable names,
% so I need to ensure that subtraction is
% clearly distinguished by the typography
\newcommand{\subtract}{\,-\,}

\newcommand{\var}[1]{\ensuremath{\texttt{#1}}}
\newcommand{\varprime}[2]{\ensuremath{\texttt{#1}#2}}
\newcommand{\boldvar}[1]{\ensuremath{\textbf{#1}}}

\newcommand{\cfg}{CFG}

\newcommand{\de}{\mathrel{::=}}
\newcommand{\derives}{\mapsto}
\newcommand{\nderives}[1]{
\mathrel{%
  \ifthenelse{\equal{#1}{}}{%
    {\not\mapsto}%
  }{%
    {\mbox{$\:\stackrel{\!{#1}}{\not\mapsto\!}\:$}}%
  }%
}}%
\newcommand{\xderives}[1]
    {\mathrel{\mbox{$\:\stackrel{\!{#1}}{\mapsto\!}\:$}}}
\newcommand{\destar}
    {\mathrel{\mbox{$\:\stackrel{\!{\ast}}{\mapsto\!}\:$}}}
\newcommand{\ndestar}{\nderives{\ast}}
\newcommand{\deplus}
    {\mathrel{\mbox{$\:\stackrel{\!{+}}{\mapsto\!}\:$}}}
\newcommand{\ndeplus}{\nderives{+}}
\newcommand{\derivg}[1]{\mathrel{\mbox{$\:\mapsto\:$}}}
\newcommand{\derivrg}[2]{\mathrel{\mbox{$\:\stackrel{\!{#1}}%
        {\mapsto\!}\:$}}}

\newcommand{\set}[1]{{\left\lbrace #1 \right\rbrace} }
\newcommand{\frontier}[1]{{\left\lfloor #1 \right\rfloor} }
\newcommand{\Vfrontier}[1]{\frontier{\boldvar{#1}}}
\newcommand{\boldRange}[3]{\textbf{#1[#2 \ldots #3]}}
\newcommand{\boldRangeDecr}[3]{\textbf{#1[#2 \ldots %
\textbf{($\textbf{#3}\subtract 1$)}]}%
}
\newcommand{\boldRangeSizeDecr}[3]{\textbf{#1[#2 \ldots %
\textbf{($\left|\textbf{#3}\right|\subtract 1$)}%
]} }

\newcommand{\wRange}[2]{\boldRange{w}{#1}{#2}}
\newcommand{\Tvar}[2]{\ensuremath{\var{#1}_{\var{#2}}}}
\newcommand{\Tvarset}[2]{\ensuremath{\var{#1}_{\set{\var{#2}}}}}
\newcommand{\bigset}[1]{{\bigl\lbrace #1 \bigr\rbrace} }
\newcommand{\Bigset}[1]{{\Bigl\lbrace #1 \Bigr\rbrace} }
\newcommand{\element}[2]{\ensuremath{#1\left[#2\right]}}
\newcommand{\Velement}[2]{\element{\var{#1}}{#2}}
\newcommand{\VVelement}[2]{\Velement{#1}{\var{#2}}}

\newcommand{\eim}[1]{\ensuremath{#1_\type{EIM}}}
\newcommand{\es}[1]{\ensuremath{#1_\type{ES}}}
\newcommand{\mylim}[1]{\ensuremath{#1_\type{LIM}}}
\newcommand{\str}[1]{\ensuremath{\langle\langle{#1}\rangle\rangle}}
\newcommand{\term}[1]{\ensuremath{\lfloor{#1}\rfloor}}
\newcommand{\sym}[1]{\ensuremath{\langle{#1}\rangle}}
\newcommand{\Vbool}[1]{\Tvar{#1}{BOOL}}
\newcommand{\Vcfg}[1]{\Tvar{#1}{CFG}}
\newcommand{\dr}[1]{\ensuremath{#1_\type{DR}}}
\newcommand{\Vdr}[1]{\dr{\var{#1}}}
\newcommand{\Vdrset}[1]{\Tvarset{#1}{DR}}
\newcommand{\Veim}[1]{\eim{\var{#1}}}
\newcommand{\Veimset}[1]{\Tvarset{#1}{EIM}}
\newcommand{\ves}[1]{\ensuremath{#1_\type{VES}}}
\newcommand{\Ves}[1]{\es{\var{#1}}}
\newcommand{\Vesset}[1]{\Tvarset{#1}{ES}}
\newcommand{\Vext}[1]{\Tvar{#1}{EXT}}
\newcommand{\Vint}[1]{\Tvar{#1}{INT}}
\newcommand{\Vlim}[1]{\mylim{\var{#1}}}
\newcommand{\Vlimset}[1]{\Tvarset{#1}{LIM}}
\newcommand{\Vloc}[1]{\Tvar{#1}{LOC}}
\newcommand{\Vorig}[1]{\Tvar{#1}{ORIG}}
\newcommand{\Vpim}[1]{\Tvar{#1}{PIM}}
\newcommand{\Vpimset}[1]{\Tvarset{#1}{PIM}}
\newcommand{\Vrule}[1]{\Tvar{#1}{RULE}}
\newcommand{\Vruleset}[1]{\Tvarset{#1}{RULE}}
\newcommand{\Vstr}[1]{\str{\var{#1}}}
\newcommand{\Vstrset}[1]{\Tvarset{#1}{STR}}
\newcommand{\Vterm}[1]{\term{\boldvar{#1}}}
\newcommand{\Vtermset}[1]{\Tvarset{\boldvar{#1}}{TERM}}
\newcommand{\Vsym}[1]{\sym{\var{#1}}}
\newcommand{\Vsymset}[1]{\Tvarset{#1}{SYM}}
\newcommand{\Vtoken}[1]{\Tvar{#1}{TOKEN}}
\newcommand{\Vves}[1]{\ves{\var{#1}}}
\newcommand{\Vsize}[1]{\ensuremath{\size{\var{#1}}}}

\newcommand{\alg}[1]{\ensuremath{\mathtt{#1}}}
\newcommand{\Earley}{\alg{Earley}}
\newcommand{\Leo}{\alg{Leo}}
\newcommand{\Marpa}{\alg{Marpa}}

\newcommand{\Cg}{\var{g}}
\newcommand{\Cw}{\var{w}}
\newcommand{\CVw}[1]{\ensuremath{\Vsym{\Cw[\var{#1}]}}}
\newcommand{\Accept}[1]{\ensuremath{\mymathop{Accept}(#1)}}
\newcommand{\Cause}[1]{\ensuremath{\mymathop{Cause}(#1)}}
\newcommand{\Current}[1]{\ensuremath{\mymathop{Current}\left(#1\right)}}
\newcommand{\DotPos}[1]{\ensuremath{\mymathop{DotPos}(#1)}}
\newcommand{\DottedRules}[1]{\ensuremath{\mymathop{Dotted-Rules}(#1)}}
\newcommand{\DR}[1]{\ensuremath{\mymathop{DR}(#1)}}
\newcommand{\Etable}[1]{\ensuremath{\mymathop{table}\left[#1\right]}}
\newcommand{\GOTO}{\ensuremath{\mymathop{GOTO}}}
\newcommand{\ID}[1]{\ensuremath{\mymathop{ID}(#1)}}
\newcommand{\LeoEligible}[1]{\ensuremath{\mymathop{Leo-Eligible}(#1)}}
\newcommand{\LeoUnique}[1]{\ensuremath{\mymathop{Leo-Unique}(#1)}}
\newcommand{\LHS}[1]{\ensuremath{\mymathop{LHS}(#1)}}
\newcommand{\LIMPredecessor}[1]{\ensuremath{\mymathop{LIM-Predecessor}(#1)}}
\newcommand{\LinkPairs}[1]{\ensuremath{\mymathop{Link-Pairs}\left(#1\right)}}
\newcommand{\myL}[1]{\ensuremath{\mymathop{L}(#1)}}
\newcommand{\Next}[1]{\ensuremath{\mymathop{Next}(#1)}}
\newcommand{\NT}[1]{\ensuremath{\mymathop{NT}(#1)}}
\newcommand{\Origin}[1]{\ensuremath{\mymathop{Origin}(#1)}}
\newcommand{\Penult}[1]{\ensuremath{\mymathop{Penult}(#1)}}
\newcommand{\Predot}[1]{\ensuremath{\mymathop{Predot}\left(#1\right)}}
\newcommand{\Postdot}[1]{\ensuremath{\mymathop{Postdot}\left(#1\right)}}
\newcommand{\Predecessor}[1]{\ensuremath{\mymathop{Predecessor}\left(#1\right)}}
\newcommand{\Predict}[1]{\ensuremath{\mymathop{Predict}(#1)}}
\newcommand{\PreviousLink}[1]{\ensuremath{\mymathop{Previous-Link}(#1)}}
\newcommand{\PSL}[2]{\ensuremath{\mymathop{PSL}[#1][#2]}}
\newcommand{\RHS}[1]{\ensuremath{\mymathop{RHS}\left(#1\right)}}
\newcommand{\Rightmost}[1]{\ensuremath{\mymathop{Rightmost}\left(#1\right)}}
\newcommand{\RightRecursive}[1]{\ensuremath{\mymathop{Right-Recursive}\left(#1\right)}}
\newcommand{\Rtable}[1]{\ensuremath{\mymathop{table}[#1]}}
\newcommand{\Rtablesize}[1]{\ensuremath{\left| \mymathop{table}[#1] \right|}}
\newcommand{\Rule}[1]{\ensuremath{\mymathop{Rule}(#1)}}
\newcommand{\Rules}[1]{\ensuremath{\mymathop{Rules}(#1)}}
\newcommand{\Source}[1]{\ensuremath{\mymathop{Source}(#1)}}
\newcommand{\Term}[1]{\ensuremath{\mymathop{Term}(#1)}}
\newcommand{\Transition}[1]{\ensuremath{\mymathop{Transition}\left(#1\right)}}
\newcommand{\Vocab}[1]{\ensuremath{\mymathop{Vocab}(#1)}}
\newcommand{\Vtable}[1]{\Etable{\var{#1}}}

% TODO: Delete these?
% \newcommand\entail[2]{\ensuremath{{\mathcal{#1}}\models\left(#2\right)}}
% \newcommand\Mentail[1]{\entail{M}{#1}}
% \newcommand\Eentail[1]{\entail{E}{#1}}
% \newcommand\Lentail[1]{\entail{L}{#1}}
% \newcommand\gentail[1]{\ensuremath{\var{g}\vdash\left(#1\right)}}

% I want to use 'call' outside of pseudocode
\newcommand\call[2]{\textproc{#1}\ifthenelse{\equal{#2}{}}{}{(#2)}}%

% I use parboxes in equations.  This sets a useful width for them.
\newlength{\mathparwidth}
\newlength{\longtagwidth}
\settowidth{\longtagwidth}{(9999)\quad}
\setlength{\mathparwidth}{\dimexpr\textwidth-\longtagwidth}
\newcommand{\myparbox}[2][\mathparwidth]{%
  \parbox[t]{#1}{%
  %\raggedright#2\par
  \centering#2\par
  \vspace{-\prevdepth} % remove the depth of the last line
  \vspace{1ex} % add a fixed vertical space
  }%
}

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{observation}[theorem]{Observation}

\hyphenation{oper-and oper-ands}
\hyphenation{look-ahead}
\hyphenation{memo-ization}

\begin{document}

\date{\today}

\title{Marpa, a practical general parser: the recognizer}

\author{Jeffrey Kegler}
\thanks{%
Copyright \copyright\ 2019 Jeffrey Kegler.
}
\thanks{%
This document is licensed under
a Creative Commons Attribution-NoDerivs 3.0 United States License.
}

\begin{abstract}
The \Marpa{} recognizer is described.
\Marpa{} is
a fully implemented
algorithm for the recognition,
parsing and evaluation of context-free grammars.
\Marpa{} is an Earley parser
which includes the first
practical implementation
of the improvements
in Joop Leo's 1991 paper.
In addition,
\Marpa{} tracks the full state of the parse,
at run-time,
in a form accessible by the application.
This greatly improves error detection;
enables event-driven parsing;
and allows techniques such as
``Ruby Slippers'' parsing,
in which
the input is altered in response
to the parser's expectations.
\end{abstract}

\maketitle

\section{Introduction}

The \Marpa{} project was intended to create
a practical and highly available tool
to generate and use general context-free
parsers.
Tools of this kind
had long existed
for LALR~\cite{Johnson} and
regular expressions.
But, despite an encouraging academic literature,
no such tool had existed for context-free parsing.

The first stable version of \Marpa{} was uploaded to
a public archive on Solstice Day 2011.
This revision of this paper
describes the algorithm used
in the most recent version of \Marpa{},
Marpa::R2~\cite{Marpa-R2}.
It is a simplification of the algorithm presented
in the first version of this paper.

While the presentation in this paper is theoretical,
the approach is practical.
The Marpa::R2 implementation has been widely available
and seen considerable use.

In this paper,
internal references will be of the form
({\it Xn:p}),
where {\it X} is the reference type,
{\it n} is the number of the referent,
and
{\it p} is a page reference.
Reference types are
``a'' (algorithm),
``e'' (equation),
``s'' (section)
``L'' (a location within a section,
where {\it n} is the section number),
``t'' (table),
``Df'' (definition),
``Lm'' (lemma),
``Ob'' (observation), and
``Th'' (theorem).
For example
\Eref{eq:restriction1} refers to equation
\ref{eq:restriction1} on page \pageref{eq:restriction1};
\Sref{sec:features} refers to
section \ref{sec:features}
on page \pageref{sec:features};
and
\Lref{loc-brick-depth} refers
to page \pageref{loc-brick-depth}
inside section \ref{loc-brick-depth}.
Where the page reference would be to the current page,
it is omitted.

Definitions occur throughout.
Verbal defienda are in boldface.
For emphasis,
or for convenient reference,
definitions may be
in numbered sections.
The end of a definitions section is indicated with a black star ($\bigstar$).
Often one or more shorter verbal phrases are defined as a synonyms of a longer one.
For example, \Dfref{def:parse} states ``full parse'' and ``parse'' mean
the same thing.
Shorter synonyms are intended to be used for brevity,
and only in cases where they are not expected to cause confusion.

% TODO Revise once chapters are settled
\Sref{sec:features}
describes the important features of \Marpa{}
and
\Sref{sec:using-features}
goes into detail about their application.
\Sref{sec:preliminaries} describes the notation and conventions
of this paper.
\Sref{sec:CFGs} defines context-free grammars.
\Sref{sec:EXTs} introduces \Marpa{}'s external grammars ---
the grammars which the users see.
\Sref{sec:improper} describes \Marpa{}'s
implementation of improper context free grammars.
\Sref{sec:rewrite} explain how \Marpa{} rewrites its external
grammars into internal grammars, and
\Sref{sec:INTs} describes \Marpa{}'s internal grammars.
\Sref{sec:earley-table} describes the data structures used
in parsing \Marpa{} internal grammars.
\Sref{sec:leo} describes Leo's modification
to Earley's algorithm.
\Sref{sec:algorithm}
describes the \Marpa{} algorithm.
\Sref{sec:per-set-lists} describes \Marpa{}'s
per-set lists.
while \Sref{sec:complexity} contains
its complexity results.
Finally,
section \Sref{sec:input}
generalizes \Marpa{}'s input model.

We assume familiarity with the theory of parsing,
as well as Earley's algorithm.
\cite{Timeline}
contains a full description of \Marpa{}'s relationship to
prior work.
Readers may find it helpful to read
\cite{Timeline}
before this paper.

\section{Features}
\label{sec:features}

\subsection{General context-free parsing}
The \Marpa{} algorithm is capable of parsing all
context-free grammars.
As implemented,
Marpa parses
all cycle-free context-free
grammars.\footnote{
Earlier implementations of \Marpa{} allowed cycles,
but support for them
was removed because
practical interest seems to be non-existent.
For more detail, see section
\ref{sec:improper}.
}
Worst case time bounds are never worse than
those of Earley~\cite{Earley1970},
and therefore never worse than $\order{\var{n}^3}$.

\subsection{Linear time for practical grammars}
Currently, the grammars suitable for practical
use are thought to be a subset
of the deterministic context-free grammars (DCFG's).
\Marpa{} uses a technique discovered by
Leo~\cite{Leo1991}
to run in
\On{} time for LR-regular (LRR) grammars,
a superset of the the DCFG's.
\Marpa{}
also parses many ambiguous grammars in linear
time.

\subsection{Left-eidetic}
The original \Earley{} algorithm kept full information
about the parse ---
including partial and fully
recognized rule instances ---
in its tables.
At every parse location,
before any symbols
are scanned,
\Marpa{}'s parse engine makes available
its
information about the state of the parse so far.
This information is
in useful form,
and can be accessed efficiently during the parse.

\subsection{Recoverable from read errors}
When
\Marpa{} reads a token which it cannot accept,
the error is fully recoverable.
An application can attempt to read another
token,
repeatedly if that is desirable.
Once the application provides
a token that is accepted by the parser,
parsing will continue
as if the unsuccessful read attempts had never been made.

\subsection{Ambiguous tokens}
\Marpa{} allows ambiguous tokens.
These are often useful in natural language processing
where, for example,
the same word might be a verb or a noun.
Use of ambiguous tokens can be combined with
recovery from rejected tokens so that,
for example, an application could react to the
rejection of a token by reading two others.

\section{Using the features}
\label{sec:using-features}

\subsection{Error reporting}
An obvious application of left-eideticism is error
reporting.
\Marpa{}'s abilities in this respect are
ground-breaking.
For example,
users typically regard an ambiguity as an error
in the grammar.
\Marpa{}, as currently implemented,
can detect an ambiguity and report
specifically where it occurred
and what the alternatives were.

\subsection{Event driven parsing}
As implemented,
Marpa::R2\cite{Marpa-R2}
allows the user to define ``events''.
Events can be defined that trigger when a specified rule is complete,
when a specified rule is predicted,
when a specified symbol is nulled,
when a user-specified lexeme has been scanned,
or when a user-specified lexeme is about to be scanned.
A mid-rule event can be defined by adding a nulling symbol
at the desired point in the rule,
and defining an event which triggers when the symbol is nulled.

\subsection{Ruby slippers parsing}
Left-eideticism, efficient error recovery,
and the event mechanism can be combined to allow
the application to change the input in response to
feedback from the parser.
In traditional parser practice,
error detection is an act of desperation.
In contrast,
\Marpa{}'s error detection is so painless
that it can be used as the foundation
of new parsing techniques.

For example,
if a token is rejected,
the lexer is free to create a new token
in the light of the parser's expectations.
This approach can be seen
as making the parser's
``wishes'' come true,
and we have called it
``Ruby Slippers Parsing''.

One use of the Ruby Slippers technique is to
parse with a clean
but oversimplified grammar,
programming the lexical analyzer to make up for the grammar's
short-comings on the fly.
As part of Marpa::R2~\cite{Marpa-R2},
the author\footnote{
We refer to the author both as ``the author''
and as ``we'', despite this paper having a single author.
The use of ``we''
follows the tradition in which a mathematician
speaks as a representative of a community of inquiry,
one which includes the reader.
The author refers to himself as such in those cases
where the reference only makes sense
as being to a specific individual.
}
has implemented an HTML parser,
based on a grammar that assumes that all start
and end tags are present.
Such an HTML grammar is too simple even to describe perfectly
standard-conformant HTML,
but the lexical analyzer is
programmed to supply start and end tags as requested by the parser.
The result is a simple and cleanly designed parser
that parses very liberal HTML
and accepts all input files,
in the worst case
treating them as highly defective HTML.

\subsection{Ambiguity as a language design technique}
In current practice, ambiguity is avoided in language design.
This is very different from the practice in the languages humans choose
when communicating with each other.
For example,
the language of this paper, English, is notoriously
ambiguous.
Human languages exploit ambiguity in order to allow expression
to be more flexible and powerful.

Ambiguity of course can present a problem.
A sentence in an ambiguous
language may have undesired meanings.
But note that this is not a reason to ban potential ambiguity ---
it is only a problem with actual ambiguity.

Consider, for comparison purposes,
the standard treatment of syntax errors.
Syntax errors are undesired, but nobody tries
to design languages that make syntax errors impossible.
A language in which every input was well-formed and meaningful
would be not just cumbersome, but dangerous.
A parser would never warn the user about typos,
because all typos in such a language would have unintended
meanings.

With \Marpa{}, ambiguity can be dealt with in the same way
that syntax errors are dealt with in current practice.
A \Marpa{}-powered
language can be designed to allow ambiguity,
and any actual ambiguity will be detected
and reported at parse time.
This exploits \Marpa{}'s ability
to report exactly where
and what the ambiguity is.
Marpa::R2's own parser description language, the SLIF,
uses ambiguity in this way.

\subsection{Auto-generated languages}
\cite[pp. 6-7]{Culik1973} points out that the ability
to efficiently parse LRR languages
opens the way to auto-generated languages.
In particular,
\cite{Culik1973} notes that a parser which
can parse any LRR language will be
able to parse a language generated using syntax macros.

\subsection{Second order languages}
In the literature, the term ``second order language''
is usually used to describe languages with features
which are useful for second-order programming.
True second-order languages --- languages which
are auto-generated
from other languages ---
have not been seen as practical,
since there was no guarantee that the auto-generated
language could be efficiently parsed.

With \Marpa{}, this barrier is raised.
As an example,
Marpa::R2's own parser description language, the SLIF,
allows ``precedenced rules''.
Precedenced rules are specified in an extended BNF.
The BNF extensions allow precedence and associativity
to be specified for each RHS.

Marpa::R2's precedenced rules are implemented as
a true second order language.
The SLIF representation of the precedenced rule
is parsed to auto-generate a pure BNF grammar which is equivalent,
and which has the desired precedence and associativity.

In creating the auto-generated BNF grammar,
the SLIF does a standard parsing textbook transformation,
translating explicitly specified precedence and associativity
into pure BNF which implicitly implements
that precedence and associativity
Since the SLIF is powered by \Marpa{}, which is
linear for LRR,
the SLIF can easily ensure that the grammar
that it auto-generates will
be parsed in linear time.

Notationally, \Marpa{}'s precedenced rules
are an improvement over
similar features
in LALR-based parser generators like
yacc or bison.
In the SLIF,
there are two important differences.
First, in the SLIF's precedenced rules,
precedence is generalized, so that it does
not depend on the operators:
there is no need to identify operators,
much less class them as binary, unary, etc.
This more powerful and flexible precedence notation
allows the definition of multiple ternary operators,
and multiple operators with arity greater than three.

Second, and more important, a SLIF user is guaranteed
to get exactly the language that the precedenced rule specifies.
The user of the yacc equivalent must hope their
syntax falls within the limits of LALR.

\section{Preliminaries}
\label{sec:preliminaries}

This paper will
use subscripts to indicate commonly occurring types,
as shown in
\Tref{tab:type-notation}
\begin{table}[tb]
\centering
\caption{Type notations}
\begin{tabular}{ll}
\hline
$\var{X}_\var{T}$ & The variable \var{X} of type \type{T} \\
$\var{agg-one}_\set{\var{T}}$ & The variable \var{agg-one} of type ``aggregate of \type{T}'' \\
\type{SYM} & The type for a symbol \\
\type{STR} & The type for a string \\
\type{EIM} & The type for an Earley item \\
\type{RULE} & The type for the rule of a grammar\\
\Tvar{a}{SYM} & A variable \var{a} of type \type{SYM}, subscripted form \\
\Vsym{a} & A variable \var{a} of type \type{SYM}, angle bracket form \\
\Tvar{a}{STR} & A variable \var{a} of type \type{STR}, subscripted form \\
\Vstr{a} & A variable \var{a} of type \type{STR}, angle bracket form \\
\Veim{a} & A variable \var{a} of type \type{EIM} \\
\Vrule{a} & A variable \var{a} of type \type{RULE} \\
\Vsymset{agg-two} & The variable \var{agg-two}, an aggreggate  of strings \\
\Vstrset{strings} & The variable \var{strings}, an aggreggate of strings \\
\Veimset{agg-three} & The variable \var{agg-three}, an aggregate of Earley items \\
\Vruleset{agg-four} & The variable \var{agg-four}, an aggregate of grammar rules \\
\hline
\end{tabular}
\label{tab:type-notation}
\end{table}

The type system of this paper is opportunistic,
and intended to aid the reader.
It is emphatically not intended as a contribution to
the theory of types.

An ``aggregate'' may be a set, a multiset, an indexable sequence, etc.
Its precise semantics will be specified
when it is defined.
For all aggregates,
the notation $\var{element} \in \var{aggregate}$
means "\var{element} is contained in \var{aggregate}".

Subscripts may be omitted when the type
is obvious from the context.
Symbols and strings
have an alternate type notation,
which uses angle brackets.
Type names are often used in the text
as a convenient way to refer to
their type.

Aggregates

Multi-character variable names will be common.
When the angle bracket notation is used,
concatenation of strings and symbols may be implicit.
Otherwise, operations are never implicit
\Tref{tab:op-notation}.
\begin{table}[tb]
\centering
\caption{Selected operator notations}
\begin{tabular}{ll}
\hline
Multiplication &  $\var{a} \times \var{b}$ \\
Concatenation & $\var{a} \cat \var{b}$ \\
Concatenation & $\Vstr{a} \Vsym{b}$ \\
Subtraction & $\var{symbol-count} \subtract \var{terminal-count}$ \\
\hline
\end{tabular}
\label{tab:op-notation}.
\end{table}

Where \var{set} is a set,
\Vsize{set} is the size of the set.
Where \var{seq} is a sequence,
the first element of \var{seq} is \Velement{seq}{0},
and the last element is \Velement{seq}{\Vlastix{seq}}.
Note that this implies that $\Vlastix{seq} = \Vsize{seq}\subtract 1$.
\begin{equation*}
\VVelement{seq}{i} =
\begin{cases}
\text{the \var{i}'th element of \var{seq},
if $0 \le \var{i} \le \Vlastix{seq}$,} \\
\text{$\Lambda$, if $\var{i} < 0$,} \\
\text{$\Lambda$, if $\var{i} > \Vlastix{seq}$.}
\end{cases}
\end{equation*}

We will write
$\var{s}[\var{a} \ldots \var{z}]$
for the subsequence
\[
    \var{s}[\var{a}], \;\;
    \var{s}[\var{a}+1], \;\;
    \ldots \;\;
    \var{s}[\var{z}].
\]
We require of the subsequence indices that
$0 \le \var{a} \le \var{z} \le \Vlastix{seq}$,
except in one special case.
If
$\var{z} < \var{a}$ then
the values of \var{i} and \var{j} may be negative,
and $\var{s}[\var{a} \ldots \var{z}]$
is the empty sequence, which we may write as $\epsilon$:
\[
\var{z} < \var{a} \implies \var{s}[\var{a} \ldots \var{z}] = \epsilon.
\]

Let \Vsymset{alphabet} be a non-empty set of symbols.
Symbols are of type \type{SYM}.
Let \Tvar{x}{SYM} be a symbol.
More commonly we write
\Tvar{x}{SYM} as \Vsym{x}.

let $\var{alphabet}^\ast$ be the set of all strings
(type \type{STR}) formed
from those symbols.
Let \Tvar{s}{STR} be a string.
More commonly we write
\Tvar{s}{STR} as \Vstr{s}.
\size{\Vstr{s}} is the length of \Vstr{s},
counted in symbols.
Let $\var{alphabet}^+$ be
\begin{equation*}
\left\{ \Vstr{x}
\; \middle| \;
\Vstr{x} \in \var{alphabet}^\ast
\; \land \;
\Vsize{\Vstr{x}} > 0
\right\}.
\end{equation*}

A string can be seen as sequence of symbols,
and we use sequence index
to represent individual symbols of a string,
and subsequence notation
to represent substrings.
Thus,
the \var{i}'th character of the string,
is \sym{\var{str}[\var{i}]};
and
\begin{equation*}
\str{\var{str}[\var{a} \ldots \var{z}]} \defined
\str{\lbrace
    \var{str}[\var{a}], \;\;
    \var{str}[\var{a}+1], \;\;
    \ldots \;\;
    \var{str}[\var{z}]
\rbrace}.
\end{equation*}
We sometimes describe the last character of a
string as its rightmost:
\[
\Rightmost{\str{\var{str}[\var{a} \ldots \var{z}]}} = \var{z}.
\]

Typing is not strict.
For example,
we will move freely between strings, and their equivalent sequences of symbols.
When it is convenient,
we will treat strings of length 1 as symbols,
and symbols as strings of length 1.

$\Lambda$ is often used as a special ``null'' value.
When used in a comparison,
$\Lambda$ compares equal to itself
and unequal to any integer or tuple.

Explicit ``Definition'' sections and boldface are reserved
for definitions that are of significance throughout
this paper.
In some contexts,
as for example in stating and proving some lemmas,
or in proving some theorems,
it is convenient to have definitions
which are ``local'' ---
useful only within
a particular presentation.
Local definitions are given in double quotes,
(``definition''),
and in numbered equations.

Equations are sometimes stated verbally,
at least in part.
They are often followed by a justification,
especially inside proofs.
For clarity, the justification may be separated
from the body of the equation by a ``because'' symbol ($\because$).

Where an equation is a definition,
its justification will state that is a
``definition for the purpose of
presentation'',
which may be abbreviated ``DFPP''.
``WLOG'' abbreviates ``assumed without loss of generality''.
``Def'' abbreviates ``definition''.
``AF'' abbreviates ``assumed for''.
For example, ``AF theorem'' abbreviates ``assumed for the theorem''.

\section{Context-free grammars}
\label{sec:CFGs}

A \dfn{context free grammar} (CFG) is a 4-tuple,
\begin{equation}
\begin{gathered}
\text{$[\Vsymset{nt}, \Vsymset{term}, \Vruleset{rules}, \Vsym{accept}]$ such that} \\
\Vsymset{nt} \cup \Vsymset{term} = \emptyset, \\
\text{$\Vsym{accept} \in \Vsymset{nt}$, and} \\
\text{\Vruleset{rules} is a set of elements of type \type{RULE}.}
\end{gathered}
\end{equation}
We will define type \type{RULE} shortly.
Context-free grammars have the type CFG.

For convenience,
for a CFG, \Vcfg{g},
we write
\begin{equation*}
\begin{alignedat}{3}
& \NT{\Vcfg{g}} && \defined && \quad \Vsymset{nt}, \\
& \Term{\Vcfg{g}} && \defined && \quad \Vsymset{term}, \\
& \Accept{\Vcfg{g}} && \defined && \quad \Vsym{accept}, \\
& \Rules{\Vcfg{g}} && \defined && \quad \Vruleset{rules}, \quad \text{and} \\
& \Vocab{\Vcfg{g}} && \defined && \quad \Vsymset{nt} \cup \Vsymset{term}.
\end{alignedat}
\end{equation*}
The elements of the set \NT{\var{g}}, are non-terminal symbols,
or simply \dfn{non-terminals}.
The elements of the set \Term{\var{g}}, are terminal symbols,
or simply \dfn{terminals}.
\Accept{\var{g}} is the \dfn{accept symbol},
and in the parsing literature is often called
the ``start symbol''.
\Vocab{\var{g}} is
the \dfn{vocabulary} of \Vext{g}.

A \dfn{rule}
(type \type{RULE})
is a duple
\begin{equation*}
\left[ \Vsym{lhs} \de \Vstr{rhs} \right]
\end{equation*}
such that
\begin{equation*}
\Vsym{lhs} \in \NT{\var{g}} \; \land \;
\Vstr{rhs} \in \Vocab{\var{g}}^\ast.
\end{equation*}
\Vsym{lhs} is referred to as the left hand side (LHS)
of \Vrule{r}.
\Vstr{rhs} is referred to as the right hand side (RHS)
of \Vrule{r}.
The LHS and RHS of \Vrule{r} may also be
referred to as
$\LHS{\var{r}}$ and $\RHS{\var{r}}$, respectively.

The size of \var{r}, $\size{\Vrule{r}}$,
is the number of symbols on its RHS,
so that
\begin{equation*}
\size{\Vrule{r}} = \size{\RHS{\var{r}}}.
\end{equation*}
If $\size{\Vrule{r}} = 0$, then \Vrule{r}
is an \dfn{empty rule}.
If $\size{\Vrule{r}} = 1$, then \Vrule{r}
is a \dfn{unit rule}.

Where the choice of \Vcfg{g} is clear in context,
and unless otherwise specified,
use of the types \type{SYM}, \type{STR} and \type{RULE} will bind
their variables.
Specifically,
\begin{align*}
& \text{\Vsym{sym} will imply that $\Vsym{sym} \in \Vocab{\var{g}}$}, \\
& \text{$\var{sym}_\type{SYM}$ will imply that $\Vsym{sym} \in \Vocab{\var{g}}$}, \\
& \text{\Vstr{str} will imply that $\Vstr{str} \in \Vocab{\var{g}}^\ast$}, \\
& \text{$\var{str}_\type{STR}$ will imply that $\Vstr{str} \in \Vocab{\var{g}}^\ast$, and} \\
& \text{\Vterm{str} will imply that $\Vterm{str} \in \Term{\var{g}}^\ast$}, \\
& \text{\Vrule{r} will imply that $\Vrule{r} \in \Rules{\var{g}}$}.
\end{align*}

A grammar describes a rewriting system.
We consider
\begin{equation}
\label{eq:rewrite-step}
\Vstr{prefix}\Vsym{lhs}\Vstr{suffix} \derives \Vstr{prefix}\Vstr{rhs}\Vstr{suffix}
\end{equation}
a valid rewrite step of \Vcfg{g} if and only if
\begin{gather*}
\Vstr{prefix} \in \Vocab{\Vcfg{g}}, \\
\Vstr{suffix} \in \Vocab{\Vcfg{g}}, \quad \text{and} \\
[\Vsym{lhs} \de \Vstr{rhs}] \in \Rules{\Vcfg{g}}.
\end{gather*}
The valid rewrite step of
\Eref{eq:rewrite-step}
is a \dfn{right rewrite step} if
and only if \Vsym{lhs} is the rightmost non-terminal.

A sequence of iterated rewrite steps of \Vcfg{g}, such that
\begin{equation}
\label{eq:derivation}
\Vstr{s1} \derives \Vstr{s2} \derives \Vstr{s3} \ldots \Vstr{sec:n}
\end{equation}
is called a \dfn{derivation} of \Vcfg{g},
and a rewrite step that is part of a derivation is called
a \dfn{derivation step}.
A derivation is a \dfn{right derviation} if and only if
all of its rewrite steps are right rewrite steps.

% TODO: Do I need the definition of \dfn{derivation sequence}.
The sequence of strings in a derivation, its sequence of strings
is called its \dfn{derivation sequence}.
The derivation sequence of \Eref{eq:derivation} is
\begin{equation*}
\Vstr{s1}, \Vstr{s2}, \Vstr{s3} \ldots \Vstr{sec:n}.
\end{equation*}

We explicitly indicate that some \Vstr{x} derives \Vstr{y} in \var{k} steps
by writing
\begin{equation*}
\Vstr{x} \xderives{\var{k}} \Vstr{y}.
\end{equation*}
Writing $\Vstr{x} \xderives{1} \Vstr{y}$ is equivalent
to writing $\Vstr{x} \derives \Vstr{y}$.

By convention,
every string derives itself in zero steps.
\begin{equation*}
\Vstr{x} = \Vstr{y} \equiv \Vstr{x} \xderives{0} \Vstr{x}.
\end{equation*}
A derivation of zero steps is called a
\dfn{trivial derivation}.
A derivation of more than zero steps is called a
\dfn{non-trivial derivation}.

We write
\begin{equation*}
\Vstr{x} \deplus \Vstr{y}
\end{equation*}
to say that \Vstr{x} derives \Vstr{y} in one or more steps.
We write
\begin{equation*}
\Vstr{x} \destar \Vstr{y}
\end{equation*}
to say that \Vstr{x} derives \Vstr{y} in zero or more steps.
Where
\begin{equation*}
\var{d} = \Vstr{first} \destar \Vstr{last}
\end{equation*}
is a derivation and
\Vstrset{dseq} is the derivation sequence of \var{d},
then saying that
\begin{equation*}
\var{dseq}[\var{i}] = \Vstr{s}
\end{equation*}
is equivalent to saying that
\begin{equation*}
\var{d} = \Vstr{first} \xderives{\var{i}} \Vstr{s} \destar \Vstr{last}.
\end{equation*}
Note that, as a result of the above definitions,
$\epsilon \xderives{0} \epsilon$ and
$\epsilon \destar \epsilon$.

We write
\begin{equation*}
\Vstr{x} \xderives{R} \Vstr{y}.
\end{equation*}
to say that \Vstr{x} right derives \Vstr{y}.
The ``$R$'' superscript may be combined with any of the other superscripts
so that, for example,
\begin{equation*}
\Vstr{x} \xderives{R+} \Vstr{y}
\end{equation*}
states that that \Vstr{x} right derives \Vstr{y} in one or more steps;
and
\begin{equation*}
\Vstr{x} \xderives{R(\var{k})} \Vstr{y}
\end{equation*}
states that that \Vstr{x} right derives \Vstr{y} in \var{k} steps.

Let \Vcfg{g} be a CFG.
A \dfn{sentential form} is a string which can be derived from the start
symbol, \Accept{\Vcfg{g}}.
If the sentential form \Vterm{sf} is a string of terminals,
then \Vterm{sf} is a \dfn{sentence} of \Vcfg{g}.

Let $\Vstr{a} \in \Vocab{\var{g}}^\ast$.
The \dfn{language} of \var{g} and \Vstr{a},
is
\begin{equation}
\label{eq:language}
\myL{\Vcfg{g}, \Vstr{a}}
\defined
\set{
\quad
\begin{aligned}
& \Vterm{sentence} \quad \text{such that} \\
& \qquad \begin{aligned}
  & \Vstr{a} \destar \Vterm{sentence} \\
  & \land \Vterm{sentence} \in \Term{\Vcfg{g}}^\ast.
  \end{aligned}
\end{aligned}
\quad
}
\end{equation}
The \dfn{language} of the grammar \Vcfg{g} is its set
of sentential forms:
\[
\myL{\var{g}} \defined \myL{\var{g}, \Accept{\var{g}}}.
\]

When a string in double-angle-bracket form
is a string of terminals,
we will write it in boldface.
Use of boldface for a terminal \Vterm{term} is intended as a reading aid ---
it implies
$\Vterm{term} \in \Vocab{\var{g}}^\ast$,
but not that
$\Vterm{term} \in \Term{\var{g}}^\ast$.
That the string is composed entirely of terminals must be separately justified by assumption,
implication or explicit claim.

\begin{definition}[Parse]
\label{def:parse}
A \dfn{full parse} is a duple $[\Vcfg{g}, \Vterm{w}]$ such that
$\Vterm{w} \in \myL{\var{g}}$.
A \dfn{partial parse} is a duple $[\Vcfg{g}, \Vterm{prefix}]$ such that
\[
\left[\Vcfg{g}, \Vterm{prefix}\Vterm{lookahead}\right]
\]
is a parse
for some \Vterm{lookahead}.
A partial parse,
$[\Vcfg{g}, \Vterm{prefix}]$
is \dfn{consistent} with a full parse $[\Vcfg{g}, \Vterm{w}]$
if and only if
\[
\exists \; \Vterm{lookahead} : \Vterm{prefix}\Vterm{lookahead} = \Vterm{w}.
\]
A \dfn{full parse} is often called, simply, a \dfn{parse}. $\bigstar$
Note that, by the this definition,
a full parse is a special case of a partial parse.
\end{definition}

We say that symbol \Vsym{x} is \dfn{nullable} if and only if
$\Vsym{x} \destar \epsilon$.
\Vsym{x} is \dfn{nulling} if and only if it always derives the null
string, that is, for all \Vstr{y}
\begin{equation*}
\Vsym{x} \destar \Vstr{y} \implies \Vstr{y} \destar \epsilon.
\end{equation*}
\Vsym{x} is \dfn{non-nullable} if and only if it is not nullable.
\Vsym{x} is a \dfn{proper nullable} if and only if it is nullable,
but not nulling.

Locations in the input will be of type \type{LOC}.
By tradition, the input to the parse is written as \Cw{},
where $\Cw \in \Term{\var{g}}^\ast$.
\Vsize{w} will be the length of the input, counted in symbols.
When we state our complexity results later,
they will often be in terms of $\var{n}$,
where $\var{n} = \Vsize{w}$.

Where the choice of partial parse,
$\left[\Vcfg{g}, \Vterm{input}\right]$
is clear in context,
and unless otherwise specified,
use of the types \type{ES} and \type{LOC} will bind
their variables, so that
\begin{align*}
& \text{\Vloc{j} will imply that $0 \le \var{j} \le \size{\Vterm{input}}$; and} \\
& \text{\Ves{es} will imply that $\Ves{es} = \es{(\Vloc{j})} \land 0 \le \var{j} \le \size{\Vterm{input}}$.}
\end{align*}

\section{LR Grammars}

\begin{definition}
Let \Vstr{sf} be a sentential form of \Vcfg{g}.
A \dfn{handle} of \Vstr{sf} is an ordered pair $[\Vrule{r}, \var{i}]$ such
that,
for some $\Vterm{lookahead} \in \Term{\var{g}}^\ast$,
\begin{gather}
0 \le \var{i} \le \size{\Vstr{sf}}, \\
\begin{aligned}
\Accept{\var{g}} \xderives{R\ast} & \Vstr{stack}\Vsym{lhs}\Vterm{lookahead} \\
   \xderives{R} & \Vstr{stack}\Vsym{rhs}\Vterm{lookahead} \\
   = & \Vstr{sf},
\end{aligned}
\\
\Vrule{r} = \var{lhs} \de \Vstr{rhs}, \;\; \text{and} \\
\var{i} = \size{\Vstr{stack}\Vstr{rhs}}. \quad \bigstar
\end{gather}
\end{definition}

Let $\Vtermset{phrases} = \Term{\var{g}}^\ast$.
In this paper, a \dfn{partition}, call it $\pi$,
is a subset of the powerset of \var{phrases},
where all of the elements of $\pi$ are non-empty and disjoint,
and where the elements of $\pi$ ``cover'' \var{phrases} in the sense that
their union is equal to \var{phrases}.
That is,
\begin{gather}
\pi \subseteq \mathcal{P}(\var{phrases}), \\
(\forall \var{x} \in \pi) \; : \; \var{x} \neq \emptyset, \\
\var{phrases} = \bigcup_{\Vtermset{X}\in\pi} \Vtermset{X},
\end{gather}
and
\begin{gather}
(\forall \Vtermset{x}, \Vtermset{y} \in \pi) \; : \; \var{x} \cap \var{y} = \emptyset \lor \var{x} = \var{y}
\end{gather}
Note that in this paper $\pi$ will usually, but not necessarily,
be of finite cardinality.

We call an elements of $\pi$, a \dfn{cell}.
When two terminal strings, \Vterm{ss1} and \Vterm{ss2},
are in the same cell of $\pi$,
we say that \var{ss1} and \var{ss2} are ``equivalent modulo $\pi$'', or
\[
\var{ss1} \equiv \var{ss2} \pmod\pi.
\]

Informally, an LR($\pi$) grammar is a grammar where two
sentential forms that differ only in their lookaheads
have the same handle,
if those lookaheads are in the same cell of $\pi$.
More formally,

\begin{definition}
\label{def:LR}
\begin{align}
\begin{aligned}
\Accept{\Vint{g}} & \xderives{R\ast} \Vstr{history1}\Vsym{lhs1}\Vstr{lookahead1} \\
& \xderives{R} \Vstr{history1}\Vstr{rhs1}\Vstr{lookahead1} \\
& = \Vstr{behind}\Vstr{lookahead1},
\end{aligned}
\\
\begin{aligned}
\Accept{\Vint{g}} & \xderives{R\ast} \Vstr{history2}\Vsym{lhs2}\Vstr{lookahead2} \\
& \xderives{R} \Vstr{history2}\Vstr{rhs2}\Vstr{lookahead2} \\
& = \Vstr{behind}\Vstr{lookahead2},
\end{aligned}
\\
\Vstr{lookahead1} \equiv \Vstr{lookahead2} \pmod \pi,
\end{align}

We say that an LR($\pi$) grammar is LR-regular (LRR)
if and only if $\pi$ is a finite regular partition.
We say that an LR($\pi$) grammar is LR($\var{k}$)
if and only if
\begin{equation}
\pi =
\begin{gathered}
\left\lbrace
\left\lbrace
\var{u}\var{v}
\; \middle| \;
\var{v} \in \Term{\var{g}}^\ast
\right\rbrace
\; \middle| \;
(
\var{u} \in \Term{\var{g}}^\ast \land \Vsize{u} = \var{k}
)
\right\rbrace
\\
\cup
\left\lbrace
\left\lbrace
\var{u}
\right\rbrace
\; \middle|\;
( \var{u} \in \Term{\var{g}}^\ast \land \Vsize{u} \le \var{k} )
\right\rbrace
\end{gathered}
\end{equation}

Every LR($\pi$) grammar is unambiguous.
If a grammar is unambiguous then it is LR($\pi$)
for the partition of infinite cardinality,
\[
\pi = \set{\set{\Vstr{x}} : \var{x} \in \Term{\var{g}}^\ast}. \quad \bigstar
\]

\end{definition}

\Thref{th:LR}
presents \Dfref{def:LR}
in a way that is easier to apply.
\begin{theorem}
\label{th:LR}
\footnote{
The statement of the theorem follows \cite[p. 70]{Culik1973}.
}%
A grammar, \Vcfg{g}, is LR($\pi$) for a partition $\pi$,
if, given
\begin{gather}
\begin{aligned}
\Accept{\Vcfg{g}} & \xderives{R\ast} \Vstr{history1}\Vsym{lhs1}\Vstr{lookahead1} \\
& \xderives{R} \Vstr{history1}\Vstr{rhs}\Vstr{lookahead1},
\end{aligned}
\\
\begin{aligned}
\Accept{\Vcfg{g}} & \xderives{R\ast} \Vstr{history2}\Vsym{lhs2}\Vstr{lookahead3} \\
& \xderives{R} \Vstr{history1}\Vstr{rhs}\Vstr{lookahead2},
\end{aligned}
\;\; \text{and}
\\
\Vstr{lookahead1} \equiv \Vstr{lookahead2} ( \text{mod $\pi$} )
\end{gather}
we have
\begin{gather*}
\Vsym{lhs1} =
\Vsym{lhs2},
\\
\Vstr{history1} =
\Vstr{history2}, \;\; \text{and}
\\
\Vstr{lookahead3} =
\Vstr{lookahead2}.
\end{gather*}
\end{theorem}

\begin{proof}
Directly from \Dfref{def:LR}.
\end{proof}

\section{External grammars}
\label{sec:EXTs}

\subsection{Cycles}

An \dfn{external grammar} (type \type{EXT}) is cycle-free CFG.
A CFG is \dfn{cycle-free} if it contains no cycles.
A \dfn{cycle string}, or \dfn{cycle} is a sentential form
which non-trivially derives itself.
Intuitively, a cycle is the parsing equivalent of an infinite loop.
A \dfn{cycle derivation} is any derivation of the form
\begin{equation*}
\label{eq:cycle}
 \Vstr{cycle} \deplus \Vstr{cycle}.
\end{equation*}
Cycle strings and cycle derivations are also often called
simply ``cycles''.

Cycles are recursions but most recursions
are not cycles.
In a recursion, a symbol \Vsym{recurse} derives itself as part of a string.
That is, \Vsym{recurse} is recursive if and only if
\begin{equation*}
 \Vsym{recurse} \deplus \Vstr{pre} \cat \Vsym{recurse} \cat \Vstr{post}.
\end{equation*}
\Vsym{recurse} is a cycle only if
both \Vstr{pre} and \Vstr{post} are the null string:
\begin{equation*}
 \Vstr{pre} = \Vstr{post} = \epsilon.
\end{equation*}

Earlier implementations of \Marpa{} have supported cycles,
but the current implementation of \Marpa{} does not.
When the current implementation finds a cycle in a grammar,
it print a message describing the cycle and throws
a fatal error.
This seems to be the behavior users desire.

There is no indication that cycles are of practical use.
Nor does the theoretical literature suggest potential uses for
them.
Support for cycles did require complicated extra code,
and removing the support of cycles seems to be cost-free.

When \Marpa{} did support cycles,
it did so by cutting the cycle short at cycle depth 1.
Intuitively, the cycle depth is the number of times the cycle
string returns to itself.
More formally, if \Vstrset{d} is the derivation sequence
of the cycle derivation
$\Vstr{cycle} \xderives{\var{n}} \Vstr{cycle}$
then the cycle depth is
\begin{equation}
\left| \;
\left\lbrace \;
\var{i}
\quad \middle| \quad
\begin{gathered}
0 \le \var{i} \le \var{n} \quad \text{and} \; \\
\var{d}[\var{i}] = \Vstr{cycle}
\end{gathered}
\; \right\rbrace
\; \right|
\end{equation}
It follows that
a cycle of cycle depth 1 begins and ends with the
cycle string, but does not derive the cycle string
in any of its intermediate steps.

\section{Improper context free grammars}
\label{sec:improper}

A \dfn{proper CFG} is a CFG with no
\begin{itemize}
\item no unproductive symbols;
\item no inaccessible symbols; and
\item no cycles.
\end{itemize}
A \dfn{improper CFG} is a CFG which is
not proper.
We discussed \Marpa{}'s handling of cycles earlier
(section \ref{sec:EXTs}, p. \pageref{sec:EXTs}).

\subsection{Unproductive symbols}

The current implementation of \Marpa{} supports
unproductive symbols.
By default, \Marpa{} treats an unproductive symbol
as a fatal error,
but the user may bypass the fatal error
by marking the unproductive symbol as a ``unicorn''.

A \dfn{productive symbol}
is one which derives a terminal string.
That is, \Vsym{prod} is productive if
and only if, for some \Vterm{phrase},
\begin{equation*}
\Vterm{phrase} \in \Term{\var{g}}^\ast
\;  \land \;
  \Vsym{prod} \destar \Vterm{phrase}
\end{equation*}
It is vacuously true that
the empty string is a terminal string,
and therefore all nullable
symbols are productive.

A symbol is \dfn{unproductive} if it is not
productive.
In the current version of \Marpa{},
unproductive symbols can be implemented by
defining \dfn{unicorn} terminals.
A unicorn terminal is a terminal symbol
which can never be
found in the input.

In the \Marpa{} implementation,
a terminal can be made into a unicorn by
requiring that
a terminal match a regular expression that does not match
any string in $\Vocab{\Vint{g}}^\ast$.
For example,
the POSIX expression
\texttt{[\char`^ \char`\\ D\char`\\ d]}
indicates that the terminal
must match a single character which
is both a digit and a non-digit.
This is, of course, not possible
and therefore any terminal required to match that POSIX expression
will never match anything in the input.

Unproductive symbols often prove useful in \Marpa{}
applications.
One example of the utility of unproductive symbols
is interaction with custom lexers.
Marpa allows custom lexers, and these can feed lexemes
to the parser under application control,
observe the result,
and react accordingly.
The custom lexer can hand control back to \Marpa{}'s
own lexer at any point.

It is often convenient for \Marpa{} to use its internal lexer
in most situations,
switching over to custom lexer only at certain points.
To do this, unicorn terminals can
be defined and placed in the grammar at points where the
custom lexer should take over.
Marpa will never recognize a unicorn in the input,
but it does keep track of when unicorns are expected.
Marpa can be configured to trigger a run-time event
when the unicorn is expected.
The application will receive control when the event occurs,
and it can then invoke the custom lexer.

\subsection{Inaccessible symbols}

An \dfn{inaccessible symbol} is one which can never be derived from
the accept symbol.
That is, \Vsym{noacc} is inaccessible in \Vext{g} if and only if
\begin{equation*}
\neg \; \exists \; \Vstr{pre}, \Vstr{post} \; \mid \; \Accept{\var{g}} \destar \Vstr{pre} \Vsym{noacc} \Vstr{post}.
\end{equation*}

The current implementation of \Marpa{} supports inaccessible symbols.
The user can specify \Marpa{}'s treatment
of inaccessible symbols:
inaccessible symbols can be silently ignored,
they can be reported as warnings,
or they can be treated as fatal errors.
Treatment of inaccessible symbols can be specified
symbol by symbol,
and the default treatment can also be specified.
The ``factory default'' is
to report inaccessible symbols as warnings.

Inaccessible symbols have been found useful in \Marpa{} applications
that use higher-level languages ---
languages which generate other languages.
Often the generated grammar consists of
\begin{itemize}
\item ``custom''
rules which vary, and
\item ``boilerplate'' rules,
which are the same in every generated grammar.
\end{itemize}
It can be useful to have sections of the ``boilerplate''
be optional,
so that some of the ``boilerplate'' rules are only used
if the ``custom'' rules call for them.
If the custom rules do not call for a set of optional rules
within the boilerplate,
those boilerplate rules become inaccessible.

Optional boilerplate can be implemented
by silencing inaccessibility errors for the symbols in the optional boilerplate.
Usually it is possible to locate a topmost symbol for
each section of optional boilerplate,
in which case the warning need only be silenced for those
topmost symbols.

\section{Rewriting the grammar}
\label{sec:rewrite}

\subsection{The Aycock-Horspool parser}

A parser that claims to be practical must accept
grammars with nullable and nulling symbols
and empty rules.
But difficulties in handling these correctly
have bedeviled previous attempts
at Earley parsing.

Marpa follows Aycock and Horspool\cite{AH2002} in using
grammar rewrites to handle these issues.
Marpa's recognizer works using
Marpa internal grammars, but
Marpa's grammar rewriting allows the user to specify
their parse in terms of an external grammar.
In \Marpa{}, parsing, evaluation and
run-time events are performed
in such a way that the internal grammar
is invisible to
the user.

We defined \Marpa{}'s external grammars in
\Sref{sec:EXTs}.
Recall that an external grammar may be any cycle-free grammar.
Marpa internal grammars are described by construction
in this section.
They will be defined more carefully
in \Sref{sec:INTs}.
This section describes the rewrite
from an external grammar
to an internal grammar.

Grammar rewriting was not the focus
of \cite{AH2002} ---
instead that paper centered on revising Earley's algorithm
to use LR(0) states.
The first version of \Marpa{}\cite{Kegler2019}
used the LR(0) states of \cite{AH2002},
combining them with Joop Leo's improvements\cite{Leo1991}.

Adding the LR(0) states to the first version of \Marpa{}
made its theory much more complex,
and the change offered
no complexity gains in terms of Landau notation.
The hope was that benefits would be seen in practice.

Unfortunately, these benefits did not emerge.
Marpa's experience in using, developing and maintaining
LR(0) states in \Marpa{}'s early versions showed
that LR(0) states added
greatly to the complexity of the code;
were an obstacle to the implementation of run-time features
in the parser;
and produced no real improvement in speed or in space requirements.
On the other hand,
Aycock and Horspool's idea
of using grammar rewriting
to deal with
with nullable symbols became
fundamental to the \Marpa{} algorithm.

Rewriting grammars is a common
technique
in the parsing literature,
but many of these rewrites are almost totally without
usefulness in practice.
In the academic literature,
a rewritten grammar may recognize the same language,
but often it will not preserve the semantics
of the grammar.

Very few users of CFG's are
content with recognizing a language.
Almost always the user of a grammar-driven parser
formulated the rules and symbols of that grammar
in terms of a semantics,
and that user wants the parser
to preserve their semantics.
If a rewrite scrambles the user's semantics,
then that rewrite cannot play an essential role
in a general-purpose practical parsing tool.

Marpa restricts itself to using rewrites that
are \dfn{semantics-safe}.
The intent behind
semantics-safe rewrites
is that they
allow the rules and symbols of the external grammar
to be reconstructed from a parse using the internal grammar,
not just at evaluation time,
but at run-time as well.
\Sref{sec:augmentation},
\Sref{sec:eliminating-proper_nullables},
\Sref{sec:removing-nulling-symbols},
and
\Sref{sec:trivial-grammars}
describe the rewrites
applied by \Marpa{}.
We will state \Marpa{}'s definition
of semantics-safe more precisely
in section \Sref{semantics-safe}.

\subsection{Augmenting the grammar}
\label{sec:augmentation}

Let \Vint{g} be a \Marpa{} internal grammar
such that $\Vsym{accept} = \Accept{\Vint{g}}$.
There must be
a dedicated \dfn{accept rule} for \Vint{g},
\begin{gather}
\label{eq:accept-rule}
\Vrule{accept} = [\Vsym{accept} \de \Vsym{top}], \\
\intertext{such that}
\Vsym{top} \in \NT{\Vint{g}}, \\
\Vrule{accept} \in \Rules{\Vint{g}}, \\
\intertext{and for all \Vrule{r}, $\var{r} \in \Rules{\Vint{g}}$,}
\LHS{\Vrule{r}} = \Vsym{accept} \implies \Vrule{r} = \Vrule{accept} \\
\text{and} \quad \Vsym{accept} \notin \RHS{\Vrule{r}}.
\end{gather}

The dedicated accept rule is also called
the \dfn{augment rule}.
The semantics for the augment rule are simple ---
the semantics of \Vsym{top} are passed up to
\Vsym{accept}.
This is obviously semantics-safe.

In the \Marpa{} implementation,
the general method
described in section \Sref{semantics-safe}
for dealing with the semantics of rewritten rules
is not applied to the accept rule.
Instead, the accept rule is
dealt with as a special case.

\subsection{Eliminating proper nullables}
\label{sec:eliminating-proper_nullables}

Recall that a proper nullable
is a nullable symbol which is not nulling.
Marpa eliminates proper nullables with an improved
version of the rewrite in~\cite{AH2002}.
In \Marpa{}'s rewrite,
rules with proper nullables are identified
and a new grammar is created
in which the external grammar's rules are divided
up so that no rule has more than two proper nullables.
This is similar to a rewrite into Chomsky form.

Every proper nullable symbol is then cloned into two others:
one nulling and one non-nullable.
All occurrences of the original proper nullable symbol are then replaced
with one of the two new symbols.
New rules are created as necessary to ensure that all possible combinations
of nulling and non-nullable symbols are accounted for.

The rewrite in~\cite{AH2002} was similar, but did not do the Chomsky-style
rewrite before replacing the proper nullables.
As a result the number of rules in the post-rewrite grammar of \cite{AH2002}
could be \order{e^\ell}, where $\ell$ is the maximum length of a rule in the
the pre-rewrite grammar of \cite{AH2002},
\[
\ell = \max\left(\set{ \Vsize{r} : \Vrule{r} \in \Rules{\Vext{pre-rewrite}} }\right).
\]
In our version, the worst case growth in the number of rules is \order{\ell}.

In traditional complexity terms,
because $\ell$ is a constant which
depends on the grammar, and often a very reasonable one,
minimizing the rule count in terms of $\ell$ does not
appear to be a important consideration.
In practice, however, languages and language statements which have many optional clauses are important.
The SELECT statement of SQL is one example.
A straight-forward BNF representation of the syntax of a statement
with many optional clauses has many proper nullables on the RHS,
so that, in these cases,
going exponential on $\ell$ has a cost
which it is worth some trouble to avoid.

\subsection{Removing nulling symbols}
\label{sec:removing-nulling-symbols}

After the rewrite of
\Sref{sec:eliminating-proper_nullables},
all the nullable symbols in the grammar are also
nulling symbols.
Note that, if \Vext{g} is an external grammar,
and
if \varprime{g}{'} is \Vext{g}
revised to eliminate
all nulling symbols,
that
the language of the two grammars is the same: $\myL{\Vext{g}} = \myL{\varprime{g}{'}}$.
Similarly,
other than
the presence and absence of the nodes for the nulling symbols themselves,
the parse trees generated from \Vext{g} are the same as those generated from \varprime{g}{'}.

Thus, \Marpa{} may rewrite its grammars to eliminate nulling symbols.
For the purposes of evaluation and to allow the generation of parse-time events
based on nulled symbols,
Marpa records the locations of the eliminated nulling symbols.

A corner case occurs when eliminating nulling symbols from two distinct
rules makes them identical.
In this paper,
we assume a rewrite that creates
a new LHS symbol for each rule that must remain distinct;
and that adds new rules to derive the new LHS symbols from the original ones.
Less clean theoretically,
but easier in practice,
is to have the implementation treat rules as distinct
even if they differ only in their recorded nulling symbols.
This is what the \Marpa{} implementation does.

\subsection{Trivial grammars and null parses}
\label{sec:trivial-grammars}

Null parses are parses of zero length inputs.
The \Marpa{} implementation
handles null parses by treating them
as a special case.

\dfn{Degenerate grammars} are grammars which do not accept any string.
A degenerate grammar is treated as a fatal error.

\dfn{Trivial grammars} are grammars which only accept
zero length inputs.
No \Marpa{} internal grammar can be a trivial grammar.
Trivial grammars are handled as a special case.

\subsection{Semantics-safe rewriting}
\label{semantics-safe}

This subsection describes the restrictions
that rewrites must follow in order to be semantics-safe.
Consider an input \Cw{} and
an external grammar, \Vext{g}.
Let \Vint{g} be the internal grammar that is
the rewritten version of \Vext{g}.

Let
\begin{equation}
\label{eq:parse-tree}
\var{pt} = \var{tree}(\var{g},\Vstr{w})
\end{equation}
be the parse tree that results from a parse of \Vstr{w}
by a grammar \var{g}.
If \var{g} is an internal grammar,
\var{pt} is an \dfn{internal parse}.
If \var{g} is an external grammar,
\var{pt} is an \dfn{external parse}.

Every node of \var{pt} is a \dfn{symbol instance}.
The symbol instances of the parse correspond one-to-one with 4-tuples
of the form
\begin{equation*}
[ \Vsym{s}, \Vloc{start}, \var{length}, \var{depth} ].
\end{equation*}
where
\begin{itemize}
\item \Vsym{s} is the node's symbol.
\item \boldRange{w}{\Vloc{start}}{(\Vloc{start}+\var{length}-1)}
is the terminal string derived from \var{s}.
\item \var{depth}
is the ``brick depth''~\Lref{loc-brick-depth}.
\end{itemize}

In an internal grammar, there are
brick and mortar symbols.
Internal \dfn{brick} symbols correspond, many-to-one, to
external symbols.
Internal \dfn{mortar} symbols exist entirely for the purposes
of the internal grammar.
Only brick symbols have semantics attached to them.
All terminal symbols are bricks.

Recall \var{pt} from \Eref{eq:parse-tree}.
If the symbol of an instance in \var{pt} is a brick,
the instance is a \dfn{brick instance}.
If the symbol of an instance in \var{pt} is a mortar symbol,
the instance is a \dfn{mortar instance}.

The
\dfn{brick depth}~\label{loc-brick-depth}
of an instance in a external grammar is
the depth of the node of the instance measured in the traditional way --
the distance in nodes to the root.
In an internal grammar, the brick depth is more complicated.
Intuitively, it is the distance of the node
in brick nodes.
More formally,
\begin{itemize}
\item The brick depth of the root instance of an internal grammar is undefined.
\item Since an internal grammar is augmented, the root instance must
have only one child, a brick instance whose brick depth is defined to be zero.
\item For all other brick nodes,
the brick depth is one plus the brick depth of its parent node.
\item For mortar nodes, the brick depth is the brick depth of its parent node.
\end{itemize}

For evaluation, it is useful to convert the internal parse tree to
a \dfn{brick tree}.
To create a brick tree, we traverse the internal parse tree in pre-order,
ignoring internal nodes,
and creating instances for the brick nodes.
Note that this implies that the root node (the node
whose symbol is \Accept{\Vint{g}}) is ignored.

When, while constructing a brick tree,
a brick node is encountered that is marked with memoized
nulling symbols,
new instances must be created for the nulling symbols.
The new instances are called \dfn{nulling instances}.

Let \var{marked} be a node in a parse tree.
Assume that \var{marked} has
one or more recorded nulling instances.
Recorded with each nulling instance will be whether it occurs
before or after \var{marked}.
In the brick tree, each nulling instance will take the form
\begin{equation*}
[ \Vsym{nulling}, \Vloc{bound}, 0, \var{depth} ]
\end{equation*}
where
\begin{itemize}
\item \Vsym{nulling} is the memoized nulling symbol.
\item \Vloc{bound} is the location of the nulling symbol.
If the nulling instance occurs before \var{marked},
\var{bound} is the start location of \var{marked}.
If the nulling instance occurs after \var{marked},
\var{bound}
is the location
immediately after
\var{marked}.
\item \var{depth} is the same as the depth of \var{marked}.
\end{itemize}
Nulling instances are brick instances.

In what follows, we will want to match instances from an internal
parse
to instances of an external parse of the same input.

\begin{definition}[Equality of symbol instances]
Symbol instances are \dfn{equal} if their elements
are equal.  This is, if \var{node1} and \var{node2} are
instances,
\begin{equation*}
\begin{aligned}
& \var{node1} = \var{node2} \quad \defined \\
& \quad
\left(
\begin{gathered}
\exists \Vsym{s}, \Vloc{start}, \var{length}, \var{depth} : \\
\var{node1} = [ \Vsym{s}, \Vloc{start}, \var{length}, \var{depth} ] \\
\land \;\; \var{node2} = [ \Vsym{s}, \Vloc{start}, \var{length}, \var{depth} ]
\end{gathered}
\right) \quad \bigstar
\end{aligned}
\end{equation*}
\end{definition}
Duplicate symbol instances do not occur within a parse tree,
so we will usually be speaking of the equality of symbol
instances in different trees.

\begin{definition}[Semantics-safe]
Let
\begin{itemize}
\item
$\var{Etree} = \var{parse}(\Vext{g},\Vstr{w})$
be the parse from
an external grammar,
\item $\var{Itree} = \var{parse}(\Vint{g},\Vstr{w})$
be the parse from
an internal grammar, and
where \Vint{g} is a rewrite of \Vext{g},
\item \var{Btree} be the brick tree
produced from \var{Itree}.
\end{itemize}
Let \var{Instance-matches} be the relation defined by
the equality of symbol instances
in \var{Btree} and \var{Etree}:
\begin{equation*}
\var{Instance-matches} \; \defined \;
\left\lbrace
\begin{gathered}
[ \var{Enode}, \var{Bnode} ] : \\
\begin{aligned}
& \var{Enode} \in \var{Etree} \\
\land \; & \var{Bnode} \in \var{Btree} \\
\land \; & \var{Enode} = \var{Bnode}
\end{aligned}
\end{gathered}
\right\rbrace
\end{equation*}

A rewrite is \dfn{semantics-safe}, if and only if
\var{Instance-matches}
is a one-to-one correspondence
(bijection). $\bigstar$
\end{definition}

\section{\Marpa{} internal grammars}
\label{sec:INTs}

The formal definition of
a \Marpa{} internal grammar is that of
a \Marpa{} external grammar,
with two additional restrictions.

\begin{itemize}
\item \Marpa{} internal grammars are augmented.
For details of grammar augmentation,
see \Sref{sec:augmentation}.
\item \Marpa{} internal grammars do not allow nullable symbols.
\end{itemize}


The notations for CFG's \Sref{sec:CFGs}
carry over to internal grammars,
although with the changes imposed
by the restricted form of INT's.
Noteworthy among these changes are
that \Marpa{} internal grammars

\begin{itemize}
\item do not allow nulling symbols,
\item do not allow the input to a parse to be of length zero, and
\item do not allow the RHS of a rule to be empty.
\end{itemize}

In the rest of this paper,
unless otherwise stated,
we will be assuming that the grammar
we are discussing
is a \Marpa{} internal grammar.
The restrictions imposed on INT's allow us
to state some
definitions, for example \Dfref{def:rr},
more simply than we could if they had to apply
to EXT's, or to CFG's,

\begin{definition}
\label{def:rr}
Let \Vint{g} be an internal grammar.
Let \Vstr{sf} be a sentential form of \var{g} non-trivially derived from
\Vsym{rr}, so that
\begin{equation}
\label{eq:rr}
\Vsym{rr} \derives \Vstr{rhs} \destar \Vstr{sf}.
\end{equation}
The derivation of \Eref{eq:rr}
is \dfn{right-recursive} if and only if
\begin{equation}
\Rightmost{\Vstr{sf}} = \Vsym{rr}.
\end{equation}
We also say that the rule used in the first step of \Eref{eq:rr},
is \dfn{right-recursive},
\begin{equation*}
\RightRecursive{[\Vsym{rr} \de \Vstr{rhs}]}
\end{equation*}
and that the symbol $\Vsym{rr}$
is \dfn{right-recursive},
\begin{equation*}
\RightRecursive{\Vsym{rr}}. \quad \bigstar
\end{equation*}
\end{definition}
\Dfref{def:rr} implies that, for any rule \var{r}, $\var{r} \in \Rules{\var{g}}$,
we have
\begin{equation}
\begin{gathered}
\RightRecursive{\var{r}} \implies \RightRecursive{\LHS{\var{r}}}.
\end{gathered}
\end{equation}

\begin{theorem}
Let \Vint{g} be a \Marpa{} internal grammar.
If \Vrule{r} is a rule used in a step of a right recursive derivation,
then \Vrule{r} is right recursive.
\end{theorem}

\begin{proof}
Note that for the case of a right recursion of length 1,
the theorem is trivially true.
We now consider the case of an arbitrarily chosen step
in a right recursion in \var{g} of
length 2 or more:
\begin{align}
\label{rr-proof-10a}
\Vsym{rr1} \derives & \; \Vstr{rhs1} \\
\label{rr-proof-10b}
 \destar & \; \Vstr{preX} \Vsym{symX} \\
\label{rr-proof-10c}
 \derives & \; \Vstr{preX} \Vstr{rhsX} \\
\label{rr-proof-10d}
 \destar & \; \Vstr{preX} \Vstr{leftX} \Vsym{rr1}.
\end{align}
The arbitrarily chosen step is from
\Eref{rr-proof-10b} to
\Eref{rr-proof-10c}
and uses the rule $[ \Vsym{symX} \de \Vstr{rhsX} ]$.
We may ``pump'' the derivation of
\Eref{rr-proof-10a}--\Eref{rr-proof-10d}
to produce another right recursion in \var{g}
\begin{align}
\nonumber
\Vsym{rr1} \derives & \; \Vstr{rhs1} \\
\label{rr-proof-20a}
 \destar & \; \Vstr{preX} \Vsym{symX} \\
\label{rr-proof-20b}
 \derives & \; \Vstr{preX} \Vstr{rhsX} \\
\nonumber
 \destar & \; \Vstr{preX} \Vstr{leftX} \Vsym{rr1} \\
\nonumber
 \derives & \; \Vstr{preX} \Vstr{leftX} \Vstr{rhs1} \\
\label{rr-proof-20c}
 \destar & \; \Vstr{preX} \Vstr{leftX} \Vstr{preX} \Vsym{symX} \\
\nonumber
 \derives & \; \Vstr{preX} \Vstr{leftX} \Vstr{preX} \Vstr{rhsX} \\
\nonumber
 \destar & \; \Vstr{preX} \Vstr{leftX} \Vstr{preX} \Vstr{leftX} \Vsym{rr1}.
\end{align}

Isolating the derivation from \Eref{rr-proof-20a} to
\Eref{rr-proof-20c} we have
\begin{equation}
 \Vsym{symX} \derives \Vstr{rhsX} \destar
 \Vstr{leftX} \Vstr{preX} \Vsym{symX}
\end{equation}
so that
\begin{equation}
\RightRecursive{[ \Vsym{symX} \de \Vstr{rhsX} ]} \because \Dfref{def:rr}.
\end{equation}
\end{proof}

In this paper,
\Earley{} will refer to the Earley's original
recognizer\cite{Earley1970}
with zero characters of lookahead.
\Leo{} will refer to Leo's revision of \Earley{}
as described in~\cite{Leo1991}.
\Marpa{} will refer to the recognizer described in
this paper, and implemented in
Marpa::R2~\cite{Marpa-R2}.

\section{Earley tables}
\label{sec:earley-table}

The definitions of this section
assume that \Vint{g} is a \Marpa{} internal grammar.
Definitions of \Earley{} for more general CFG's
can be found in many places,
including
\cite[pp. 320-321]{AU1972},
\cite{AH2002},
\cite{Earley1968},
\cite{Earley1970},
\cite{GJ2008}, and
\cite{Leo1991}.

Let $\Vrule{r} \in \Rules{\Vint{g}}$
be a rule.
Recall that $\Vsize{r}$
is the length of the RHS of \var{r}.
A \dfn{dotted rule} (type \type{DR}) is a duple, $[\Vrule{r}, \var{pos}]$,
where $0 \le \var{pos} \le \size{\Vrule{r}}$.
The position, \var{pos}, indicates the extent to which
the rule has been recognized,
and is represented with a large raised dot,
so that if
\begin{equation*}
[\Vsym{A} \de \Vsym{X} \Vsym{Y} \Vsym{Z}]
\end{equation*}
is a rule,
\begin{equation*}
[\Vsym{A} \de \Vsym{X} \Vsym{Y} \mydot \Vsym{Z}]
\end{equation*}
is the dotted rule with the dot at
$\var{pos} = 2$,
between \Vsym{Y} and \Vsym{Z}.
\DottedRules{\Vcfg{g}} is the set of dotted rules
in \Vcfg{g}, that is,
\begin{align*}
& \DottedRules{\Vcfg{g}} \defined \\
& \qquad \qquad \left\lbrace \;
\begin{aligned}
& \hspace{0pt} [ \Vrule{r}, \var{pos} ] \quad \text{such that} \; \\
& \begin{aligned}
         & \Vrule{r} \in \Rules{\Vint{g}} \\
\qquad \land \quad & 0 \le \var{pos} \le \size{\RHS{\Vrule{r}}} \\
\end{aligned} \\
\end{aligned}
\; \right\rbrace .
\end{align*}
Where the choice of \Vcfg{g} is clear in context,
and unless otherwise specified,
\Vdr{dr} will imply that $\var{dr} \in \DottedRules{\var{g}}$.
For convenience, where
\[
\Vdr{x} = [\Vrule{r}, \var{pos}]
\]
is a dotted rule,
\begin{equation*}
\begin{gathered}
\text{$\Rule{\Vdr{x}} \defined \Vrule{r}$ and} \\
\DotPos{\Vdr{x}} \defined \var{pos}.
\end{gathered}
\end{equation*}

We consider the size of a grammar, \Vsize{\var{g}}, to be the
number of distinct dotted rules it allows,
$\size{\Vcfg{g}} = \size{\var{Dotted-Rules}[\var{g}]}$.

If we let \Vdr{x} be a dotted rule, such that
\begin{equation}
\Vdr{x} = \left[ \Vrule{x}, \var{pos} \right],
\end{equation}
then
\begin{align}
& \LHS{\Vdr{x}} && \defined && \quad
\LHS{\Vrule{x}} \\
%
& \Predot{\Vdr{x}} && \defined && \quad
\begin{cases}
\RHS{\Vrule{x}}[\var{pos}\subtract 1], \; \text{if $\var{pos} > 0$} \\
\Lambda, \; \text{otherwise}
\end{cases} \\
\label{eq:def-postdot}
& \Postdot{\Vdr{x}} && \defined && \quad
\begin{cases}
\RHS{\Vrule{x}}[\var{pos}], \; \text{if $\var{pos} < \size{\Vrule{x}}$} \\
\Lambda, \; \text{otherwise}
\end{cases} \\
\label{eq:def-next}
& \Next{\Vdr{x}} && \defined && \quad
\begin{cases}
[\Vrule{x}, \var{pos}+1], \; \text{if $\var{pos} < \size{\Vrule{x}}$} \\
\Lambda, \; \text{otherwise}
\end{cases} \\
\label{eq:def-penult}
& \Penult{\Vdr{x}} && \defined && \quad
\begin{cases}
\Postdot{\Vdr{x}}, \; \text{if $\var{pos} = \size{\Vrule{x}}\subtract 1$} \\
\Lambda, \quad \text{otherwise}
\end{cases}
%
\end{align}

A dotted rule, \Vdr{d},
is a \dfn{penult} if it is such that $\Penult{\var{d}} \neq \Lambda$.
The \dfn{initial dotted rule} is
\begin{equation*}
\Vdr{initial} = [\Vsym{accept} \de \mydot \Vsym{start} ].
\end{equation*}
A \dfn{predicted dotted rule},
or \dfn{prediction} is a dotted rule,
other than the initial dotted rule,
with a dot position of zero,
for example,
\begin{equation*}
\Vdr{predicted} = [\Vsym{A} \de \mydot \Vstr{rhs} ].
\end{equation*}
A \dfn{confirmed dotted rule},
or \dfn{confirmation},
is a dotted rule
with a dot position greater than zero.
A \dfn{completed dotted rule},
or \dfn{completion},
is a dotted rule with its dot
position after the end of its RHS,
for example,
\begin{equation*}
\Vdr{completed} = [\Vsym{A} \de \Vstr{rhs} \mydot ].
\end{equation*}

A dotted rule which is not a completion is called an
\dfn{incomplete rule},
or a \dfn{incompletion}.
A rule which is both an incompletion and a confirmation
is called a \dfn{medial rule}, or \dfn{medial}.

A Earley item (type \type{EIM}) is a duple
\begin{equation}
\label{eq:d-eim}
    [\Vdr{dotted}, \Vorig{x}]
\end{equation}
of dotted rule and origin.
(The origin is the location where recognition of the rule
started.
It is sometimes called the ``parent''.)
For convenience, the type \type{ORIG} will be a synonym
for \type{LOC}, indicating that the variable designates
the origin element of an Earley item.

For convenience,
for an EIM such as
$\Veim{x} = [\Vdr{x}, \Vorig{x}]$,
we write,
\begin{equation*}
\label{eq:def-dr}
\begin{alignedat}{3}
& \DR{\Veim{x}} && \defined && \quad \Vdr{x}, \quad \text{and} \\
& \Origin{\Veim{x}} && \defined && \quad \Vorig{x}
\end{alignedat}
\end{equation*}

When a dotted rule concept is applied to an EIM,
it is the same as applying that concept to the EIM's
dotted rule.
When a rule concept is applied to an dotted rule
it is the same as applying that concept to the rule
of the dotted rule.
When a rule concept is applied to an EIM
it is the same as applying that concept to the rule
of the dotted rule of the EIM.

For example, let
\begin{equation*}
\Veim{x} = [\Vdr{x}, \Vorig{x}] =
\left[[\Vrule{x}, \var{pos}], \Vorig{x} \right].
\end{equation*}
Then
\begin{equation*}
\begin{gathered}
\Penult{\Veim{x}} = \Penult{\Vdr{x}}, \\
\Postdot{\Veim{x}} = \Postdot{\Vdr{x}}, \\
\LHS{\Veim{x}} = \LHS{\Vdr{x}} = \LHS{\Vrule{x}}, \\
\RHS{\Veim{x}} = \RHS{\Vdr{x}} = \RHS{\Vrule{x}}, \\
\end{gathered}
\end{equation*}
and so forth.

Similarly, we say that
an EIM is a \dfn{penult} if its dotted rule is a penult,
an EIM is a \dfn{prediction} if its dotted rule is a prediction,
and so on.

A \dfn{parse item} is either an Earley or a Leo item.
Parse items are of type \type{PIM}.
Leo items (type \type{LIM})
will be defined in \Sref{sec:leo}.

Within \Marpa{},
an Earley set is defined as a set of \dfn{parse items}.
Earley sets are of type \type{ES}.
As a convenient way to say that a PIM
is in an Earley set,
we will put an `$@$' operator between
the PIM and its Earley set.
For example, to say that an arbitrary EIM,
$\Veim{x} = [ \Vdr{dotted}, \Vorig{i}]$
is in Earley set \Ves{j}, we may write any of
\begin{gather*}
    \Veim{x} @ \Ves{j}, \qquad \Veim{x} @ \Vloc{j}, \qquad \var{x} @ \var{j}, \\
    \text{$[ \Vdr{dotted}, \Vorig{i}]_\type{EIM} @ \Ves{j}$, or} \\
    \text{$[ \Vdr{dotted}, \Vorig{i}] @ \var{j}$, etc.}
\end{gather*}
with the type-less variations used in contexts where type is
understood.
Similarly, for the
PIM \Vpim{p} at \Ves{j}, we may write
\[
    \text{$\Vpim{p} @ \Ves{j}$, $\var{p} @ \var{j}$, etc.}
\]
Also,
for aribtary
$\Veim{x}@\Vloc{eLoc}$ and
$\Vpim{x}@\Vloc{pLoc}$,
\begin{gather*}
\text{$\Current{\Veim{x}@\Vloc{eLoc}} \defined \Vloc{eLoc}$, and}\\
\Current{\Vpim{x}@\Vloc{pLoc}} \defined \Vloc{pLoc}.
\end{gather*}

When we speak of Earley sets in procedural terms,
we will say that an Earley item is \dfn{unioned into}
an Earley set,
or \dfn{unioned},
rather than say that the Earley item is ``added'' to an Earley set,
or ``added''.
This is because
Earley sets are sets,
so that ``adding'' a duplicate EIM to
an ES leaves the ES unchanged.

An Earley parser builds a table of Earley sets,
which is called
a \dfn{parse table},
or an \dfn{Earley table}.
An Earley table
is a sequence of Earley sets.
If \Vesset{table} is an Earley table, then
\begin{equation*}
\size{\Vesset{table}} = \size{\Vstr{w}},
\end{equation*}
where \Vstr{w} is the input of the parse.
and \Vesset{table} is the parse table.
Earley set 0 is the initial Earley set.
Earley set $\Vloc{i}+1$ describes the state of
the parse after processing the input at location \Vloc{i}.

Earley sets are often named by their location,
so that \Ves{i} means the Earley set at \Vloc{i}.
Where \Vloc{i} is an integer location,
the notation \Ves{i} represents an Earley set, so that
if \Vesset{table}
is the parse table,
\begin{equation*}
\Ves{i} = \Vesset{table}[\Vloc{i}].
\end{equation*}

Whenever helpful,
we will allow ourselves
to go back and forth between integer location,
\Vloc{i},
and Earley set,
\Ves{i}.\footnote{
For a discussion of this from the implementation
point of view, see \Sref{sec:es-indexing}.
}
If \var{working} is an Earley set,
$\size{\Ves{working}}$ is the number of Earley items
in \Ves{working}.

Recall that
there was a unique accept symbol,
\Accept{\Vint{g}}, in \Vint{g}.
The input \Cw{} is accepted if and only if,
for some \Vstr{rhs},
\begin{gather*}
\Vdr{accept} = [\Accept{\Vint{g}} \de \Vstr{rhs} \mydot] \quad \text{and} \\
\left[\Vdr{accept}, 0\right] \in \Etable{\Vsize{\Cw}}.
\end{gather*}

\TODO{Revisit productivity, accessiblity.  Require in INT?}
\begin{definition}
\label{def:EIM-valid}
Let $[\Vcfg{g}, \Vterm{prefix}]$ be a partial parse.
Let, without loss of generality,
\begin{equation}
\label{eq:EIM-valid-10}
\Veim{x} = \left[ \left[ \Vsym{lhs} \de \Vstr{rhs1} \mydot \Vstr{rhs2}
   \right], \Vorig{i} \right] @ \Ves{j}
\end{equation}
be an EIM,
where $\var{i} \le \var{j} < \Vsize{prefix}$.
\Veim{x} is \dfn{valid}
if and only if
\begin{gather}
\label{eq:EIM-valid-22a}
\left[ \Vsym{lhs} \de \Vstr{rhs1} \Vstr{rhs2} \right] \in \Rules{\var{g}}
\\
\label{eq:EIM-valid-22b}
\Accept{\var{g}} \destar \Vstr{before} \Vsym{lhs} \Vstr{after}, \\
\label{eq:EIM-valid-22c}
\Vstr{before} \destar \var{prefix}[0  \ldots  (\var{i}\subtract 1)], \\
\label{eq:EIM-valid-22d}
\Vstr{rhs1} \destar \var{prefix}[\var{i}  \ldots  (\var{j}\subtract 1)].
\end{gather}
\Veim{x}
is \dfn{useful}
if and only if
\begin{equation*}
\label{eq:EIM-valid-30}
\Vstr{rhs2}\Vstr{after} \destar \var{prefix}[\var{j}  \ldots  (\Vsize{w}\subtract 1)].
\quad \bigstar
\end{equation*}
\end{definition}

\section{The Leo algorithm}
\label{sec:leo}

\subsection{The history of a conjecture}

Jay Earley~\cite[p. 60]{Earley1968}
conjectured that \Earley{} could be
modified to be \On{} for
all deterministic context-free grammars (DCFG's).
(Recall that the DCFG's are the union of the
\var{LR}(\var{k})
grammars for all \var{k}.)
He gave no details of the method he had in mind.
Earley left the field shortly thereafter ---
by 1973 he had earned a second Ph.D. and
was a practicing psychotherapist in California.

In the late 1980's, Joop Leo
also conjectured that
Earley's algorithm could be modified to be linear for all
DCFG's.
Leo was initially unaware of Earley's earlier conjecture ---
he discovered it in the course of writing up his
discovery.
Leo published his method in~\cite{Leo1991}.

The problem both investigators noticed was that,
while \Earley{} is \On{}
for left recursion,
and in fact very efficient,
it is $\order{\var{n}^2}$ for right recursion.
This is because all EIM's for a right
recursion are created at every parse location
where the right recursion might end.

For simplicity,
in the discussion of this subsection,
we will assume that the grammar is unambiguous.
Consider one right recursion,
and the set of EIM's in it.
We will the subset of these EIM's that are part of a single Earley set,
a ``right recursion edge'', or \dfn{RR edge}.
All of the EIM's in an RR edge will have the same end location.

As an \Earley{} parse proceeds through a right recursion,
the potential length of the recursion grows.
The actual length of the right recursion will not be known
until the \Earley{} parser finds the end of the right recursion,
which can be arbitrarily long.

At the first RR edge, the potential length is 1,
and the RR edge contains 1 EIM.
at the second RR edge, the potential length is 2,
and the RR edge contains 2 EIM's.
If the potential length of the right
recursion at an RR edge is \var{n},
then the RR edge contains \var{n} EIM's.
The total number of EIM's in the RR edges
needed for a right recursion of length \var{n} is
\[
  \sum_{\var{i} = 1}^\var{n} \var{i} = \order{\var{n}^2}.
\]
Whne the end of the right recursion is found,
and its actual length is known,
only the EIM's in the last RR edge will be used.
That is, only \var{n} EIM's are useful
\Dfref{def:EIM-valid}.

% TODO: Move or replace with later discussion on Leo memoization?
Leo's idea was to memoize the right recursions.
With Leo memoization, each RR edge is represented by
a pair of EIM's and a memo.
There are \Oc{} Leo memos per Earley set,
so the time and space complexity
of an RR edge is \Oc{}.
The time and space complexity of all the RR edges
in a right recursion of length \var{n} will
be
\[
  \sum_{\var{i} = 1}^\var{n} 1 = \On.
\]

If, at evaluation time,
it is desirable to expand the Leo memoizations,
only the useful EIM's
need to be expanded.
The number of memoized
EIM's that need to be expanded
will be \On{},
where \var{n} is the length of the recursion.
As a result,
even if the time and space
required to expand Leo memoization
during evaluation
are taken into account,
the time and space complexity of
a right recursion become
$\On{} + \On{} = \On{}$.

% TODO: Define LR(k) here?
% TODO: Move this to earlier definitions?
In \cite{Leo1991}, Joop Leo
showed that,
with his modification, \Earley{}
is \On{} for all LR-regular grammars.
LR-regular (LRR) is LR where lookahead
is infinite length, but restricted to
distinguishing between regular expressions.
Earley did not claim \On{} for LRR
because LRR was not introduced
to the literature until 1973~\cite{Culik1973}.
Even in 1991, LRR was not well-known,
which is why Leo only claims
the weaker
\var{LR}(\var{k})
bound in
his title.

\subsection{Leo items}
\label{sec:LIMs}

Summarizing Leo's method,
it consists of spotting potential right recursions
and memoizing them.
Leo restricts the memoization to situations where
the right recursion is unambiguous.
Potential right recursions are memoized by
Earley set, using what Leo called
``transitive items''.
In this paper Leo's ``transitive items''
will be called Leo items.
Leo items
will have type \type{LIM}.
Recall that every parse item (\type{PIM})
is either
an Earley item or a Leo item.

In each Earley set, there is at most one Leo item
for each transition symbol.
A Leo item (LIM) is a triple.
Let
\begin{equation*}
\Vlim{x} = [ \Vdr{top}, \Vsym{transition}, \Vorig{top} ]
\end{equation*}
be an arbitrary Leo item.
\Vsym{transition} is the transition symbol of \Vlim{x},
and
\begin{equation*}
\Veim{top} = [\Vdr{top}, \Vorig{top}]
\end{equation*}
is the Earley item to be unioned as a result of a operation
using the LIM.
To say that \Vlim{x}
is at \Ves{j}, we may write
\begin{gather*}
    \Vlim{x} @ \Ves{j}, \\
    [ \Vdr{top}, \Vsym{transition}, \Vorig{top} ]_\type{LIM} @ \Vloc{j}, \\
    \text{$[ \Vdr{top}, \Vsym{transition}, \Vorig{top} ] @ \var{j}$, etc.}
\end{gather*}
Also,
\begin{align}
\DR{\Vlim{x}} & \defined \Vdr{top}, \\
\Transition{\Vlim{x}} & \defined \Vsym{transition},
\;\; \text{and} \\
\Origin{\Vlim{x}} & \defined \Vorig{top},
\end{align}
and, for arbitrary $\Vlim{x}@\Vloc{j}$,
\[
\Current{\Vlim{x}@\Vloc{j}} \defined \Vloc{j}.
\]

Leo items memoize what would otherwise be sequences
of Earley items.
Leo items only memoize unambiguous (or
deterministic) sequences,
so that the top of the sequence can represent
the entire sequence --
the only role the other EIM's in the sequence
play in the parse is to derive the top EIM.
We will call these memoized sequences, Leo sequences.

\subsection{Leo uniqueness}
\label{sec:leo-uniqueness}

To quarantee that a Leo sequence is deterministic,
\Leo{} enforced \dfn{Leo uniqueness}.
\begin{definition}[Leo unique]
\label{def:leo-unique}
An EIM \Veim{uniq}
is \dfn{Leo unique} in \Ves{j}
if and only if
\[
\Veim{uniq} \in \LeoUnique{\Ves{j}},
\]
where
\LeoUnique{\Vloc{j}} is defined as the set of \Veim{uniq}
such that
\begin{itemize}
\item $\Penult{\var{uniq}} \neq \Lambda$,
\item $\Current{\var{uniq}} = \Vloc{j}$, and
\item
for all \Veim{x},
\begin{gather*}
\left(
  \begin{gathered}
  \Current{\var{uniq}} = \Current{\var{x}} \\
  \land \; \Penult{\var{uniq}} = \Postdot{\var{x}}
  \end{gathered}
\right)
\\
\implies \var{uniq} = \var{x}.
\end{gather*}
\end{itemize}
Note that \Veim{x} ranges over all the
EIMs of Earley set \Ves{j},
even those which are not penults.
$\bigstar$
\end{definition}

\subsection{LIM predecessor}
\label{sec:lim-predecessor}

Tracking the Leo chains requires that there
be an analog to the concept of predecessor EIM
for LIMs.
We say that \Vlim{predecessor}
is the \dfn{LIM predecessor} of \Veim{successor},
\begin{equation*}
\Vlim{predecessor} = \LIMPredecessor{\Veim{successor}},
\end{equation*}
if and only if
\begin{equation*}
\begin{gathered}
\Transition{\Vlim{predecessor}} = \Predot{\Veim{successor}} \\
\land \; \Current{\Vlim{predecessor}} = \Origin{\Veim{successor}}.
\end{gathered}
\end{equation*}
If there is no
\Predot{\Veim{successor}},
that is, if
$\Predot{\Veim{successor}} = \Lambda$,
then there is no LIM predecessor,
$\LIMPredecessor{\Veim{successor}} = \Lambda$.

\section{Ancestry}
\label{sec:ancestry}

\Marpa{}, like other variants of \Earley{}, performs
``operations'' on the Earley sets to produce new
Earley items and, in \Marpa{}'s case,
new Leo items.
A operation may require that one or two other
PIM's already exist in the Earley sets before
it becomes applicable.
If an operation requires that \Vpim{anc}
exist in the Earley sets
before unioning \Vpim{desc} into an Earley set \Ves{j},
then \Vpim{anc} is said to be the \dfn{direct ancestor}
of \Vpim{desc},
and \Vpim{desc} is said to be the \dfn{direct descendant}
of \Vpim{anc}.
if \Vpim{desc} is already a member of \Ves{j}.
\Vpim{anc}
becomes a direct ancestor of \Vpim{desc},
and \Vpim{desc} becomes a direct descendant of \Vpim{anc}.

An \dfn{ancestor} of \Vpim{p} is recursively defined as
\begin{itemize}
\item \Vpim{p} itself, as its own \dfn{trivial ancestor};
\item any direct ancestor of \Vpim{p}; and
\item any direct ancestor of an ancestor of \Vpim{p}.
\end{itemize}
Similarly,
An \dfn{descendant} of \Vpim{p} is defined to be
\Vpim{p} itself, as itw own \dfn{trivial descendant};
any direct descendant of \Vpim{p}; and
any direct descendant of a descendant of \Vpim{p}.

For each operation, there are at most two direct ancestors.
We follow \cite{AH2002} in calling one the ``predecessor''
and the other the ``cause''.
This terminology makes sense in some operations and,
for consistency, we stretch it to cover all cases.
If the predecessor or the cause is not applicable
to a particular operation, we represent it as a
$\Lambda$ value.

The \Marpa{} implementation tracks the ancestry of
PIM's.
The reasons are tracked in \dfn{link pairs},
and are essential for efficient evaluation.
There is at most one link pair for every attempt to union a
PIM into an Earley set.

A link pair
is a duple of predecessor and cause:
\[
[ \var{predecessor}, \var{cause} ]
\]
The elements of a link pair are implemented
as pointers.\footnote{
The concept of link pairs was present
in
\cite{Earley1968} and
\cite{Earley1970},
but \Marpa{} owes much to the
careful elaboration of the idea in
\cite{AH2002}.
}
The pointers are used as ``links''.
to other data.
$\Lambda$ elements are implemented
as a null pointer.

A link pair has type \type{LP}.
The set of link pairs of a PIM
can be written
\LinkPairs{\Vpim{pim}},
or as
\LinkPairs{\Veim{eim}}, or
\LinkPairs{\Vlim{lim}},
for the PIM subtypes.

\section{The \Marpa{} algorithm}
\label{sec:algorithm}

For the purpose of defining the \Marpa{} algorithm,
let $[\Vint{g}, \Vstr{w}]$ be a parse.
and let \Vesset{esets} be the Earley table.

We will first introduce the various ``operations''
of the \Earley{} and \Marpa{} algorithms.
These operations union PIM's in Earley sets.

\subsection{Common prediction logic}
\label{sec:common-predict}

Let \var{PredictDR1} be the relation defined by the set
\begin{equation*}
\left\lbrace
\begin{gathered}
\left[ \Vdr{cause}, \dr{\left[ \Vrule{predicted}, 0 \right]} \right] : \\
  \Postdot{\var{cause}} \neq \Lambda \\
\land \; \LHS{\var{predicted}} = \Postdot{\var{cause}}.
\end{gathered}
\quad \right\rbrace
\end{equation*}
Let \var{PredictDR} be the transitive closure of
\var{PredictDR1}:
\begin{equation*}
\var{PredictDR}(\Vint{g}) \defined \var{PredictDR1}(\Vint{g})^\ast.
\end{equation*}
The set of predictions
indirectly caused by \Veim{cause}
is
\begin{align}
& \var{Predict}(\Veim{cause}@\Vloc{j} ) \defined \\
\nonumber
& \quad \left\lbrace
\begin{gathered}
\left[ \Vdr{predict}, \Vloc{j} \right]_\type{EIM} : \\
[ \DR{\var{cause}}, \Vdr{predict} ] \in \var{PredictDR}(\Vint{g}).
\end{gathered}
\right\rbrace
\end{align}

\begin{observation}[Ancestry]
\label{obs:common-predict-ancestry}
Predictions have predecessors, but no causes.
The implementation does not record link pairs for predictions.

Tracking link pairs for a prediction would be cumbersome,
because, even in an unambiguous grammar,
each prediction can have many reasons to be in an Earley
set.
For a given prediction,
\begin{align}
\label{eq:common-predict-ancestry-10}
& \LinkPairs{
\left[
  \left[\Vsym{lhs} \de \mydot \Vstr{rhs}\right],
  \Vorig{j}
\right]} =
\\ \nonumber
& \qquad
\left\lbrace
  \begin{gathered}
  \left[
    \left[ \Vdr{anc}, \Vorig{anc} \right]@\Vloc{j},
    \Lambda
  \right]_\type{LP} : \\
  \eim{\left[ \Vdr{anc}, \Vorig{anc} \right]} \in \Ves{j} \\
  \land \;\; \Postdot{\Vdr{anc}} = \Vsym{lhs}.
  \end{gathered}
\right\rbrace
\end{align}
The computation of
\Eref{eq:common-predict-ancestry-10}
could be sped up using
the transition table \Sref{sec:transition-tables} for \Ves{j}.
$\bigstar$
\end{observation}

\begin{observation}[Complexity]
\TODO{}
$\bigstar$
\end{observation}

\subsection{Initializer operation}
\label{sec:initializer}

Recall from \Eref{eq:accept-rule} the accept rule of \Vint{g},
\Vrule{accept},
and from \Sref{sec:common-predict}
the definition of \var{Predict}.
Then
\begin{equation}
\label{eq:def-initializer}
\begin{gathered}
\var{Initializer}() \defined
\left\lbrace \;
\Veim{start}
\; \right\rbrace \cup
\var{Predict}(\Veim{start}, (0)_\type{loc})
\\
\text{where $\Veim{start} = \left[ [ \Vrule{accept}, 0 ], 0 \right]$.}
\end{gathered}
\end{equation}

\begin{observation}[Ancestry]
\label{obs:initializer-ancestry}
The \Marpa{} implementation does not keep
any link pairs for any PIM in Earley set 0.
\Veim{start} does not have any non-trivial ancestor,
so that a link pair does not exist for it,
even conceptually.
Link pairs
for the predictions in Earley set 0
are as described in \Obref{obs:common-predict-ancestry}.
$\bigstar$
\end{observation}

\begin{observation}[Complexity]
\TODO{}
$\bigstar$
\end{observation}

\begin{observation}[\Earley{}'s Earley set 0]
\label{obs:earley-set-0}
\Marpa{}'s \var{Initialization} operation has the same effect as the construction
of Earley set 0 by \Earley{}.
$\bigstar$
\end{observation}

\subsection{Scanner operation}
\label{def:scanner}

\begin{align}
\label{eq:def-scanner}
& \var{Scanner}(\Ves{i}) \defined
\\
\nonumber
& \qquad \left\lbrace
\begin{gathered}
\left[
  \Next{\Vdr{cuz}},
  \Vloc{origin}
\right] :
\\
\Postdot{\Vdr{cuz}} = \Vstr{w}[\Vloc{i} \subtract 1] \\
\land \left[ \Vdr{cuz}, \Vloc{origin} \right]
\in \Ves{(\Vloc{i}\subtract 1)}.
\end{gathered}
\right\rbrace
\end{align}

\begin{observation}[Ancestry]
\label{obs:scanner-ancestry}
The \Marpa{} implementation tracks all link pairs for scanned EIM's.
In an unambiguous parse, a scanned EIM will always have
exactly one link pair.
In an ambiguous parse, a scanned EIM will have one or more link pairs.

The predecessor of a scanned EIM is an incomplete EIM.
The cause of a scanned EIM is a token,
which conceptually is
the duple
\[
  \left[ \Vsym{token}, \var{token-value} \right],
\]
where \Vsym{token} is a token symbol,
and \var{token-value} is the token's value.

The implementation of the token is a pointer to a structure
containing a pointer to the token symbol,
and a pointer to the token value,
which is a value of arbitrary type.\footnote{
In C language,
these can be implemented as so-called ``void pointers''.}
The token value is irrelevant to parsing --- it exists
in order to be passed on to the semantics.
\end{observation}

\begin{observation}[Complexity]
\TODO{}
\end{observation}

\begin{observation}[Original Earley algorithm: Scanner Operation]
\label{obs:earley-scanner-operation}
\Marpa{}'s scanner operation has the same effect as the ``Scanner''
operation of \Earley{}\cite{Earley1970}.
\end{observation}

\subsection{Reducer operation}
\label{def:reducer}

\begin{align}
\label{eq:def-reducer-10}
& \var{Reducer}(\Ves{i}) \defined
\\ \nonumber \displaybreak[0]
& \qquad \var{Earley-reducer}(\Ves{i}) \cup \var{Leo-reducer}(\Ves{i})
\end{align}
where
\begin{align}
\label{eq:def-reducer-20}
& \var{Earley-reducer}(\Ves{i}) \defined
\\ \nonumber \displaybreak[0]
& \qquad
\left\lbrace
\begin{gathered}
  \left [ \Next{\Veim{pred}}, \Origin{\var{pred}} \right] @ \Ves{i} :
  \\
  \begin{gathered}
    \Veim{cuz} \in \Ves{i} \\
    \land \; \Veim{pred} \in \Origin{\var{cuz}} \\
    \land \; \Postdot{\var{pred}} = \LHS{\var{cuz}} \\
    \land \; \neg \; \var{Blocker}(\var{cuz})
  \end{gathered}
\end{gathered}
\right\rbrace,
\end{align}
%
\begin{align}
\label{eq:def-reducer-23}
& \var{Leo-reducer}(\Ves{i}) \defined
\\ \nonumber \displaybreak[0]
& \qquad
\left\lbrace
\begin{gathered}
  \left[
    \DR{\var{blocker}}, \Origin{\var{blocker}}
  \right]_\var{EIM}
  :
\\
    \Veim{cuz} \in \Ves{i}
\\
    \land \; \var{Blocker}(\var{cuz})
\end{gathered}
\right\rbrace,
\end{align}
and
\begin{align}
\label{eq:def-reducer-24}
& \var{Blocker}(\Veim{cuz}) \defined
\\ \nonumber \displaybreak[0]
& \qquad \qquad
   \begin{gathered}
    \exists \; \Vlim{blocker} \in \Origin{\var{cuz}} : \\
    \Transition{\var{blocker}} = \LHS{\var{cuz}}
   \end{gathered}
\end{align}

\begin{observation}[Ancestry]
\label{obs:reducer-ancestry}
The \Marpa{} implementation tracks all link pairs for reducer-generated
EIM's.
Completer-generated EIM's will have a link pair for every attempt to create them.
In an unambiguous parse, a reducer-generated EIM will always have exactly one link pair.
In an ambiguous parse, a reducer-generated EIM will have one or more link pairs.

The cause of a reducer-generated EIM will always be
a completed EIM.
The predecessor of a reducer-generated EIM varies depending on whether the attempt
to create the reducer-generated EIM was a \var{Leo-reducer} operation
or an \var{Earley-reducer} operation.
For an \var{Earley-reducer} operation,
the predecessor will be an incomplete EIM.
For a \var{leo-reducer} operation,
the predecessor will be either a LIM,
or $\Lambda$.
$\bigstar$
\end{observation}

\begin{observation}[Complexity]
\TODO{}
\end{observation}

\begin{observation}[Original Earley algorithm: Completer Operation]
\label{obs:earley-completer-operation}
When there are no LIM's in \Ves{predecessor}
\Marpa{}'s \var{Reducer} operation has the same effect as the ``Completer''
operation of \Earley{}\cite{Earley1970}.\footnote{
For all other \Marpa{} operations we have used the name of its
equivalent in \Earley{}.
It is convenient to use a different name
for \Marpa{}'s \var{Reducer} operation,
however,
because it differs in effect from the \Earley{}
``Completer'' operation,
and because the cognates of ``complete'' are
overloaded in confusing ways.
For example, the result of an \Earley{} ``Completer'' operation
is not necessarily an EIM completion.
}
Put another way, \Marpa{}'s \var{Reducer} operation
is the same as the Completer operation of \Earley{},
if, for all \Vdr{top}, \Vsym{completed}, \Vloc{top},
\[
\bigl([ \Vdr{top}, \Vsym{completed}, \Vloc{top} ]\bigr)_\var{LIM}
\in \Ves{predecessor}
\]
is false in
\Eref{eq:def-reducer-20}
and
\Eref{eq:def-reducer-23}.
In this case,
we will always have
\[
\var{Leo-reducer}(\Ves{i}) = \emptyset
\]
so that we may also the \Earley{} completer is equivalent
in effect to $\var{Earley-reducer}(\Ves{i})$.
\end{observation}

\begin{observation}[Summary of \Earley{} Completer Operation]
\label{obs:earley-completer-summary}
In what follows,
we will find the following summary of the original, \Earley{} completer
operation useful.
If
\begin{equation}
\label{eq:earley-completer-summary-1}
\myparbox{$\Veim{cause}@\Vloc{current}$ is a completion,} \\
\end{equation}
\begin{equation}
\label{eq:earley-completer-summary-2}
\myparbox{$\var{pred} @ (\Origin{\var{cause}})$ is an incompletion, and} \\
\end{equation}
\begin{equation}
\label{eq:earley-completer-summary-3}
\myparbox{$\Postdot{\var{pred}} = \LHS{\var{cause}}$,}
\end{equation}
then we union
\begin{equation}
\label{eq:earley-completer-summary-4}
\myparbox{%
$\left[ \Next{\DR{\var{pred}}}, \Origin{\var{pred}} \right]@\Vloc{current}$
}
\end{equation}
into the Earley table with the link pair
\begin{equation}
\label{eq:earley-completer-summary-5}
[ \var{pred}, \var{cause} ]. \quad \bigstar
\end{equation}
\end{observation}

\subsection{Predictor operation}
\label{def:prediction}

\begin{equation}
\label{eq:def-predictor}
\var{Predictor}(\Ves{i}) \;\; \defined \;\;
\smashoperator[r]{\mathop{\bigcup}_{\textstyle \var{eim}\in\Ves{i}}}
\;\; \var{Predict}(\Veim{eim}, \Vloc{i})
\end{equation}

\begin{observation}[Ancestry]
\label{obs:predictor-ancestry}
The \Marpa{} implementation does not track
link pairs
for predictions.
For details, see \Obref{obs:common-predict-ancestry}.
$\bigstar$
\end{observation}

\begin{observation}[Complexity]
\TODO{}
$\bigstar$
\end{observation}

\begin{observation}[Original Earley algorithm: Predictor Operation]
\label{obs:earley-predictor-operation}
\Marpa{}'s predictor operation has the same effect as the ``Predictor''
operation of \Earley{}\cite{Earley1970}.
$\bigstar$
\end{observation}

\subsection{Memoizer operation}
\label{def:memoizer}

Let
\begin{align}
\label{eq:memoizer-operation-10}
& \LeoEligible{\Ves{i}} \defined \\
\nonumber
& \qquad \left\lbrace \;\;
\begin{gathered}
\Veim{eligible} : \\
\Veim{eligible} \in \LeoUnique{\Ves{i}} \\
\land \; \RightRecursive{\Veim{eligible}}
\end{gathered}
\;\; \right\rbrace,
\end{align}
%
\begin{align}
\label{eq:memoizer-operation-20}
& \mymathop{Start-Leo-Chain}({\Ves{i}}) \defined \\
\nonumber
& \qquad \left\lbrace \;\;
\begin{gathered}
\left[
  \begin{gathered}
  \DR{\var{source}},
  \Postdot{\Veim{source}}, \\
  \Origin{\var{source}}
  \end{gathered}
\right]_\type{LIM} @ \Ves{i}
: \\
\Veim{source} \in \LeoEligible{\Ves{i}} \\
\land \; \LIMPredecessor{\Veim{source}} = \Lambda
\end{gathered}
\;\; \right\rbrace,
\end{align}
and
\begin{align}
\label{eq:memoizer-operation-30}
& \mymathop{Extend-Leo-Chain}({\Ves{i}}) \defined \\
\nonumber
& \qquad \left\lbrace \;\;
\begin{gathered}
\left[
  \begin{gathered}
  \DR{\var{pred}}, \Postdot{\Veim{source}}, \\
  \Origin{\var{pred}}
  \end{gathered}
\right]_\type{LIM} @ \Ves{i}
: \\
\Veim{source} \in \LeoEligible{\Ves{i}} \\
\land \; \Vlim{pred} = \LIMPredecessor{\Veim{source}}
\end{gathered}
\;\; \right\rbrace .
\end{align}
Then
\begin{align}
\label{eq:memoizer-operation-40}
& \var{Memoizer}(\Ves{i}) \defined \\
\nonumber
& \qquad \qquad \var{Start-Leo-Chain}(\Ves{i})
\cup
\var{Extend-Leo-Chain}(\Ves{i})
\end{align}

\begin{observation}[Ancestry]
\label{obs:memoizer-ancestry}
The \Marpa{} implementation tracks the link pairs
for the LIM's.
LIMs memoize Leo chains,
which are deterministic, so that
a LIM always has exactly one link pair:
\[
\left[ \LIMPredecessor{\Veim{source}}, \Veim{source} \right],
\]
where \var{source} is as in
\Eref{eq:memoizer-operation-20} and
\Eref{eq:memoizer-operation-30}.

In the link pair for a LIM, call it \Vlim{$\ell$},
the ``cause''
is more often called the \dfn{source} of the LIM,
\begin{equation}
\label{eq:def-source}
\Source{\Vlim{$\ell$}} \defined \Veim{source},
\end{equation}
the ``predecessor'' is more often called the \dfn{previous link} of the LIM,
and
\begin{equation}
\label{eq:def-previous-link}
\PreviousLink{\Vlim{$\ell$}} \defined \LIMPredecessor{\Veim{source}}.
\end{equation}
Because every LIM has exactly one link pair,
\var{PreviousLink} and \var{Source} are functions.

The source of the link pair of a LIM is the EIM which that LIM,
in a intuitive sense, ``replaces''.
The source EIM will always be a penult in the same Earley set as the LIM.
The previous link of the link pair of a LIM is either
$\Lambda$ or another LIM.
$\bigstar$
\end{observation}

\begin{observation}[Complexity]
\TODO{}
$\bigstar$
\end{observation}

\begin{observation}[Original Earley algorithm: Memorizer Operation]
\label{obs:earley-memoizer-operation}
\Marpa{}'s \var{Memoizer} operation does not exist in \Earley{}\cite{Earley1970}.
$\bigstar$
\end{observation}

\subsection{Top level of \Marpa{}}

The top-level view of \Marpa{} is
shown in \Aref{alg:marpa}.
\begin{algorithm}[tb]
\caption{The \Marpa{} Algorithm}
\label{alg:marpa}
\begin{algorithmic}[1]
\Procedure{Main}{}
\State $\Ves{(0)} \gets \Call{Initializer}{{}}$
\For{ $\Ves{i}, 1 \le \var{i} \le \Vsize{w}$ }
\State \Comment At this point, $\Ves{x}$ is complete, for $0 \le \var{x} < \var{i}$
\State $\Ves{i} \gets \Call{Scanner}{\Ves{i}}$
\If{$\size{\Ves{i}} = 0$}
\State return \Comment \Cw{} will be rejected
\EndIf
\State $\Ves{i} \gets \Ves{i} \cup \Call{Completer}{\Ves{i}}$
\State $\Ves{i} \gets \Ves{i} \cup \Call{Predictor}{\Ves{i}}$
\label{alg:marpa-line-80}
\State $\Ves{i} \gets \Ves{i} \cup \Call{Memoizer}{\Ves{i}}$
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{observation}[Original Earley Algorithm: Top level]
If line \ref{alg:marpa-line-80} is removed,
\Aref{alg:marpa} has the same effect as \Earley{}\cite{Earley1970}.
This can be seen from
\Aref{alg:marpa},
\Obref{obs:earley-set-0},
\Obref{obs:earley-scanner-operation},
\Obref{obs:earley-completer-operation},
\Obref{obs:earley-predictor-operation},
and
\Obref{obs:earley-memoizer-operation}.
\end{observation}

\subsection{Summary}

\begin{observation}[\Marpa{} operation by type of result]
\label{obs:marpa-operation-by-result}
Let \Vpim{rez} be the result of a \Marpa{} operation.
Every PIM is the result of exactly \Marpa{} operation,
as follows:
\begin{itemize}
\item $\Current{\Vpim{rez}} = 0$
if and only if \Vpim{rez} is the result of the
\Marpa{} \var{Initializer} operation.
\item \Vpim{rez} is an EIM,
$\Predot{\Veim{rez}} = \Lambda$, and
$\Current{\var{rez}} > 0$,
if and only if
\var{rez} is the result of a
\Marpa{} \var{Predictor} operation.
\item \Vpim{rez} is an EIM,
$\Predot{\Veim{rez}} \in \Term{\var{g}}$, and
$\Current{\var{rez}} > 0$,
if and only if
\var{rez} is the result of a
\Marpa{} \var{Scanner} operation.
\item \Vpim{rez} is an EIM,
$\Predot{\Veim{rez}} \in \NT{\var{g}}$, and
$\Current{\var{rez}} > 0$
if and only if \var{rez} is the result of a
\Marpa{} \var{Reducer} operation.
\item \Vpim{rez} is a LIM
if and only if
\var{rez} is the result of a
\Marpa{} \var{Memoizer} operation.
\end{itemize}
This observation follows from
\Eref{eq:def-initializer},
\Eref{eq:def-scanner},
\Eref{eq:def-reducer-10}--\Eref{eq:def-reducer-23},
\Eref{eq:def-predictor}, and
\Eref{eq:memoizer-operation-10}--\Eref{eq:memoizer-operation-40}.
\end{observation}

\begin{observation}[\Earley{} operation by type of result]
\label{obs:earley-operation-by-result}
The operation of the original \Earley{} algorithm can be determined
from the result as described in
\Obref{obs:marpa-operation-by-result},
except
in \Earley{} there is no \var{Memoizer} operation,
and there are no LIM's.
This follows from
\Obref{obs:earley-set-0},
\Obref{obs:earley-scanner-operation},
\Obref{obs:earley-completer-operation},
\Obref{obs:earley-predictor-operation},
\Obref{obs:earley-memoizer-operation}, and
\Obref{obs:marpa-operation-by-result}.
\end{observation}

\begin{observation}[\Marpa{} operation by direct ancestor]
\label{obs:marpa-operation-by-direct-ancestor}
If a completion is a direct ancestor of a \Marpa{}
operation, that operation is the \var{Reducer} operation.
If a token is a direct ancestor of a \Marpa{}
operation, that operation is the \var{Scanner} operation.
This observation follows from
\Eref{eq:def-initializer},
\Eref{eq:def-scanner},
\Eref{eq:def-reducer-10}--\Eref{eq:def-reducer-23},
\Eref{eq:def-predictor}, and
\Eref{eq:memoizer-operation-10}--\Eref{eq:memoizer-operation-40}.
\end{observation}

\begin{observation}[\Earley{} operation by direct ancestor]
\label{obs:earley-operation-by-direct-ancestor}
If a completion is a direct ancestor of an \Earley{}
operation, that operation is the Completer operation.
If a token is a direct ancestor of a \Marpa{}
operation, that operation is the Scanner operation.
This observation follows from
\Obref{obs:earley-set-0},
\Obref{obs:earley-scanner-operation},
\Obref{obs:earley-completer-operation},
\Obref{obs:earley-predictor-operation} and
\Obref{obs:earley-memoizer-operation}.
\end{observation}

\TODO{complexity observations}

\section{Ambiguity}

\TODO{Discuss the sources of this approach.}

We say that a parse $[ \Vcfg{g}, \Vstr{w1} ]$
is ambiguous, if there is more than one derivation tree for it.
Most derivation trees allow more than one derivation,
but for every derivation tree there is exactly one rightmost
derivation.
So a parse is ambiguous if and only if there is more than
one rightmost derivation tree for \Vstr{w1}.
A grammar is ambiguous if and only if there is some input
for which the parse is ambiguous.

We call a string of terminals derived from a sentential form
its \dfn{frontier}.
If a parse is unambiguous, the frontier of a sentential form
form derives is unique, so that
we can write the frontier of
\Vstr{sf} as $\Vfrontier{sf}$,
where we will have that
$\Vfrontier{sf} \in \Term{\var{g}}^\ast$
for the current choice of \Vcfg{g}.

We recognize two kinds of ambiguity:
horizontal and vertical.

\begin{definition}[Horizontal Ambiguity]
\label{def:h-ambig}
Let $\var{p} = [\Vcfg{g}, \Vstr{w}]$ be a parse,
and let \var{gallia} be a triple $[\Vrule{r}, \var{pos1}, \var{pos2}]$,
where $0 \le \var{pos1} < \var{pos2} \le \size{\RHS{\var{r}}}$,
indicating a rule whose RHS is partitioned
into three parts.
The 3-partitioned rule may be written as
\begin{equation}
\Vrule{r} = [ \Vsym{lhs} \de \Vstr{rhs1} \Vstr{rhs2} \Vstr{rhs3} ],
\end{equation}
where
\begin{gather}
\Vstr{rhs1} = (\RHS{\var{r}})[0 \ldots \var{pos1}\subtract 1], \\
\Vstr{rhs2} = (\RHS{\var{r}})[\var{pos1} \ldots \var{pos2}\subtract 1], \;\; \text{and} \\
\Vstr{rhs3} = (\RHS{\var{r}})[\var{pos2} \ldots \size{\RHS{\var{r}}}\subtract 1].
\end{gather}
A \dfn{horizontal ambiguity} is a 5-tuple,
\begin{equation}
\begin{gathered}
[ \var{gallia}, \Vloc{origin}, \Vloc{overlap1}
\Vloc{overlap2}, \Vloc{dot} ]
\end{gathered}
\end{equation}
such that
\begin{align}
\label{eq:def-h-ambig-21a}
& \qquad \Accept{\var{g}} \\
\label{eq:def-h-ambig-21b}
& \destar \Vstr{prefix} \Vsym{lhs} \Vstr{suffix} \\
\label{eq:def-h-ambig-21c}
& \derives \Vstr{prefix} \Vstr{rhs1} \Vstr{rhs2} \Vstr{rhs3} \Vstr{suffix} \\
\label{eq:def-h-ambig-21c2}
& \destar \Vstr{prefix} \Vstr{rhs1} \Vstr{rhs2} \cat \boldRangeSizeDecr{w}{dot}{w} \\
\label{eq:def-h-ambig-21d}
& \destar \Vstr{prefix} \Vstr{rhs1} \cat \boldRangeSizeDecr{w}{overlap1}{w} \\
\label{eq:def-h-ambig-21e}
& \destar \Vstr{prefix} \cat \boldRangeSizeDecr{w}{origin}{w} \\
\label{eq:def-h-ambig-21f}
& \destar \Vterm{w},
\end{align}
and also such that
\begin{gather}
\label{eq:def-h-ambig-30a}
\Vstr{rhs2} \destar \boldRangeDecr{w}{overlap2}{dot}
\;\; \text{and} \\
\label{eq:def-h-ambig-30b}
\var{overlap1} \neq \var{overlap2}.
\end{gather}

\var{gallia} is called the \dfn{partitioned rule},
\Vloc{orig} the \dfn{origin}, and
\Vloc{dot} the \dfn{dot position}
of the horizontal ambiguity.
The duple $[\var{overlap1}, \var{overlap2}]$ is
called its \dfn{overlap}.
The string
\[
\var{w}[\var{overlap1}\ldots(\var{overlap2}\subtract 1)]
\]
is also called its \dfn{overlap}.

If there is a horizontal ambiguity for a parse \var{p},
then
\var{p} is a horizontally ambiguous parse.
A grammar is \dfn{horizontally ambiguous} if and only if it has a
horizontally ambiguous parse. $\bigstar$
\end{definition}

Note that if $\Vstr{rhs1} = \epsilon$, that
$\var{overlap1} = \var{overlap2}$.
Among other things, this implies that a unit rule
cannot be the rule of a horizontally ambiguity.

\begin{theorem}
\label{th:h-ambig-g}
If a parse is horizontally ambiguous,
then its grammar is ambiguous.
\end{theorem}

\begin{proof}
Let $\var{p} = [\Vcfg{g}, \Vstr{w}]$ be a parse.
We consider first the case where \var{p} contains a
cycle.
If \var{p} contains a cycle, it and its grammar are
ambiguous, which shows the theorem directly.
Therefore, for the rest of this proof, we assume that
\var{p} is free of cycles.

We assume for a reductio that
\var{p} is unambiguous,
but contains a horizontal ambiguity as defined in \Dfref{def:h-ambig}.
Since the parse is unambiguous, we can write
\[
\Vstr{suffix} \destar \Vfrontier{suffix},
\]
where
$\Vfrontier{suffix} \in \Term{\var{g}}^\ast$.
Then we can rewrite
\Eref{eq:def-h-ambig-21a}--\Eref{eq:def-h-ambig-21f}
as the following right derivation
\begin{align}
\label{eq:th-h-ambig-10-1-a}
& \qquad \Accept{\var{g}} \\
& \xderives{R\ast} \Vstr{prefix} \Vsym{lhs} \Vstr{suffix} \\
& \xderives{R\ast} \Vstr{prefix} \Vsym{lhs} \Vfrontier{suffix} \\
& \xderives{R} \Vstr{prefix} \Vstr{rhs1} \Vstr{rhs2} \Vstr{rhs3} \Vfrontier{suffix} \\
& \xderives{R\ast} \Vstr{prefix} \Vstr{rhs1} \Vstr{rhs2} \cat \boldRangeSizeDecr{w}{current}{w} \\
\label{eq:th-h-ambig-10-1-d}
& \xderives{R\ast} \Vstr{prefix} \Vstr{rhs1} \mathop{\spadesuit} \boldRangeSizeDecr{w}{overlap1}{w} \\
\label{eq:th-h-ambig-10-1-e}
& \xderives{R\ast} \Vstr{prefix} \cat \boldRangeSizeDecr{w}{origin}{w} \\
\label{eq:th-h-ambig-10-1-f}
& \xderives{R\ast} \Vterm{w}.
\end{align}
We have replaced the concatenation operator
in \Eref{eq:th-h-ambig-10-1-d}
with the $\spadesuit$ symbol
for convenience in referring to that location.

Since the parse is assumed to be unambiguous, the right derivation must be
unique.
The step which derives \Eref{eq:th-h-ambig-10-1-d} in unique within its derivation
as the first to produce a sentential form
with no non-terminals to the right of $\spadesuit$,
but before the elimination of any non-terminals to the left of $\spadesuit$.
Therefore
\Eref{eq:th-h-ambig-10-1-d}
is a unique step of a unique derivation.

Note that no assumption is made that any of
\Vstr{prefix}, \Vstr{rhs1}, \Vstr{rhs2}, \Vstr{rhs3},
and \Vstr{suffix} contain
non-terminals.
If they do not, the derivations of terminal strings are trivial,
and our claims about the presence
and elimination of non-terminals are trivially or vacuously true.
For example, if \Vstr{rhs1} contains no non-terminals,
then the step from
\Eref{eq:th-h-ambig-10-1-d} to
\Eref{eq:th-h-ambig-10-1-e}
is trivial and the sentential forms of
\Eref{eq:th-h-ambig-10-1-d} and
\Eref{eq:th-h-ambig-10-1-e}
are identical,
so that our claim that
\Eref{eq:th-h-ambig-10-1-d} occurs prior to the elimination
of any non-terminals in \Vstr{rhs1} is vacuously true.

We go back
to \Dfref{def:h-ambig}, and write a second right derivation for the parse.
Because our parse is unambiguous, this derivation should be equivalent to the
derivation from
\Eref{eq:th-h-ambig-10-1-a}
to
\Eref{eq:th-h-ambig-10-1-f}.
\begin{align}
\label{eq:th-h-ambig-10-2-a}
& \qquad \Accept{\var{g}} \\
& \xderives{R\ast} \Vstr{prefix} \Vsym{lhs} \Vstr{suffix} \\
\label{eq:th-h-ambig-10-2-d}
& \xderives{R\ast} \Vstr{prefix} \Vstr{rhs1} \mathop{\spadesuit} \boldRangeSizeDecr{w}{overlap2}{w} \\
\label{eq:th-h-ambig-10-2-f}
& \xderives{R\ast} \Vterm{w}.
\end{align}
The step which derives
the sentential form of \Eref{eq:th-h-ambig-10-2-d}
also the first to produce a sentential form
with no non-terminals to the right of $\spadesuit$,
but before the elimination of any non-terminals to the left of $\spadesuit$ and,
since they result from the same steps of identical derivations,
the two sentential forms \Eref{eq:th-h-ambig-10-1-d}
and \Eref{eq:th-h-ambig-10-2-d}
should be identical.
But $\var{overlap1} \neq \var{overlap2}$
\Eref{eq:def-h-ambig-30b}
and therefore
\begin{equation}
\label{eq:th-h-ambig-50}
\begin{gathered}
\Vstr{prefix} \Vstr{rhs1} \mathop{\spadesuit} \boldRangeSizeDecr{w}{overlap1}{w} \\
\neq \Vstr{prefix} \Vstr{rhs1} \mathop{\spadesuit} \boldRangeSizeDecr{w}{overlap2}{w}
\end{gathered}
\end{equation}
From
\Eref{eq:th-h-ambig-50}
we see that the
derivation from
\Eref{eq:th-h-ambig-10-1-a}
to
\Eref{eq:th-h-ambig-10-1-f}
is not the same as the derivation from
\Eref{eq:th-h-ambig-10-2-a} to
\Eref{eq:th-h-ambig-10-2-f}.
Since these are both right derviations,
the parse must be ambiguous, which shows
the reductio.
Since the parse is ambiguous, its grammar is ambiguous.
This shows the theorem.
\end{proof}

\begin{definition}[Vertical Ambiguity]
\label{def:v-ambig}
Let $[\Vcfg{g}, \Vstr{w}]$ be a parse.
A \dfn{vertical ambiguity} is a 4-tuple,
\begin{equation}
\label{eq:def-v-ambig-10a}
\left[ \Vrule{r1}, \Vrule{r2}, \Vloc{origin}, \Vloc{dot} \right],
\end{equation}
such that
\begin{gather}
\label{eq:def-v-ambig-15a}
\var{r1} = [ \var{lhs} \de \Vstr{rhs1} ], \\
\label{eq:def-v-ambig-15b}
\var{r2} = [ \var{lhs} \de \Vstr{rhs2} ], \\
\label{eq:def-v-ambig-15c}
\var{rhs1} \neq \var{rhs2}, \\
\label{eq:def-v-ambig-15d}
\var{rhs2} \destar \boldRangeDecr{w}{origin}{dot},
\end{gather}
and
\begin{align}
\label{eq:def-v-ambig-20a}
& \qquad \Accept{\var{g}} \\
\label{eq:def-v-ambig-20b}
& \destar \Vstr{prefix} \Vsym{lhs} \cat \boldRangeSizeDecr{w}{dot}{w} \\
\label{eq:def-v-ambig-20c}
& \derives \Vstr{prefix} \Vstr{rhs1} \cat \boldRangeSizeDecr{w}{dot}{w} \\
\label{eq:def-v-ambig-20d}
& \destar \;
\begin{aligned}
& \Vstr{prefix} \cat \boldRangeDecr{w}{origin}{dot} \\
& \qquad \cat \boldRangeSizeDecr{w}{dot}{w}
\end{aligned}
\\
\label{eq:def-v-ambig-20e}
& = \Vstr{prefix} \cat \boldRangeSizeDecr{w}{origin}{w} \\
\label{eq:def-v-ambig-20f}
& \destar \Vterm{w}.
\end{align}

\var{r1} and \var{r2} are the \dfn{rules}
of the vertical ambiguity,
\Vloc{orig} is its \dfn{origin}, and
\Vloc{dot} is its \dfn{dot position}.
A grammar is \dfn{vertically ambiguous} if and only if it has a
vertically ambiguous parse. $\bigstar$

If there is a vertical ambiguity for a parse \var{p},
then
\var{p} is a vertically ambiguous parse.
A grammar is \dfn{vertically ambiguous} if and only if it has a
vertically ambiguous parse. $\bigstar$
\end{definition}

\begin{theorem}
\label{th:v-ambig-g}
If a parse is vertically ambiguous,
then its grammar is ambiguous.
\end{theorem}

\begin{proof}
Let $\var{p} = [\Vcfg{g}, \Vstr{w}]$ be a parse.
We consider first the case where \var{p} contains a
cycle.
If \var{p} contains a cycle, it and its grammar are
ambiguous, which shows the theorem directly.
Therefore, for the rest of this proof, we assume that
\var{p} is free of cycles.

We assume for a reductio that
\var{p} is unambiguous,
but has a vertical ambiguity as defined in \Dfref{def:v-ambig}.
Noting that
\Eref{eq:def-v-ambig-20a} through
\Eref{eq:def-v-ambig-20f}
in \Dfref{def:v-ambig} is a right derivation,
we can write
\begin{align}
\label{eq:th-v-ambig-10-1-a}
& \qquad \Accept{\var{g}} \\
\label{eq:th-v-ambig-10-1-b}
& \xderives{R\ast} \Vstr{prefix} \Vsym{lhs} \cat \boldRangeSizeDecr{w}{(dot+1)}{w} \\
\label{eq:th-v-ambig-10-1-c}
& \xderives{R} \Vstr{prefix} \Vstr{rhs1} \cat \boldRangeSizeDecr{w}{(dot+1)}{w} \\
\label{eq:th-v-ambig-10-1-d}
& \xderives{R\ast} \Vstr{prefix} \cat \boldRangeSizeDecr{w}{origin}{w} \\
\label{eq:th-v-ambig-10-1-e}
& \xderives{R\ast} \Vterm{w}.
\end{align}
We also have a right derivation
\begin{align}
\label{eq:th-v-ambig-10-2-a}
& \qquad \Accept{\var{g}} \\
\label{eq:th-v-ambig-10-2-b}
& \xderives{R\ast} \Vstr{prefix} \Vsym{lhs} \cat \boldRangeSizeDecr{w}{(dot+1)}{w} \\
\label{eq:th-v-ambig-10-2-c}
& \xderives{R} \Vstr{prefix} \Vstr{rhs2} \cat \boldRangeSizeDecr{w}{(dot+1)}{w} \\
\label{eq:th-v-ambig-10-2-d}
& \xderives{R\ast} \Vstr{prefix} \cat \boldRangeSizeDecr{w}{origin}{w} \\
\label{eq:th-v-ambig-10-2-e}
& \xderives{R\ast} \Vterm{w}.
\end{align}
by
\begin{itemize}
\item letting the derivation of
\Eref{eq:th-v-ambig-10-2-b} from
\Eref{eq:th-v-ambig-10-2-a}
be identical to the derivation of
\Eref{eq:th-v-ambig-10-1-b} from
\Eref{eq:th-v-ambig-10-1-a};
\item
using the rule
\Eref{eq:def-v-ambig-15b}
to derive
\Eref{eq:th-v-ambig-10-2-c};
\item
using
\Eref{eq:def-v-ambig-15d}
to derive
\Eref{eq:th-v-ambig-10-2-d};
\item and letting the derivation of
\Eref{eq:th-v-ambig-10-2-e} from
\Eref{eq:th-v-ambig-10-2-d}
be identical to the derivation of
\Eref{eq:th-v-ambig-10-1-e} from
\Eref{eq:th-v-ambig-10-1-d};
\end{itemize}

Since \var{p} is assumed to be unambiguous,
the two right derivations
\Eref{eq:th-v-ambig-10-1-a}--\Eref{eq:th-v-ambig-10-1-e}
and
\Eref{eq:th-v-ambig-10-2-a}--\Eref{eq:th-v-ambig-10-2-e}
must be identical,
and therefore must be a sequence of identical sentential forms.

\Eref{eq:th-v-ambig-10-1-c} and
\Eref{eq:th-v-ambig-10-2-c}
must be sentential forms in the same position of their
identical derivations.
We know this
because both result from the production of the \Vsym{lhs}
when that \Vsym{lhs} is the rightmost non-terminal and
the underived part of sentential form is $\Vstr{prefix}\Vsym{lhs}$.
We know that this the only time that happens because otherwise
\[
\Vstr{prefix}\Vsym{lhs} \deplus \Vstr{prefix}\Vsym{lhs},
\]
in which case \var{p} contains a cycle,
which is contrary to assumption for this case.
Therefore
\Eref{eq:th-v-ambig-10-1-c} and
\Eref{eq:th-v-ambig-10-2-c} are sentential forms after the
same step of identical derivations,
and are therefore equal.

But from the definition of vertical ambiguity,
we have $\var{rhs1} \neq \var{rhs2}$,
so that
the sentential forms at
\Eref{eq:th-v-ambig-10-1-c} and
\Eref{eq:th-v-ambig-10-2-c}
are not equal.
Therefore the two right derivations differ,
and \var{p} is ambiguous,
which shows the reductio.
Since \var{p} is ambiguous, its grammar, \Vcfg{g} must
also be ambiguous.
This shows the theorem.
\end{proof}

\begin{definition}
\label{def:ambig-EIM}
An confirmed EIM is \dfn{ambiguous} if it is of the form
\begin{equation}
\label{eq:ambig-EIM-05}
\left[
[ \var{lhs} \de \Vstr{rhs1} \Vsym{rhs2} \mydot \Vstr{rhs3} ],
\Vloc{origin}
\right] @ \Vloc{dot}
\end{equation}
\Eref{eq:ambig-EIM-05}
and has two distinct and valid link pairs.
Without loss of generality,
the predecessor in the first link pair is
the valid EIM
\begin{equation}
\label{eq:ambig-EIM-10a}
\left[
\begin{gathered}
[ \var{lhs} \de \Vstr{rhs1} \mydot \Vsym{rhs2} \Vstr{rhs3} ], \\
\Vloc{origin}
\end{gathered}
\right] @ \Vloc{overlap1},
\end{equation}
and its cause is the valid EIM
\begin{equation}
\label{eq:ambig-EIM-10b}
\left[
  [ \Vsym{rhs2} \de \Vstr{children1} \mydot ], \var{overlap1}
\right] @ \Vloc{dot}.
\end{equation}
Also without loss of generality,
the predecessor in the second link pair is the valid EIM
\begin{equation}
\label{eq:ambig-EIM-20a}
\left[
\begin{gathered}
[ \var{lhs} \de \Vstr{rhs1} \mydot \Vsym{rhs2} \Vstr{rhs3} ], \\
\Vloc{origin}
\end{gathered}
\right] @ \Vloc{overlap2},
\end{equation}
and the cause of the second link pair is the valid EIM
\begin{equation}
\label{eq:ambig-EIM-20b}
\left[
  [ \Vsym{rhs2} \de \Vstr{children2} \mydot ], \var{overlap2}
\right] @ \Vloc{dot}.
\end{equation}

The EIM is \dfn{vertically ambiguous} if
\begin{equation}
\label{eq:ambig-EIM-02}
\Vloc{overlap1} = \Vloc{overlap2},
\end{equation}
and the EIM is
\dfn{horizontally ambiguous} if
\begin{equation}
\label{eq:ambig-EIM-03}
\Vloc{overlap1} \neq \Vloc{overlap2}. \quad \bigstar
\end{equation}
\end{definition}

\begin{theorem}
\label{th:h-EIM-g-ambig}
If a partial parse has a horizontally ambiguous EIM,
then any full parse consistent with that partial parse is
horizontally ambiguous,
and the grammar shared by the partial and full parse is horizontally ambiguous.
\end{theorem}

\begin{proof}
Assume that,
for some \Vstr{w} and \Vloc{dot},
\begin{equation}
\label{eq:th-h-EIM-g-ambig-15}
\var{partial} = \left[\Vcfg{g}, \boldRangeDecr{w}{0}{dot} \right]
\end{equation}
is a partial parse.
Futher assume that \var{partial}
has a horizontally ambiguous EIM \Dfref{def:ambig-EIM}.

Without loss of generality,
let
\begin{equation}
\label{eq:th-h-EIM-g-ambig-20}
\begin{gathered}
\var{full} = \left[\Vcfg{g}, \Vstr{w} \right], \\
\text{where $\Vstr{w} = \boldRangeDecr{w}{0}{dot} \cat \Vterm{suffix}$} \\
\text{for some \Vterm{suffix}}
\end{gathered}
\end{equation}
be a full parse consistent with \var{partial}.

We proceed by showing that there is a horizontal ambiguity in \var{full}
according to \Dfref{def:h-ambig}.
By assumption for the theorem,
\var{partial} has a horizontally ambiguous EIM according to
\Dfref{def:ambig-EIM}.
Let \Vloc{origin},
\Vloc{overlap1},
\Vloc{overlap2},
\Vloc{dot},
\Vsym{lhs},
\Vsym{rhs1}, and
\Vstr{rhs3}
in \Dfref{def:h-ambig}
be their eponyms in \Dfref{def:ambig-EIM}.
And let
\Vstr{rhs2} in \Dfref{def:h-ambig}
be the string of length 1 whose only character is
\Vsym{rhs2} in \Dfref{def:ambig-EIM}.

In the horizontally ambiguous EIM
let the 3-partitioned rule be
\[
[ \var{lhs} \de \Vstr{rhs1} \Vsym{rhs2} \Vstr{rhs3} ].
\]
from \Dfref{def:ambig-EIM}.
and let \Vloc{origin}, \Vloc{overlap1}, \Vloc{overlap2},
\Vloc{dot},
\Vsym{lhs}, \Vstr{rhs1}, and \Vstr{rhs3}
in
\Dfref{def:h-ambig} be their eponyms in
\Dfref{def:ambig-EIM}.
Let \Vstr{rhs2}
in \Dfref{def:h-ambig} be
the string of length 1 consisting of \Vsym{rhs2}
in \Dfref{def:ambig-EIM}.

To show that
writing the derivation
\Eref{eq:def-h-ambig-21a}--\Eref{eq:def-h-ambig-21f}
in \Dfref{def:h-ambig}
is justified by the definition
\Dfref{def:ambig-EIM}, we note that
\begin{itemize}
\item from the definition of validity
\Dfref{def:EIM-valid}
for the Earley item
\Eref{eq:ambig-EIM-05},
we have the accessibility and productivity
of all the symbols in
\Eref{eq:def-h-ambig-21a}-\Eref{eq:def-h-ambig-21f};
\item from the validity of
the EIM \Eref{eq:ambig-EIM-10b}, we have
the location of \Vloc{dot}
in \Eref{eq:def-h-ambig-21d}
and of \Vloc{origin}
in \Eref{eq:def-h-ambig-21f}; and
\item
from the definition of EIM validity for
\Eref{eq:ambig-EIM-10b},
we have the location of \Vloc{overlap1}
in \Eref{eq:def-h-ambig-21e}.
\end{itemize}

To show
the remaining elements of \Dfref{def:h-ambig}
we note that
\begin{itemize}
\item we have the location of \Vloc{overlap2}
in \Eref{eq:def-h-ambig-30a}
from the definition of EIM validity for
\Eref{eq:ambig-EIM-20a}; and
\item
we have \Eref{eq:def-h-ambig-30b}
from \Eref{eq:ambig-EIM-03}.
\end{itemize}

We now have shown all the elements
in the definition of horizontal ambiguity \Dfref{def:h-ambig}.
Since our assumption of a full parse consistent with
\Eref{eq:th-h-EIM-g-ambig-15} is without loss of generality
\Eref{eq:th-h-EIM-g-ambig-20},
this shows that any full parse consistent with
\Eref{eq:th-h-EIM-g-ambig-15} is horizontally ambiguous.
And since by \Thref{th:h-ambig-g}, if a parse is horizontally
ambiguous,
its grammar is horizontally ambiguous,
we see that \Vcfg{g} is horizontally ambiguous.
\end{proof}

\begin{theorem}
\label{th:v-EIM-g-ambig}
If a partial parse has a vertically ambiguous EIM,
then any full parse consistent with that partial parse is vertically ambiguous,
and the grammar shared by the partial and full parse is vertically ambiguous.
\end{theorem}

\begin{proof}
Assume that,
for some \Vstr{w} and \Vloc{dot},
\begin{equation}
\label{eq:th-v-EIM-g-ambig-15}
\var{partial} = \left[\Vcfg{g}, \boldRangeDecr{w}{0}{dot} \right]
\end{equation}
is a partial parse.
Futher assume that \var{partial}
has a vertically ambiguous EIM \Dfref{def:ambig-EIM}.

Without loss of generality,
let
\begin{gather}
\var{full} = \left[\Vcfg{g}, \Vstr{w} \right], \\
\label{eq:th-v-EIM-g-ambig-20b}
\text{where $\Vstr{w} = \boldRangeDecr{w}{0}{dot} \cat \Vterm{suffix}$} \\
\nonumber
\text{for some \Vterm{suffix}}
\end{gather}
be a full parse consistent with \var{partial}.

We proceed by showing that there is a vertical ambiguity in \var{full}
according to \Dfref{def:v-ambig}.
By assumption for the theorem,
\var{partial} has a vertically ambiguous EIM according to
\Dfref{def:ambig-EIM}.

Let
\begin{align}
\label{eq:th-v-EIM-g-ambig-25a}
\text{\Vsym{rhs2} in \Dfref{def:ambig-EIM}} & = \text{\Vsym{lhs} in \Dfref{def:v-ambig}} \\
\label{eq:th-v-EIM-g-ambig-25b}
\text{\Vstr{children1} in \Dfref{def:ambig-EIM}} & = \text{\Vstr{rhs1} in \Dfref{def:v-ambig}} \\
\label{eq:th-v-EIM-g-ambig-25c}
\text{\Vstr{children2} in \Dfref{def:ambig-EIM}} & = \text{\Vstr{rhs2} in \Dfref{def:v-ambig}} \\
\label{eq:th-v-EIM-g-ambig-25d}
\text{\Vloc{dot} in \Dfref{def:ambig-EIM}} & = \text{\Vloc{dot} in \Dfref{def:v-ambig}} \\
\label{eq:th-v-EIM-g-ambig-25e}
\text{\Vloc{origin} in \Dfref{def:ambig-EIM}} & = \text{\Vloc{origin} in \Dfref{def:v-ambig}} \\
\label{eq:th-v-EIM-g-ambig-25f}
\text{\Vloc{overlap1} in \Dfref{def:ambig-EIM}} & = \text{\Vloc{origin} in \Dfref{def:v-ambig}} \\
\label{eq:th-v-EIM-g-ambig-25g}
\text{\Vloc{overlap2} in \Dfref{def:ambig-EIM}} & = \text{\Vloc{origin} in \Dfref{def:v-ambig}}
\end{align}

To justify writing
\Eref{eq:def-v-ambig-20a}--\Eref{eq:def-v-ambig-20f}
after the renamings of
\Eref{eq:th-v-EIM-g-ambig-25a}--\Eref{eq:th-v-EIM-g-ambig-25g},
we note that we have
\begin{itemize}
\item
\Eref{eq:def-v-ambig-15a} from
from
\Eref{eq:th-v-EIM-g-ambig-25a},
\Eref{eq:th-v-EIM-g-ambig-25b} and
\Eref{eq:ambig-EIM-10b};
\item
\Eref{eq:def-v-ambig-15b} from
\Eref{eq:th-v-EIM-g-ambig-25a},
\Eref{eq:th-v-EIM-g-ambig-25c} and
\Eref{eq:ambig-EIM-20b};
\item
the accessibility and productivity of the variables in
\Eref{eq:def-v-ambig-20a}--\Eref{eq:def-v-ambig-20f}
from the definition of EIM validity for
\Eref{eq:ambig-EIM-10b};
\item
the location of \Vloc{dot},
in \Eref{eq:def-v-ambig-20b}
from the definition of EIM validity for
\Eref{eq:ambig-EIM-10b}
and \Eref{eq:th-v-EIM-g-ambig-25d};
\item
the location of \Vloc{origin}
in \Eref{eq:def-v-ambig-20d}
from the definition of EIM validity for
\Eref{eq:ambig-EIM-10b}
and \Eref{eq:th-v-EIM-g-ambig-25f}; and
\item
\Eref{eq:def-v-ambig-15d} follows from
\Eref{eq:ambig-EIM-20b},
\Eref{eq:th-v-EIM-g-ambig-25d}, and
\Eref{eq:th-v-EIM-g-ambig-25g}.
\end{itemize}

It remains to justify
$\var{rhs1} \neq \var{rhs2}$,
\Eref{eq:def-v-ambig-15c}.
In \Dfref{def:ambig-EIM}
the two link pairs are required to be distinct.
The predecessors of the two links,
\Eref{eq:ambig-EIM-10a} and
\Eref{eq:ambig-EIM-20a}, differ only in their dot locations:
\Vloc{overlap1} versus \Vloc{overlap2}.
In
\Eref{eq:th-v-EIM-g-ambig-25f} and
\Eref{eq:th-v-EIM-g-ambig-25g},
we have set
both of these
to \Vloc{origin}
in \Dfref{def:v-ambig},
so that
\begin{equation}
\label{eq:th-v-EIM-g-ambig-40}
\begin{gathered}
\text{\Vloc{overlap1} in \Eref{eq:ambig-EIM-10a}} \\
= \text{\Vloc{overlap2} in \Eref{eq:ambig-EIM-20a}} \\
= \text{\Vloc{origin} in \Dfref{def:v-ambig}.}
\end{gathered}
\end{equation}
So, to be distinct, the two link pairs must differ in the causes.
The two causes,
\Eref{eq:ambig-EIM-10b} and
\Eref{eq:ambig-EIM-20b}
can differ only in their origins
and in their RHS's.
From
\Eref{eq:th-v-EIM-g-ambig-40}
we know that the origins of
\Eref{eq:ambig-EIM-10a} and
\Eref{eq:ambig-EIM-10b}
are identical,
so that
\Eref{eq:ambig-EIM-10b} must differ from
\Eref{eq:ambig-EIM-20b} in its RHS:
\begin{equation}
\label{eq:th-v-EIM-g-ambig-43}
\begin{gathered}
\text{\Vstr{children1} in \Eref{eq:ambig-EIM-10a}} \\
\neq \text{\Vstr{children2} in \Eref{eq:ambig-EIM-20a}}.
\end{gathered}
\end{equation}
From
\Eref{eq:th-v-EIM-g-ambig-43},
\Eref{eq:th-v-EIM-g-ambig-25b}, and
\Eref{eq:th-v-EIM-g-ambig-25c}
we have
\begin{equation}
\label{eq:th-v-EIM-g-ambig-46}
\text{$\Vstr{rhs1} \neq \Vstr{rhs2}$ in \Eref{eq:def-v-ambig-15c} of \Dfref{def:v-ambig}.}
\end{equation}
\end{proof}

\begin{theorem}
\label{th:ambig-EIM-ambig-g}
If a partial parse has an ambiguous EIM,
then its grammar is ambiguous.
\end{theorem}

\begin{proof}
Let $\var{partial} = [ \Vcfg{g}, \Vstr{prefix} ]$ be a partial
parse with an ambiguous EIM.
Let $\var{full} = [ \Vcfg{g}, \Vstr{prefix}\Vterm{suffix} ]$ be a full
parse consistent with \var{partial}.
From \Dfref{def:ambig-EIM}, we see that an ambiguous EIM
is either a horizontally ambiguous EIM, or
a vertically ambiguous EIM.
We proceed by cases.

If the ambiguous EIM is horizontally ambiguous then
by \Thref{th:h-EIM-g-ambig}
any full parse consistent with the partial parse
is horizontally ambiguous,
and therefore \var{full} is horizontally ambiguous.
By \Thref{th:h-ambig-g} if \var{full} is horizontally ambiguous,
then \Vcfg{g} is ambiguous.

If the ambiguous EIM is vertically ambiguous then
by \Thref{th:v-EIM-g-ambig}
any full parse consistent with the partial parse
is vertically ambiguous,
and therefore \var{full} is vertically ambiguous.
By \Thref{th:v-ambig-g} if \var{full} is vertically ambiguous,
then \Vcfg{g} is ambiguous.
\end{proof}

\begin{theorem}
\label{th:unambig-g-unambig-EIM}
If a grammar is unambiguous, then it does not have
an ambiguous EIM.
\end{theorem}

\begin{proof}
This theorem is the contrapositive of \Thref{th:ambig-EIM-ambig-g}.
\end{proof}

\section{Implementation}
\label{sec:implementation}

\subsection{Leo memoization}

\begin{sloppypar}
Recall that we
call a dotted rule \Vdr{d} a \dfn{penult} if $\Penult{\var{d}} \neq \Lambda$.
In Leo's original algorithm, any penult
was treated as a potential right-recursion.
\Marpa{} applies the Leo memoizations in more restricted circumstances.
For \Marpa{} to consider a dotted rule
\begin{equation*}
\Vdr{candidate} = [\Vrule{candidate}, \var{i}]
\end{equation*}
for Leo memoization,
not only must \Vdr{candidate} be a penult
but \Vrule{candidate} must be right-recursive.
\end{sloppypar}

By restricting Leo memoization to right-recursive rules,
\Marpa{} incurs the cost of Leo memoization only in cases
where Leo sequences could be infinitely
long.
This more careful targeting of the memoization is for efficiency reasons.
If all penults are memoized,
many memoizations will be performed where
the longest potential Leo sequence is short,
and the payoff is therefore limited.

A future extension might be to identify
non-right-recursive rules
which generate Leo sequences long enough to
justify inclusion in the Leo memoizations.
Such cases would be unusual, but may occur.

Omission of a memoization does not affect correctness,
so \Marpa{}'s restriction of Leo memoization
preserves the correctness as shown in Leo\cite{Leo1991}.
% TODO Add ref instead of "Later"
Later in this paper
we will
show that this change also leaves
the complexity results of
Leo\cite{Leo1991} intact.

% TODO: Reference to Leo chain -- cataphoric?
The \Marpa{} implementation does not implement
Leo memoization in Earley set 0.
$\es{(0)}$ memoization
would have to be implemented as a special case,
and omission of a Leo memoization at one end of a Leo
chain unions at one EIM into the Earley tables.
If the difference is measurable
$\es{(0)}$ memoization will probably be found to
be counter-productive.

\subsection{Unproductive symbols}

Users using ``unicorns'' symbols
might be concerned about the assumption for the
theoretical results
that \Marpa{}'s symbols are productive.
Almost always,
this restriction in the theory is made
without loss of its relevance to practice.

For example, in cases where
\Eref{eq:EIM-valid-30}
in the definition of EIM validity
is violated,
the result would be that EIM's that were ``invalid''
in a pedantic sense would be
physically present in the Earley sets.
That these pedantically invalid EIM's should affect correctness
is probably
exactly what the user intended.
The effect of ``unicorns'' on time and space complexity tends
to be insignificant in practical use.

Users who are using ``unicorns'',
who are concerned about the theoretical results,
and who wish to be conservative,
should consider evaluate their grammar treating
``unicorns'' as if they were productive symbols.

\subsection{Earley set indexing}
\label{sec:es-indexing}

In our theoretical discussions,
we assumed that we could efficiently convert between
an Earley set location as an integer
(\Vloc{i} or \Ves{i}),
and an Earley set as data structure including the PIM's.
Marpa::R2\cite{Marpa-R2}
builds Earley sets as a linked list of structures,
and stores the \Vloc{i} in an integer field.
An array for indexing is created on an ``as needed''
basis.

Earley sets are stored internally as pointers.
These pointers are sufficient even for the run-time
event mechanism.
In practice,
indexing by \Vloc{i} only becomes necessary
during run-time for run-time tracing and debugging.
In normal usage, the creation of the array of
Earley sets on an ``as needed'' basis means that it
is created once,
after the parse is complete.
The Earley sets will never be changed after this point
and therefore the array of Earley sets will never need to be resized.\footnote{
In this discussion a parse is said to be ``complete'' when it is ready to be evaluated.
This becomes relevant when an application takes advantage of
Marpa::R2's ability to resume an already evaluated
parse, reading more input.
Each of these is, in fact, a separate parse,
and we take this point of view when discussing per-parse resource.
}

\subsection{Transition tables}
\label{sec:transition-tables}

\TODO{}

\section{Per-set lists}
\label{sec:per-set-lists}

In the general case,
where \var{x} is an arbitrary datum,
it is not possible
to use duple $[\Ves{i}, \var{x}]$
as a search key and expect the search to use
\Oc{} time.
Within \Marpa, however, there are specific cases
where it is desirable to do exactly that.
This is accomplished by
taking advantage of special properties of the search.

If it can be arranged that there is
a direct link to the Earley set \Ves{i},
and that $0 \leq \var{x} < \var{c}$,
where \var{c} is a constant of reasonable size,
then a search can be made in \Oc{} time,
using a data structure called a PSL.
Data structures identical to,
or very similar to,
PSL's are
briefly outlined in both
\cite[p. 97]{Earley1970} and
\cite[Vol. 1, pages 326-327]{AU1972}.
But neither source gives them a name.
The term PSL
(``per-Earley set list'')
is new
with this paper.

A PSL is a fixed-length array of
integers, indexed by an integer,
and kept as part of the structure
that implements each Earley set.
While \Marpa{} is building a new Earley set,
\Ves{j},
a PSL is kept for every previous Earley set ---
that is, for every
$\Vloc{i} < \Vloc{j}$.

While \Marpa{} is unioning EIM's into \Ves{j},
the PSL for \Ves{i}
keeps track of the Earley items in \Ves{j} that have \Vloc{i}
as their origin.
The maximum number of Earley items that must be tracked
in each PSL is
the number of dotted rules,
\Vsize{g}.
Recall that \Vsize{g}
is a constant of reasonable size
that depends on \Cg{}.

It would take more than \Oc{} time
to clear and rebuild the PSL's
for all the Earley sets
every time
that a new Earley set is started.
This overhead is avoided by ``time-stamping'' each PSL
entry with the Earley set
that was current when that PSL
entry was last updated.

Let $\ID{\Vdr{x}}$ be the unique integer ID of a dotted rule.
One way to create a unique integer value
for every dotted rule is create an arbitrary but fixed order
for the rules;
sort the dotted rules into a sequence by dot position with rule;
and then number the elements of the sorted sequence consecutively.

Intuitively, the idea is avoid most re-computation
of the PSL's by keeping timestamps with the PSL data.
To illustrate the use of PSL's,
we will consider the case
where \Marpa{} is building \Ves{j}
and wants to check whether Earley item,
\begin{equation*}
\Veim{x} = [ \Vdr{x}, \Vorig{x} ],
\end{equation*}
is new,
or if \Veim{x} needs to be unioned into the Earley sets.

Let
\begin{equation*}
\var{psl-entry} = \PSL{\Ves{i}}{\var{y}}
\end{equation*}
be the entry for integer \var{y} in the PSL in
the Earley set at \Vloc{x}, such that
\begin{equation*}
\begin{alignedat}{3}
& \var{psl-entry} && \defined && \quad
\begin{cases}
\Lambda, \quad \text{if the entry at $\var{psl-entry}$ has never been used,} \\
[\var{time-stamp}, \Vbool{z}], \text{otherwise} \\
\end{cases} \\
%
\end{alignedat}
\end{equation*}
Here \var{time-stamp} is a time stamp which will be used to
avoid unnecessary re-computation,
and \Vbool{z} is \var{true} if \Veim{x}
is already present in \Ves{j},
and \var{false} otherwise.

\begin{algorithm}[tb]
\caption{Union EIM}
\label{alg:union-eim}
\begin{algorithmic}[1]
\Procedure{Add-EIM-If-New}{$[\Vdr{x},\Vorig{x}]@\Ves{j}$}
\If{$\PSL{\Ves{j}}{\ID{\Vdr{x}}} = [\Vloc{j}, \var{true}]$}
\State return \Comment Do nothing if \Veim{x} exists
\EndIf
\State $\Ves{j} \gets \Ves{j} \cup \set{[\Vdr{x]}, \Vorig{x}]}$
\State $\PSL{\Ves{x}}{\ID{\Vdr{x}}} \gets [\Vloc{j}, \var{true}]$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\Aref{alg:union-eim} contains pseudo-code
showing the use of PSL's.
The reader may notice that
\Aref{alg:union-eim} could be simplified,
because the data to be kept for EIM's is a single boolean.
Further, this boolean is never set to \var{false}, so that
if a PSL entry exists, its boolean may be assumed to be \var{true}.
This means that an explicit boolean is not actually needed.
The simpler approach, which omits the boolean,
is the one that
the \Marpa{} implementation sometimes uses.

\section{\Marpa{} is correct}
\label{sec:correctness}

In this paper,
we call a parsing algorithm \dfn{correct} if and
only if, for every parse, it is \dfn{consistent} and
\dfn{complete}.
We call a parsing algorithm \dfn{consistent} if
it produces only valid Earley items.
We call a parsing algorithm \dfn{complete} if
it produces all of the valid Earley items.

\begin{theorem}
\label{th:earley-is-correct}
\Earley{} is correct.
\end{theorem}

\begin{proof}
\Earley{} is proved correct in
\cite[Theorem 4.9, pp. 323-325]{AU1972} and
\cite[pp. 18-25]{Earley1968}.
\end{proof}

The following is proved as part of
\Thref{th:earley-is-correct},
but we state and prove it separately
because we make special use of it.

\begin{theorem}[Earley completer consistency]
\label{th:earley-completer-is-consistent}
The \Earley{} completer operation preserves validity.
That is, if the predecessor and cause
of an Earley{} completer operation are valid,
the result of the Earley{} completer operation is valid.
\end{theorem}

\begin{proof}
Let $[\Vint{g}, \Vstr{prefix}]$ be a partial parse,
let
\[
[ \Veim{pred}, \Veim{cuz} ]_\type{LP}
\]
be an arbitrary ancestry of an Earley{} completer operation,
and let the result of the operation be \Veim{rez}.
Then
\begin{equation}
\label{eq:earley-completer-is-consistent-20}
  \begin{gathered}
  \Vdr{pred} =
  \left[ \Vsym{lhsP} \de \Vstr{rhs1} \mydot \Vsym{postdot} \Vstr{rhs3}
  \right] \\
  \because \Eref{eq:earley-completer-summary-2}, \text{WLOG}.
  \end{gathered}
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-22}
  \begin{gathered}
  \Veim{pred} =
  \left[ \Vdr{pred}, \Vloc{orig} \right] @ \Vloc{meet} \\
  \because \Eref{eq:earley-completer-summary-2},
    \Eref{eq:earley-completer-is-consistent-20}, \text{WLOG}.
  \end{gathered}
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-24}
\LHS{\Vdr{cuz}} = \Vsym{postdot}
\because \Eref{eq:earley-completer-summary-3},
  \Eref{eq:earley-completer-is-consistent-20}.
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-26}
  \begin{gathered}
  \Veim{cuz} = \left[ \Vdr{cuz}, \Vloc{meet} \right]
    @ \Vloc{current} \\
  \because \Eref{eq:earley-completer-is-consistent-22},
  \Eref{eq:earley-completer-summary-1},
  \Eref{eq:earley-completer-summary-2}, \text{WLOG}.
  \end{gathered}
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-28}
  \begin{gathered}
  \Veim{rez} =
  \left[
    \begin{gathered}
      \Next{\DR{\var{pred}}}, \\
      \Origin{\var{pred}}
    \end{gathered}
  \right]@\Vloc{current} \\
  \because
  \Eref{eq:earley-completer-summary-4},
  \Eref{eq:earley-completer-is-consistent-22},
  \Eref{eq:earley-completer-is-consistent-26}.
  \end{gathered}
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-30}
\myparbox{\Veim{pred} is valid
by assumption for theorem.}
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-32}
\myparbox{\Veim{cuz} is valid
by assumption for theorem.}
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-34}
\Rule{\Veim{pred}} \in \Rules{\var{g}}
\because \Eref{eq:EIM-valid-22a},
  \Eref{eq:earley-completer-is-consistent-30}.
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-36}
\begin{gathered}
  \Accept{\var{g}} \destar \Vstr{before} \cat \LHS{\Veim{pred}} \cat \Vstr{after} \\
  \because \Eref{eq:EIM-valid-22b},
    \Eref{eq:earley-completer-is-consistent-30}, \text{WLOG}.
  \end{gathered}
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-38}
\begin{gathered}
  \Vstr{before} \destar \var{prefix}[0  \ldots  (\var{orig}\subtract 1)] \\
  \because \Eref{eq:EIM-valid-22c},
    \Eref{eq:earley-completer-is-consistent-22},
    \Eref{eq:earley-completer-is-consistent-30}.
  \end{gathered}
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-40}
\begin{gathered}
  \Vstr{rhs1} \destar \var{prefix}[\var{orig}  \ldots  (\var{meet}\subtract 1)] \\
  \because \Eref{eq:EIM-valid-22d},
  \Eref{eq:earley-completer-is-consistent-20},
  \Eref{eq:earley-completer-is-consistent-22},
  \Eref{eq:earley-completer-is-consistent-30}.
  \end{gathered}
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-42}
\Rule{\Veim{pred}} = \Rule{\Veim{rez}}
\because \Eref{eq:earley-completer-is-consistent-28}.
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-44}
\Rule{\Veim{rez}} \in \Rules{\var{g}}
\because \Eref{eq:earley-completer-is-consistent-34},
\Eref{eq:earley-completer-is-consistent-42}.
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-46}
\myparbox{$\LHS{\Veim{pred}} = \LHS{\Veim{rez}}$
$\because$ Def \var{Next} \Eref{eq:def-next},
  \Eref{eq:earley-completer-is-consistent-42}.
}
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-48}
\begin{gathered}
  \Accept{\var{g}} \destar \Vstr{before} \cat \LHS{\Veim{rez}} \cat \Vstr{after} \\
  \because \Eref{eq:earley-completer-is-consistent-36},
  \Eref{eq:earley-completer-is-consistent-46}.
  \end{gathered}
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-50}
\begin{aligned}
  & \Vdr{rez} = \left[ \Vsym{lhsP} \de \Vstr{rhs1} \Vsym{postdot} \mydot \Vstr{rhs3}
  \right] \\
  & \qquad \quad \because \Eref{eq:earley-completer-is-consistent-20}, \Eref{eq:earley-completer-is-consistent-28}.
  \end{aligned}
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-52}
\myparbox{\Veim{cuz} is a completion
  $\because \Eref{eq:earley-completer-summary-1}$.}
\end{equation}
%
\begin{equation}
\label{eq:earley-completer-is-consistent-53}
  \begin{gathered}
    \LHS{\Vdr{cuz}} \destar \var{prefix}[\var{meet}  \ldots  (\var{current}\subtract 1)] \\
    \because \Eref{eq:EIM-valid-22d},
    \Eref{eq:earley-completer-is-consistent-26},
    \Eref{eq:earley-completer-is-consistent-32},
    \Eref{eq:earley-completer-is-consistent-52}.
  \end{gathered}
\end{equation}
%
\begin{equation}
\label{eq:earley-completer-is-consistent-54}
  \begin{gathered}
  \Vstr{postdot} \destar
  \var{prefix}[\var{meet}  \ldots  (\var{current}\subtract 1)] \\
  \because
  \Eref{eq:earley-completer-is-consistent-24},
  \Eref{eq:earley-completer-is-consistent-53}.
  \end{gathered}
\end{equation}
%
\begin{equation}
\label{eq:earley-completer-is-consistent-56}
\begin{aligned}
  & \Vstr{rhs1} \Vstr{postdot} \\
  & \qquad \destar \var{prefix}[\var{orig}  \ldots  (\var{current}\subtract 1)] \\
  & \qquad \qquad \because \Eref{eq:earley-completer-is-consistent-40},
    \Eref{eq:earley-completer-is-consistent-54}
  \end{aligned}
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-58}
\myparbox{\Veim{rez} is valid
  $\because$ \Dfref{def:EIM-valid},
  \Eref{eq:earley-completer-is-consistent-38},
  \Eref{eq:earley-completer-is-consistent-44},
  \Eref{eq:earley-completer-is-consistent-48},
  \Eref{eq:earley-completer-is-consistent-56}
}
\end{equation}
\end{proof}

% \TODO{Delete this?}
% Let \Ves{physical} be an Earley set as produced by \Marpa{},
% and let \Vves{virtual} be \Ves{physical} with its Leo memoized
% items restored.
% We will call \Ves{physical} a \dfn{physical} Earley set, and
% we will call \Vves{virtual} a \dfn{virtual} Earley set
% (type \type{VES}).

\begin{definition}[Leo chain]
\label{def:leo-chain}
A \dfn{Leo chain},
or simply \dfn{chain} when the meaning is clear in context,
is a sequence of one or more LIM's,
call it $\Vlimset{links}$,
where
\[
\PreviousLink{\var{links}[\Vlastix{links}]} = \Lambda
\]
and, for all $0 \le \var{n} < \Vlastix{links}$,
\[
\PreviousLink{\var{links}[\var{n}]} = \var{links}[\var{n}+1].
\]
We say that \var{links} is the chain of \Vlim{first} if and
only if \var{links} is a Leo chain
such that $\var{links}[0] = \Vlim{first}$.

A chain is \dfn{maximal} if and only if it is not properly
contained in another chain.
A chain is \dfn{partial} if it is contained in another chain.
Note that a maximal chain is a special case of a partial chain.
Chains are maximal unless stated otherwise.
$\bigstar$
\end{definition}

\begin{definition}[LIM Validity]
\label{def:lim-validity}
We say that a LIM,
call it \mylim{\ell},
is \dfn{valid}
if and only if,
where \var{chain} is the LIM chain of $\ell$,
\begin{gather}
\DR{\Source{\Velement{chain}{\Vlastix{chain}}}} = \DR{\ell}, \\
\Origin{\Source{\Velement{chain}{\Vlastix{chain}}}} = \Origin{\ell}, \\
\Postdot{\Source{\ell}} = \Transition{\ell}, \\
\text{\Source{\ell} is valid, and} \\
\text{\PreviousLink{\ell} is $\Lambda$ or a valid LIM.}
  \quad \bigstar
\end{gather}
We say that a Leo chain is \dfn{valid} if and only if
all the links of the chain are valid.
$\bigstar$
\end{definition}

\begin{theorem}[LEO Chain containment]
\label{th:chain-containment}
Let
\begin{gather}
\label{eq:th-chain-containment-01}
\myparbox{\Vlimset{longer} be a valid Leo chain,} \\
\label{eq:th-chain-containment-02}
\myparbox{$\mylim{\ell} \in \var{longer}$ be a LIM,} \\
\label{eq:th-chain-containment-03}
\myparbox{\var{shorter} be the valid Leo chain of \mylim{\ell}.}
\end{gather}
Then
\begin{gather}
\label{eq:th-chain-containment-07}
\forall \; \mylim{\ell{}2} \in \var{shorter} : \ell{}2 \in \var{longer}
\end{gather}
\end{theorem}

\begin{proof}
Let \Vlimset{longer}, \mylim{\ell},
and \Vlimset{shorter} be as stated for the theorem.

We show \Eref{eq:th-chain-containment-07} by induction on element index
of \var{shorter}.
\begin{equation}
\label{eq:th-chain-containment-10}
\begin{gathered}
0 \le \var{ix} \le \Vlastix{shorter} \implies \VVelement{shorter}{ix} \in \var{longer} \\
\text{$\because$ hypothesis for induction.}
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-chain-containment-12}
\myparbox{$\Velement{shorter}{0} \in \var{longer}$
  $\because$
  \Dfref{def:leo-chain}, \Eref{eq:th-chain-containment-03}.%
}
\end{equation}
\begin{equation}
\label{eq:th-chain-containment-13}
\begin{gathered}
0 \le 0 \le \Vlastix{shorter} \implies \Velement{shorter}{0} \in \var{longer} \\
\myparbox{$\because$ \Eref{eq:th-chain-containment-12}.
  This is the basis of an induction on
  hypothesis \Eref{eq:th-chain-containment-10}, with $\var{ix} = 0$.%
}
\end{gathered}
\end{equation}
For the step of the induction
we assume
\begin{equation}
\label{eq:th-chain-containment-14}
\begin{gathered}
0 \le \var{n} \le \Vlastix{shorter} \implies \VVelement{shorter}{n} \in \var{longer} \\
\myparbox{$\because$ \Eref{eq:th-chain-containment-10} with $\var{ix} = \var{n}$.%
}
\end{gathered}
\end{equation}
to show
\begin{equation}
\label{eq:th-chain-containment-14-10}
\begin{gathered}
0 \le \var{n}+1 \le \Vlastix{shorter} \implies \Velement{shorter}{\var{n}+1} \in \var{longer} \\
\myparbox{$\because$ \Eref{eq:th-chain-containment-10} with $\var{ix} = \var{n}+1$.%
}
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-chain-containment-15}
\begin{gathered}
0 \le \var{n} < \Vlastix{shorter} \\
\myparbox{$\because$ AF subcase.
Otherwise, we have \Eref{eq:th-chain-containment-14-10} vacuously.}
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-chain-containment-16}
\myparbox{$\exists \; \var{m} : \VVelement{shorter}{n} = \VVelement{longer}{m}$
  $\because$
  \Eref{eq:th-chain-containment-14},
  \Eref{eq:th-chain-containment-15}.%
}
\end{equation}
\begin{equation}
\label{eq:th-chain-containment-18}
\myparbox{$\PreviousLink{\VVelement{shorter}{n}} = \Velement{shorter}{\var{n}+1}$
  $\because$ \longDfref{Leo chain}{def:leo-chain},
  \Eref{eq:th-chain-containment-15}.%
}
\end{equation}
\begin{equation}
\label{eq:th-chain-containment-20}
\myparbox{$\PreviousLink{\VVelement{longer}{m}} = \Velement{shorter}{\var{n}+1}$
  $\because$
  \Eref{eq:th-chain-containment-15},
  \Eref{eq:th-chain-containment-16},
  \Eref{eq:th-chain-containment-18}.%
}
\end{equation}
\begin{equation}
\label{eq:th-chain-containment-22}
\myparbox{$\PreviousLink{\VVelement{longer}{m}} = \Velement{longer}{\var{m}+1}$
  $\because$ \longDfref{Leo chain}{def:leo-chain},
  \Eref{eq:th-chain-containment-15}.%
}
\end{equation}
\begin{equation}
\label{eq:th-chain-containment-24}
\myparbox{$\VVelement{shorter}{\var{n}+1} = \Velement{longer}{\var{m}+1}$
  $\because$
  \Eref{eq:th-chain-containment-20},
  \Eref{eq:th-chain-containment-22}.
}
\end{equation}
\begin{equation}
\label{eq:th-chain-containment-26}
\begin{gathered}
0 \le \var{n}+1 \le \Vlastix{shorter} \implies \Velement{shorter}{\var{n}+1} \in \var{longer} \\
\myparbox{$\because$ \Eref{eq:th-chain-containment-24}.
  This is \Eref{eq:th-chain-containment-10} with $\var{ix} = \var{n}+1$
  and shows the step starting at \Eref{eq:th-chain-containment-14}.
}
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-chain-containment-28}
\begin{gathered}
\forall \; \mylim{\ell{}2} \in \var{shorter} : \ell{}2 \in \var{longer} \\
\text{$\because$ induction \Eref{eq:th-chain-containment-10}--\Eref{eq:th-chain-containment-26}.}
\end{gathered}
\end{equation}
\end{proof}

\begin{theorem}[LEO Chain termination]
\label{th:chain-termination}
Let
\begin{gather}
\label{eq:th-chain-termination-01}
\myparbox{\Vlimset{longer} be a valid Leo chain,} \\
\label{eq:th-chain-termination-03}
\myparbox{\var{shorter} be the valid Leo chain contained in \var{longer}.}
\end{gather}
Then
\begin{gather}
\label{eq:th-chain-termination-08}
\Velement{shorter}{\Vlastix{shorter}} = \Velement{longer}{\Vlastix{longer}}
\end{gather}
\end{theorem}

\begin{theorem}[Memoizer consistency]
\label{th:memoizer-consistency}
The \Marpa{} memoizer operation preserves validity.
\end{theorem}

\begin{proof}
Call the LIM added by an arbitrary \Marpa{} memoizer
operation \mylim{\ell},
and its ancestry $[\Vlim{previous}, \Veim{source}]$.
\begin{equation}
\label{eq:th-memoizer-consistency-10}
\myparbox{\Vlim{previous} is $\Lambda$ or a valid LIM, by
assumption for the theorem.}
\end{equation}
\begin{equation}
\label{eq:th-memoizer-consistency-12}
\myparbox{\Veim{source} is valid, by
assumption for the theorem.}
\end{equation}
\begin{equation}
\label{eq:th-memoizer-consistency-14}
  \begin{gathered}
  \Postdot{\Veim{source}} = \Transition{\ell} \\
  \because
  \Eref{eq:memoizer-operation-20},
  \Eref{eq:memoizer-operation-30}.
  \end{gathered}
\end{equation}
\TODO{finish}
\end{proof}

\begin{definition}[Leo chain--base match]
\label{def:leo-chain-base-match}
Let \Veim{base} be an EIM
and let \Vlim{lim} be a LIM.
We say \Veim{base} \dfn{matches} \Vlim{lim},
if and only if
\begin{itemize}
\item \Veim{base} is a completion;
\item $\Transition{\Vlim{lim}} = \LHS{\var{base}}$; and
\item $\Vlim{lim} \in \es{(\Origin{\var{base}})}$.
\end{itemize}
We say that \Veim{base} \dfn{matches} a Leo chain,
call it \Vlimset{chain},
if and only if \Veim{base} matches $\var{chain}[0]$.
$\bigstar$
\end{definition}

\begin{definition}[Leo edge]
\label{def:leo-edge}
Let \Vint{g} be a grammar,
let \Vlimset{chain} be a valid Leo chain,
and let $\Veim{base} \in \Ves{current}$ be a valid EIM which
matches \var{chain}.
Then the \dfn{Leo edge}, or just \dfn{edge},
of \var{chain} and \var{base}
is a sequence of complete EIM's, call it \Veimset{edge},
such that
\begin{equation}
\label{eq:d-leo-edge-10}
\var{edge}[0] = \Veim{base}
\end{equation}
and, for all \var{n} such that $0 \le \var{n} < \Vsize{chain}$,
\begin{equation}
\label{def:leo-edge-30}
\left[ \Source{\var{chain}[\var{n}]}, \var{edge}[\var{n}] \right]_\type{LP}
  \in \LinkPairs{\var{edge}[\var{n}+1]}
\end{equation}

We say that an edge is \dfn{valid} if and only if all the elements of the edge
are valid.
The \dfn{top} of the edge is \Velement{edge}{\Vlastix{edge}}.
The \dfn{bottom} of the edge is \Velement{edge}{0}.
The \dfn{inside} of the edge is the sequence
\Velement{edge}{1 \ldots (\Vlastix{edge}\subtract 1)}.
An \dfn{inside element} of an edge is a element of the
inside of the edge.
An \dfn{upper element} of an edge is a top or an inside element.
An \dfn{lower element} of an edge is a bottom or an inside element.

An edge is \dfn{maximal} if its Leo chain is maximal.
An edge is \dfn{partial} if its Leo chain is partial.
An edge is maximal, unless stated otherwise.
$\bigstar$
\end{definition}

\begin{theorem}[Leo edge length]
\label{th:leo-edge-length}
Let \Vint{g} be a grammar;
let \Vlimset{chain} be a valid partial Leo chain;
let $\Veim{base} \in \Ves{current}$ be a valid EIM which
matches \var{chain};
and let \Veimset{edge} be their edge.
Then
\[
\Vlastix{edge} = \Vlastix{chain} + 1.
\]
\end{theorem}

\begin{proof}
\begin{equation}
\label{eq:th-leo-edge-length-10}
\Vsize{chain} > 0 \because
\text{Def of ``Leo chain'' \Dfref{def:leo-chain}.}
\end{equation}
\begin{equation}
\label{eq:th-leo-edge-length-14}
\begin{gathered}
\left[ \Source{\Velement{chain}{\Vlastix{chain}}}, \Velement{edge}{\Vlastix{chain}} \right]_\type{LP} \\
  \in \LinkPairs{\Velement{edge}{\Vlastix{chain}+1}} \\
  \because
  \text{\Eref{def:leo-edge-30} in \Dfref{def:leo-chain}},
  \Eref{eq:th-leo-edge-length-10}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-leo-edge-length-16}
\myparbox{edge index is monotonically increasing
as a function of chain index $\because$
\Eref{def:leo-edge-30} in \Dfref{def:leo-chain}.}
\end{equation}
\begin{equation}
\label{eq:th-leo-edge-length-18}
\Vlastix{edge} = \Vlastix{chain} + 1
\because
\Eref{eq:th-leo-edge-length-14},
\Eref{eq:th-leo-edge-length-16}. \quad \qedhere
\end{equation}
\end{proof}

\begin{theorem}[Leo chain source properties]
\label{th:leo-chain-source-properties}
In a \Marpa{} internal grammar,
the source of a Leo chain element is a penult in the same
Earley set as the element.
\end{theorem}

\begin{proof}
Let \Vint{g} be a \Marpa{} internal grammar.
\begin{equation}
\label{eq:th-leo-chain-source-properties-10}
\myparbox{$\mylim{\ell}@\Vloc{j}$ is a Leo chain element
$\because$ AF theorem, WLOG.
}
\end{equation}
\begin{equation}
\label{eq:th-leo-chain-source-properties-12}
\Veim{source} = \Source{\ell}
\because
\Eref{eq:th-leo-chain-source-properties-10},
\text{WLOG}.
\end{equation}
\begin{equation}
\label{eq:th-leo-chain-source-properties-20}
\begin{gathered}
\Veim{source} \in \LeoEligible{\Vloc{j}} \\
\qquad \because
\Eref{eq:memoizer-operation-20}, \Eref{eq:memoizer-operation-30},
\text{Def ``source'' \Eref{eq:def-source}}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-leo-chain-source-properties-22}
\begin{gathered}
\Veim{source} \in \LeoUnique{\Vloc{j}} \\
\because
\Eref{eq:th-leo-chain-source-properties-20}, \Eref{eq:memoizer-operation-10}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-leo-chain-source-properties-24}
\myparbox{\Veim{source} is a penult
  $\because$ \Eref{eq:th-leo-chain-source-properties-22},
  Def ``Leo unique'' \Dfref{def:leo-unique}.}
\end{equation}
\begin{equation}
\label{eq:th-leo-chain-source-properties-26}
\Current{\Veim{source}} = \Vloc{j}
  \because \Eref{eq:th-leo-chain-source-properties-22},
  \Dfref{def:leo-unique}.
\end{equation}
The theorem follows directly from
\Eref{eq:th-leo-chain-source-properties-24} and
\Eref{eq:th-leo-chain-source-properties-26}.
\end{proof}

\begin{theorem}[Edge properties]
\label{th:edge-elements-are-complete}
In a \Marpa{} internal grammar,
let \Vlimset{chain} be a valid Leo chain,
and \Veim{base} a valid EIM matching \var{chain}.
Every element of the edge of \var{chain} and \var{base}
is a valid completion in the same Earley set as \Veim{base},
and the edge is valid.
\end{theorem}

\begin{proof}
Let \Vint{g} be a \Marpa{} internal grammar,
let \Vlimset{chain} be a valid Leo chain
and let \Veim{base} be a valid EIM which matches \var{chain}.
Let \Veimset{edge} be the edge of \var{chain} and \var{base}.
We proceed by induction on the elements of the edge,
where the induction hypothesis is
\begin{equation}
\label{eq:edge-elements-are-complete-05}
\myparbox{
$\var{edge}[\var{ix}]$ a valid completion in the same Earley set as \Veim{base}.
}
\end{equation}

We use $\var{ix} = 0$ as the basis of the induction:
\begin{equation}
\label{eq:edge-elements-are-complete-10}
\var{edge}[0] = \Veim{base} \because \Eref{eq:d-leo-edge-10}.
\end{equation}
\begin{equation}
\label{eq:edge-elements-are-complete-12}
\text{\Veim{base} is a completion $\because \Dfref{def:leo-chain-base-match}$.}
\end{equation}
\begin{equation}
\label{eq:edge-elements-are-complete-14}
\text{\Veim{base} is in the same Earley set as \Veim{base} $\because$ trivial.}
\end{equation}
\begin{equation}
\label{eq:edge-elements-are-complete-16}
\text{\Veim{base} is valid by
  assumption for the theorem.}
\end{equation}
\begin{gather}
\label{eq:edge-elements-are-complete-18}
\myparbox{\var{edge}[0] is a valid completion
  in the same Earley set as \Veim{base}
  $\because$
  \Eref{eq:edge-elements-are-complete-10},
  \Eref{eq:edge-elements-are-complete-12},
  \Eref{eq:edge-elements-are-complete-14},
  \Eref{eq:edge-elements-are-complete-16}.
  This is
  \Eref{eq:edge-elements-are-complete-05} for $\var{ix} = 0$,
  the basis of the induction on \Eref{eq:edge-elements-are-complete-05}.
  }
\end{gather}

For the step of the induction, we assume
the induction hypothesis \Eref{eq:edge-elements-are-complete-05}
for $\var{ix}=\var{n}$
to show
the induction hypothesis
for $\var{ix}=\var{n}+1$.
\begin{equation}
\label{eq:edge-elements-are-complete-20}
\myparbox{
$\var{edge}[\var{n}]$ is a valid completion in the same Earley set as \Veim{base}
$\because$ assumption for step of the induction.
}
\end{equation}
\begin{equation}
\label{eq:edge-elements-are-complete-30}
  \begin{gathered}
    \left[ \Source{\var{chain}[\var{n}]}, \var{edge}[\var{n}] \right]_\type{LP}
      \in \LinkPairs{\var{edge}[\var{n}+1]} \\
    \because
      \text{\Eref{def:leo-edge-30} of Def ``Leo edge''
      \Dfref{def:leo-edge}}.
    \end{gathered}
\end{equation}
\begin{equation}
\label{eq:edge-elements-are-complete-36}
\myparbox{$\var{edge}[\var{n}+1]$
is the result of an \Earley{} completer operation
whose ancestry includes
$\left[ \Source{\var{chain}[\var{n}]},
  \var{edge}[\var{n}] \right]_\type{LP}$
$\because$
\Obref{obs:earley-operation-by-direct-ancestor},
\Eref{eq:edge-elements-are-complete-20},
\Eref{eq:edge-elements-are-complete-30}.
}
\end{equation}
\begin{equation}
\label{eq:edge-elements-are-complete-37-10}
\myparbox{
  \Vlimset{chain} is valid
  $\because$ AF theorem.}
\end{equation}
\begin{equation}
\label{eq:edge-elements-are-complete-37-15}
\myparbox{
  \Source{\var{chain}[\var{n}]} is valid
  $\because$
  Def ``LIM valdity'' \Dfref{def:lim-validity},
  \Eref{eq:edge-elements-are-complete-37-10}.
  }
\end{equation}
\begin{equation}
\label{eq:edge-elements-are-complete-37-20}
\myparbox{
  $\var{edge}[\var{n}]$ is valid
  $\because \Eref{eq:edge-elements-are-complete-20}$.
  }
\end{equation}
\begin{equation}
\label{eq:edge-elements-are-complete-37-25}
\myparbox{
  $\var{edge}[\var{n}+1]$ is valid
  $\because$
  \Thref{th:earley-completer-is-consistent},
  \Eref{eq:edge-elements-are-complete-36},
  \Eref{eq:edge-elements-are-complete-37-15},
  \Eref{eq:edge-elements-are-complete-37-20}.}
\end{equation}
\begin{equation}
\label{eq:edge-elements-are-complete-38}
\myparbox{
  \Source{\var{chain}[\var{n}]} is a penult
  $\because$ \Thref{th:leo-chain-source-properties}.}
\end{equation}
\begin{equation}
\label{eq:edge-elements-are-complete-40}
\begin{gathered}
  \DR{\var{edge}[\var{n}+1]} = \Next{\Source{\var{chain}[\var{n}]}} \\
  \because
  \Eref{obs:earley-completer-summary}, \Eref{eq:edge-elements-are-complete-36}.
  \end{gathered}
\end{equation}
\begin{equation}
\label{eq:edge-elements-are-complete-42}
\myparbox{$\var{edge}[\var{n}+1]$ is a completion
  $\because$
  Def \var{Next} \Eref{eq:def-next},
  \Eref{eq:edge-elements-are-complete-38},
  \Eref{eq:edge-elements-are-complete-40}.}
\end{equation}
\begin{equation}
\label{eq:edge-elements-are-complete-44}
\myparbox{\Velement{edge}{\var{n}+1} is in the same Earley set as \VVelement{edge}{n}
  $\because$
\Eref{eq:earley-completer-summary-1}
  \Eref{eq:earley-completer-summary-4},
  \Eref{eq:edge-elements-are-complete-36}. }
\end{equation}
\begin{equation}
\label{eq:edge-elements-are-complete-46}
\myparbox{$\var{edge}[\var{n}+1]$ is in the same Earley set as \Veim{base}
  $\because$
  \Eref{eq:edge-elements-are-complete-20},
  \Eref{eq:edge-elements-are-complete-44}.  }
\end{equation}
The step and the induction follow
from
\Eref{eq:edge-elements-are-complete-37-25},
\Eref{eq:edge-elements-are-complete-42}, and
\Eref{eq:edge-elements-are-complete-46}, so that
\begin{equation}
\label{eq:edge-elements-are-complete-50}
\myparbox{%
for all \var{n} such that
$0 \le \var{n} \le \Vlastix{edge}$,
\VVelement{edge}{n} is a valid completion in the same Earley set as \Veim{base}.%
}
\end{equation}
\begin{equation}
\label{eq:edge-elements-are-complete-52}
\myparbox{\Veimset{edge} is valid $\because$
\Dfref{def:leo-edge},
\Eref{eq:edge-elements-are-complete-50}.}
\end{equation}
We have the theorem directly from
\Eref{eq:edge-elements-are-complete-50} and
\Eref{eq:edge-elements-are-complete-52}.
\end{proof}

\begin{theorem}[Edge Descendant Uniqueness]
\label{th:edge-descendant-uniqueness}
In an \Earley{} parse,
the descendant of a lower element of an edge
is unique.
\end{theorem}

\begin{proof}
We proceed by
assuming that an arbitrary lower element has
two distinct descendants,
for a reductio.
\begin{equation}
\label{th:edge-descendant-uniqueness-08}
\myparbox{Let \Veimset{edge} be an edge WLOG.}
\end{equation}
\begin{equation}
\label{th:edge-descendant-uniqueness-10}
\myparbox{$\Veim{lo}=\VVelement{edge}{n}$ is a valid lower element of an edge
  $\because$ AF theorem, WLOG.}
\end{equation}
\begin{equation}
\label{th:edge-descendant-uniqueness-12}
\myparbox{$\Veim{desc} = \Velement{edge}{\var{n}+1}$
  is a valid direct descendant of \Veim{lo}
  $\because$ AF theorem.}
\end{equation}
\begin{equation}
\label{th:edge-descendant-uniqueness-14}
\myparbox{\Veim{desc2} is a valid direct descendant of \Veim{lo}
$\because$ WLOG.}
\end{equation}
\begin{equation}
\label{th:edge-descendant-uniqueness-16}
\myparbox{$\var{desc} \neq \Veim{desc2}$
$\because$ AF reductio.}
\end{equation}
\begin{equation}
\label{th:edge-descendant-uniqueness-18}
\myparbox{\Veim{lo} is complete
$\because$
\Thref{th:edge-elements-are-complete},
\Eref{th:edge-descendant-uniqueness-10}.}
\end{equation}
\begin{equation}
\label{th:edge-descendant-uniqueness-20}
\myparbox{\Veim{desc} is the result of an \Earley{} completer
operation with link pair
$\left[ \var{pred}, \var{lo} \right]_\type{LP}$
$\because$
\Obref{obs:earley-operation-by-direct-ancestor},
\Eref{th:edge-descendant-uniqueness-12},
\Eref{th:edge-descendant-uniqueness-18}, WLOG.
}
\end{equation}
%
\begin{equation}
\label{th:edge-descendant-uniqueness-21}
\begin{gathered}
\left[ \Veim{pred2}, \var{lo} \right]_\type{LP}
  \in \LinkPairs{\Veim{desc2}} \\
  \land \;\; \Vlim{match} = \var{chain}[\var{n}] \\
  \land \;\; \Veim{pred2} = \Source{\Vlim{match}} \\
  \because \text{Def of ``Leo edge''} \Dfref{def:leo-edge},
    \Eref{th:edge-descendant-uniqueness-12}
\end{gathered}
\end{equation}
%
\begin{equation}
\label{th:edge-descendant-uniqueness-22}
\myparbox{\Veim{desc2} is the result of an \Earley{} completer
operation with link pair
$\left[ \var{pred2}, \var{lo} \right]_\type{LP}$
$\because$
\Obref{obs:earley-operation-by-direct-ancestor},
\Eref{th:edge-descendant-uniqueness-14},
\Eref{th:edge-descendant-uniqueness-18}, WLOG.
}
\end{equation}
%
\begin{equation}
\label{th:edge-descendant-uniqueness-24}
\myparbox{$\Current{\Veim{pred}} = \Origin{\var{lo}}$
  $\because$
  \Eref{eq:earley-completer-summary-1}
  and \Eref{eq:earley-completer-summary-2}
  in \Obref{obs:earley-completer-summary};
  \Eref{th:edge-descendant-uniqueness-20}.
}
\end{equation}
\begin{equation}
\label{th:edge-descendant-uniqueness-26}
\myparbox{$\Current{\Veim{pred2}} = \Origin{\var{lo}}$
  $\because$
  \Eref{eq:earley-completer-summary-1}
  and \Eref{eq:earley-completer-summary-2}
  in \Obref{obs:earley-completer-summary};
  \Eref{th:edge-descendant-uniqueness-22}.}
\end{equation}
\begin{equation}
\label{th:edge-descendant-uniqueness-28}
\myparbox{$\Current{\Veim{pred2}} = \Current{\Veim{pred}}$
  $\because$
  \Eref{th:edge-descendant-uniqueness-24},
  \Eref{th:edge-descendant-uniqueness-26}.}
\end{equation}
%
\begin{equation}
\label{th:edge-descendant-uniqueness-30}
\myparbox{$\Postdot{\var{pred}} = \LHS{\var{lo}}$
  $\because$
  \Eref{eq:earley-completer-summary-3}
  in \Obref{obs:earley-completer-summary};
  \Eref{th:edge-descendant-uniqueness-20}.
}
\end{equation}
\begin{equation}
\label{th:edge-descendant-uniqueness-32}
\myparbox{$\Postdot{\Veim{pred2}} = \LHS{\var{lo}}$
  $\because$
  \Eref{eq:earley-completer-summary-3}
  in \Obref{obs:earley-completer-summary};
  \Eref{th:edge-descendant-uniqueness-22}.}
\end{equation}
\begin{equation}
\label{th:edge-descendant-uniqueness-34}
\myparbox{$\Postdot{\Veim{pred}} = \Postdot{\Veim{pred2}}$
  $\because$
  \Eref{th:edge-descendant-uniqueness-30},
  \Eref{th:edge-descendant-uniqueness-32}.}
\end{equation}
\begin{equation}
\label{th:edge-descendant-uniqueness-36}
\myparbox{\var{pred2} is the source of \Vlim{match}
  $\because$
  \Eref{th:edge-descendant-uniqueness-21}.}
\end{equation}
\begin{equation}
\label{th:edge-descendant-uniqueness-38}
\myparbox{\var{pred2} is a penult
  $\because$
  \Eref{th:edge-descendant-uniqueness-36},
  \Thref{th:leo-chain-source-properties}.}
\end{equation}
\begin{equation}
\label{th:edge-descendant-uniqueness-40}
\myparbox{$\Penult{\Veim{pred2}} = \Postdot{\Veim{pred2}}$
  $\because$
  \Eref{eq:def-penult},
  \Eref{th:edge-descendant-uniqueness-38}.}
\end{equation}
\begin{equation}
\label{th:edge-descendant-uniqueness-41}
\myparbox{$\Penult{\Veim{pred2}} = \Postdot{\Veim{pred}}$
  $\because$
  \Eref{th:edge-descendant-uniqueness-34},
  \Eref{th:edge-descendant-uniqueness-40}.}
\end{equation}
%
\begin{equation}
\label{th:edge-descendant-uniqueness-42}
\begin{gathered}
\Veim{pred2} \in \LeoEligible{\Current{\Vlim{match}}} \\
\because \Eref{th:edge-descendant-uniqueness-36},
\Eref{eq:memoizer-operation-20}, \Eref{eq:memoizer-operation-30}.
\end{gathered}
\end{equation}
\begin{equation}
\label{th:edge-descendant-uniqueness-44}
  \begin{gathered}
    \Veim{pred2} \in \LeoUnique{\Current{\Vlim{match}}} \\
    \because
    \Eref{th:edge-descendant-uniqueness-42}, \Eref{eq:memoizer-operation-10}.
  \end{gathered}
\end{equation}
\begin{equation}
\label{th:edge-descendant-uniqueness-46}
\begin{gathered}
\forall \; \Veim{x}
\left(
\begin{gathered}
\left(
  \begin{gathered}
  \Current{\var{pred2}} = \Current{\var{x}} \\
  \land \; \Penult{\var{pred2}} = \Postdot{\var{x}}
  \end{gathered}
\right)
\\
\implies \var{pred2} = \var{x}.
\end{gathered}
\right)
\\
\because \text{Def of \var{Leo-Unique} \Dfref{def:leo-unique}},
  \Eref{th:edge-descendant-uniqueness-44}.
\end{gathered}
\end{equation}
\begin{equation}
\label{th:edge-descendant-uniqueness-48}
\Veim{pred2} = \Veim{pred} \because
\Eref{th:edge-descendant-uniqueness-28},
\Eref{th:edge-descendant-uniqueness-41},
\Eref{th:edge-descendant-uniqueness-46}.
\end{equation}
\begin{equation}
\label{th:edge-descendant-uniqueness-50}
[\Veim{pred2}, \Veim{lo}] = [\Veim{pred}, \var{lo}] \because
\Eref{th:edge-descendant-uniqueness-48}.
\end{equation}
\begin{equation}
\label{th:edge-descendant-uniqueness-52}
\myparbox{If two \Earley{} completer operations share
a link pair, they produce the same result
$\because$
\Obref{obs:earley-completer-summary}.}
\end{equation}
\begin{equation}
\label{th:edge-descendant-uniqueness-54}
\Veim{desc2} = \Veim{desc} \because
\Eref{th:edge-descendant-uniqueness-20},
\Eref{th:edge-descendant-uniqueness-22},
\Eref{th:edge-descendant-uniqueness-50},
\Eref{th:edge-descendant-uniqueness-52}.
\end{equation}
With
\Eref{th:edge-descendant-uniqueness-52} we
have the reductio of
\Eref{th:edge-descendant-uniqueness-16}
and the theorem.
\end{proof}

\begin{theorem}[Edge Top is Border]
\label{th:edge-top-is-border}
If an EIM is the non-trivial descendant of a lower edge EIM,
it is either another lower edge EIM,
or the descendant of the top of an edge.
\end{theorem}

\begin{proof}
\begin{equation}
\label{th:edge-top-is-border-22}
\myparbox{\Veim{lo} is a lower edge EIM $\because$ AF theorem, WLOG.}
\end{equation}
\begin{equation}
\label{th:edge-top-is-border-24}
\myparbox{\Veim{no} is an EIM which is not a lower edge $\because$ AF theorem, WLOG.}
\end{equation}
For the purposes of this proof, we define \Veimset{seq} as follows:
\begin{equation}
\label{th:edge-top-is-border-26}
\myparbox{%
  \Veim{lo} and \Veim{no} are
  part of a
  \Veimset{seq}, which is such that
  \begin{itemize}
  \item $\Velement{seq}{0} = \Veim{lo}$;
  \item $\Velement{seq}{\Vlastix{seq}} = \Veim{no}$; and
  \item for all \var{i} such that $0 < \var{i} \le \Vlastix{seq}$,
  \eim{(\VVelement{seq}{i})}
  is the direct descendant of \eim{(\Velement{seq}{\var{i}\subtract 1})}.
  \end{itemize}%
}
\end{equation}
Note that the criteria in \Veimset{seq} do not necessarily determine \var{seq}
uniquely.
We make an arbitrary choice among the possible value of \var{seq},
without loss of generality.
\begin{equation}
\label{th:edge-top-is-border-27}
\myparbox{\VVelement{seq}{t} is the first element
of \var{seq}
that is not a lower edge EIM.
$\because$ \Eref{th:edge-top-is-border-26}, WLOG.}
\end{equation}
\begin{equation}
\label{th:edge-top-is-border-28}
\myparbox{\VVelement{seq}{t}
is the top of an edge, or it is not an edge EIM
$\because$
Def of ``Leo edge'' \Dfref{def:leo-edge},
\Eref{th:edge-top-is-border-27}.}
\end{equation}
%
We now consider
\Velement{seq}{\var{t}\subtract 1}.
%
\begin{equation}
\label{th:edge-top-is-border-30}
\myparbox{
\Velement{seq}{\var{t}\subtract 1} is a lower edge EIM
$\because$
\Eref{th:edge-top-is-border-27}.}
\end{equation}
\begin{equation}
\label{th:edge-top-is-border-32}
\myparbox{One of the descendants of a lower edge EIM
must be either another lower edge EIM or the top of
an edge.
$\because$ Def of ``Leo edge'' \Dfref{def:leo-edge}.}
\end{equation}
\begin{equation}
\label{th:edge-top-is-border-34}
\myparbox{%
The descendant of a lower edge EIM
is unique
$\because$ \Thref{th:edge-descendant-uniqueness}.}
\end{equation}
\begin{equation}
\label{th:edge-top-is-border-36}
\myparbox{%
The unique descendant of
\Velement{seq}{\var{t}\subtract 1} is
another lower edge EIM or the top of
an edge
$\because$
\Eref{th:edge-top-is-border-30},
\Eref{th:edge-top-is-border-32},
\Eref{th:edge-top-is-border-34}.}
\end{equation}
\begin{equation}
\label{th:edge-top-is-border-38}
\myparbox{\VVelement{seq}{t} is the unique descendant
of \Velement{seq}{\var{t}\subtract 1}.
$\because$
\Eref{th:edge-top-is-border-30}
\Eref{th:edge-top-is-border-34}.}`
\end{equation}
\begin{equation}
\label{th:edge-top-is-border-40}
\myparbox{\VVelement{seq}{t} is the top of an edge
$\because$
\Eref{th:edge-top-is-border-28},
\Eref{th:edge-top-is-border-36},
\Eref{th:edge-top-is-border-38}.}
\end{equation}
\begin{equation}
\label{th:edge-top-is-border-42}
\myparbox{\Veim{no} is a descendant of
\VVelement{seq}{t}.
$\because$ \Eref{th:edge-top-is-border-26}
}
\end{equation}
\begin{equation}
\label{th:edge-top-is-border-44}
\myparbox{\Veim{no} is a descendant of
the top of an edge.
$\because$ \Eref{th:edge-top-is-border-40}
\Eref{th:edge-top-is-border-42}.}
\end{equation}
Since the choice of \Veim{no} was without loss of generality,
with \Eref{th:edge-top-is-border-44} we have the
theorem.
\end{proof}

\begin{theorem}[Leo edge top]
\label{th:leo-edge-top}
In a \Marpa{} parse,
let \Vlimset{chain} be a valid LIM chain,
and let \Veim{base} be a valid EIM which matches \var{chain}.
Then the top of their edge is
\begin{equation}
\label{eq:th-leo-edge-top-05}
\begin{gathered}
\left[ \DR{\Veim{source}}, \Origin{\var{source}}
\right]@\Current{\var{base}}, \\
\text{where $\Veim{source} = \Source{\Velement{chain}{\Vlastix{chain}}}$.}
\end{gathered}
\end{equation}
\end{theorem}

\begin{proof}
\begin{equation}
\label{eq:th-leo-edge-top-10}
\myparbox{\Velement{edge}{\Vlastix{edge}} is the top of the edge
$\because$ Def of ``Leo edge'' \Dfref{def:leo-edge}.}
\end{equation}
%
\begin{equation}
\label{eq:th-leo-edge-top-12}
\begin{gathered}
\left[ \Source{\Velement{chain}{\Vlastix{edge}\subtract 1}}, \Velement{edge}{\Vlastix{edge}\subtract 1} \right]_\type{LP} \\
  \in \LinkPairs{\Velement{edge}{\Vlastix{edge}}} \\
\because \text{\Eref{def:leo-edge-30} in \Dfref{def:leo-edge}}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-leo-edge-top-13-1}
\begin{gathered}
\left[ \Source{\Velement{chain}{\Vlastix{chain}}}, \Velement{edge}{\Vlastix{chain}} \right]_\type{LP} \\
  \in \LinkPairs{\Velement{edge}{\Vlastix{edge}}} \\
\because \Thref{th:leo-edge-length}, \Eref{eq:th-leo-edge-top-12}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-leo-edge-top-13-2}
\begin{gathered}
\left[ \Veim{source}, \Velement{edge}{\Vlastix{chain}} \right]_\type{LP} \\
  \in \LinkPairs{\Velement{edge}{\Vlastix{edge}}} \\
\because \Eref{eq:th-leo-edge-top-05}, \Eref{eq:th-leo-edge-top-13-1}.
\end{gathered}
\end{equation}
%
\begin{equation}
\label{eq:th-leo-edge-top-14}
\myparbox{\Velement{edge}{\Vlastix{chain}} is a completion
$\because$ \Thref{th:edge-elements-are-complete}.}
\end{equation}
\begin{equation}
\label{eq:th-leo-edge-top-16}
\begin{gathered}
\Current{\Velement{edge}{\Vlastix{chain}}} = \Current{\var{base}} \\
\because \Thref{th:edge-elements-are-complete}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-leo-edge-top-18}
\begin{gathered}
\myparbox{\Velement{edge}{\Vlastix{edge}} is the
result of an \Earley{} completer
operation with link pair
$\left[ \Veim{source}, \Velement{edge}{\Vlastix{chain}} \right]_\type{LP}$%
} \\
\because \Obref{obs:earley-operation-by-direct-ancestor}, \Eref{eq:th-leo-edge-top-13-2},
  \Eref{eq:th-leo-edge-top-14}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-leo-edge-top-20}
\begin{gathered}
\Velement{edge}{\Vlastix{edge}} = \\
\left[ \DR{\var{source}}, \Origin{\var{source}} \right] @ \Current{\Velement{edge}{\Vlastix{chain}}} \\
\because
\Obref{obs:earley-completer-summary},
\Eref{eq:th-leo-edge-top-18}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-leo-edge-top-22}
\begin{gathered}
\Velement{edge}{\Vlastix{edge}} = \\
\left[ \DR{\var{source}}, \Origin{\var{source}} \right] @ \Current{\Veim{base}} \\
\because \Eref{eq:th-leo-edge-top-16}, \Eref{eq:th-leo-edge-top-20}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-leo-edge-top-24}
\myparbox{The top of \Veimset{edge} is
$\left[ \DR{\var{source}}, \Origin{\var{source}} \right] @ \Current{\Veim{base}}$ \\
$\because$ \Eref{eq:th-leo-edge-top-10}, \Eref{eq:th-leo-edge-top-22}.}
\end{equation}
\end{proof}

\begin{theorem}[LIM chain dotted rule and origin]
\label{th:LIM-chain-dr+origin}
In a \Marpa{} parse,
let \Vlimset{chain} be a valid partial LIM chain.
For every element \mylim{\ell} of \var{chain},
\begin{equation}
\label{eq-th-LIM-chain-dr+origin-05}
\begin{gathered}
\text{$\DR{\ell} = \DR{\Source{\Velement{chain}{\Vlastix{chain}}}}$ and} \\
\Origin{\ell} = \Origin{\Source{\Velement{chain}{\Vlastix{chain}}}}
\end{gathered}
\end{equation}
\end{theorem}

\begin{proof}
The proof is by a reductio.
\begin{equation}
\label{eq-th-LIM-chain-dr+origin-10}
\begin{gathered}
\exists \mylim{\ell}, \mylim{\ell} \in \var{chain} : \\
\DR{\ell} = \DR{\Source{\Velement{chain}{\Vlastix{chain}}}} \\
\lor \; \Origin{\ell} = \Origin{\Source{\Velement{chain}{\Vlastix{chain}}}} \\
\because \text{AF reductio.}
\end{gathered}
\end{equation}
\begin{equation}
\label{eq-th-LIM-chain-dr+origin-12}
\myparbox{LIM's are only added by the
\Marpa{} \var{Memoizer} operation
$\because$ \Obref{obs:marpa-operation-by-result}.%
}
\end{equation}
\begin{equation}
\label{eq-th-LIM-chain-dr+origin-14}
\myparbox{LIM's are added by either the
\var{Start-Leo-Chain} or
\var{Extend-Leo-Chain} sub-operation
$\because$
\Eref{eq-th-LIM-chain-dr+origin-12},
\Eref{eq:memoizer-operation-20},
\Eref{eq:memoizer-operation-30}.%
}
\end{equation}
\begin{equation}
\label{eq-th-LIM-chain-dr+origin-15-20}
\myparbox{Of the LIM's added which
satisfy
\Eref{eq-th-LIM-chain-dr+origin-10}
one, call it \Vlim{first} is added first
$\because$
\Eref{eq-th-LIM-chain-dr+origin-14}.%
}
\end{equation}
\begin{equation}
\label{eq-th-LIM-chain-dr+origin-15-25}
\myparbox{\Vlim{first} = \Velement{partial}{0},
where \Vlimset{partial} is the partial Leo chain
of \Vlim{first}
$\because$
\Dfref{def:leo-chain},
\Eref{eq:memoizer-operation-20},
\Eref{eq:memoizer-operation-30}.%
}
\end{equation}
\begin{equation}
\label{eq-th-LIM-chain-dr+origin-15-30}
\Velement{partial}{0} = \Vlim{first}
\end{equation}
We now proceed by subcase, starting with
\var{Start-Leo-Chain}.
\begin{equation}
\label{eq-th-LIM-chain-dr+origin-16}
\myparbox{\Vlim{first} was added by a
\var{Start-Leo-Chain} sub-operation
$\because$ AF subcase.%
}
\end{equation}
\begin{equation}
\label{eq-th-LIM-chain-dr+origin-18}
\LIMPredecessor{\Veim{source}} = \Lambda
\because
\Eref{eq:memoizer-operation-20},
\Eref{eq-th-LIM-chain-dr+origin-16}.
\end{equation}
\begin{equation}
\label{eq-th-LIM-chain-dr+origin-20}
\PreviousLink{\Vlim{first}} = \Lambda
\because
\Eref{eq:memoizer-operation-20},
\Obref{obs:memoizer-ancestry},
\Eref{eq-th-LIM-chain-dr+origin-18}.
\end{equation}
\begin{equation}
\label{eq-th-LIM-chain-dr+origin-22}
\Vsize{partial} = 1
\because
\text{Def of ``Leo chain'' \Dfref{def:leo-chain}},
\Eref{eq-th-LIM-chain-dr+origin-20}.
\end{equation}
\begin{equation}
\label{eq-th-LIM-chain-dr+origin-24}
\Vlastix{partial} = 0 \because
\Eref{eq-th-LIM-chain-dr+origin-22}.
\end{equation}
\begin{equation}
\label{eq-th-LIM-chain-dr+origin-26}
  \begin{gathered}
    \DR{\Vlim{first}} = \DR{\Source{\Velement{partial}{0}}} \\
    \land \; \Origin{\Vlim{first}} = \Origin{\Source{\Velement{partial}{0}}} \\
    \because \Eref{eq:memoizer-operation-20}.
  \end{gathered}
\end{equation}
\TODO{finish proof}
\end{proof}

\begin{theorem}
\label{th:leo-reducer-is-consistent}
The \var{Leo-reducer} sub-operation
of the \Marpa{} \var{Reducer} operation
preserves validity.
\end{theorem}
\begin{proof}
Recall that \var{Leo-reducer}
is defined at
\Eref{eq:def-reducer-23}.
\TODO{prove this}
\end{proof}

\begin{theorem}
\Marpa{} is consistent.
That is, every PIM created by a \Marpa{} parser
is valid.
\end{theorem}

\begin{proof}
We assume for a reductio,
that there is \Marpa{} parse that creates an invalid PIM.
This means that there is a first invalid PIM created
by that \Marpa{} parse.
Inspection of the \Marpa{} algorithm
\Sref{sec:algorithm} shows
that PIM's are only created by operations.

We now proceed by elimination of subcases,
where our subcases are the operations
of the \Marpa{} algorithm.
We first note that an operation creates
the first invalid PIM only if it does not preserve
validity.

The \Marpa{} initializer operation creates the
same PIM's as the \Earley{} initializer operation
\Obref{obs:earley-set-0}.
The \Earley{} initializer operation preserves
validity because the \Earley{} algorithm is
correct
\Thref{th:earley-is-correct}.
Note in the case of the Initalizer operation,
validity is preserved vacuously ---
no PIM is in the ancestry of the Initializer
operation.

The \Marpa{} scanner operation
creates the same PIM's as the \Earley{}
scanner operations
\Obref{obs:earley-scanner-operation}.
The \Marpa{} predictor operation
creates the same PIM's as the \Earley{}
predictor operations
\Obref{obs:earley-predictor-operation}.
These two operations preserve
validity because the \Earley{} algorithm is
correct
\Thref{th:earley-is-correct}.

We have shown that the memoizer operation
preserves validity
\Eref{th:memoizer-consistency}.
The remaining subcase is the \Marpa{} \var{Reducer} operation.

The \Marpa{} \var{Reducer} operation consists of two
sub-operations:
\var{Earley-reducer} and
\var{Leo-reducer}.
We have shown that the
\var{Leo-reducer} preserves validity
\Thref{th:leo-reducer-is-consistent}.

From
\Obref{obs:earley-completer-operation}
and the description of the
\var{Earley-reducer}
\Eref{eq:def-reducer-10},
the \var{Earley-reducer} creates the EIM that
the \Earley{} completer operation would have
created, unless there is a LIM that ``blocks''
\Marpa{}'s \var{Earley-reducer}.
Therefore the PIM's created by
the \Marpa{} \var{Earley-reducer} sub-operation
are a subset of those created by the \Earley{}
completer operation.
We know the \Earley{} completer operation preserves
validity
\Thref{th:earley-completer-is-consistent},
and therefore any operation which creates some subset of the
EIM's that
the \Earley{} completer operation creates also
preserves validity.

With this we have shown the last subcase,
and may conclude that no \Marpa{} operation creates the
first invalid PIM,
and therefore no \Marpa{} operation creates
an invalid PIM.
Therefore, \Marpa{} is consistent.
\end{proof}

\begin{definition}[Hidden and Visible PIM's]
An EIM is \dfn{hidden} if it is an element of the inside
of a Leo edge.
All other PIM's are \dfn{visible}.
\end{definition}

\begin{theorem}
\Marpa{} is complete, except for the hidden
PIM's.
\end{theorem}

\begin{proof}
\TODO{prove this}
\end{proof}

For convenience in the next few lemma's,
we will state some definitions for the purpose of presentation
(DFPP's).
Intuitively
the Earley tables are ``complete as far as X'',
where X is some point in running of the \Marpa{} algorithm,
if they contain all the visible valid PIM's
for X and every prior to it in
in the \Marpa{} algorithm.
\begin{equation}
\myparbox{The Earley tables of a \Marpa{} parse
are ``complete as far as''
Earley set \var{j},
if every Earley set, \Ves{i}, $0 \le \var{i} \le \var{j}$,
contains all the valid visible PIM's
$\because$ DFPP.
}
\end{equation}
\begin{equation}
\myparbox{The Earley tables of a \Marpa{} parse
are ``complete as far as''
the \Marpa{} \var{Scanner} operation of Earley set \var{j},
if they are complete as far as Earley set $\var{j}\subtract 1$,
and if they contain all the visible valid EIM's with a predot terminal
$\because$ DFPP.
}
\end{equation}
\begin{equation}
\myparbox{The Earley tables of a \Marpa{} parse
are ``complete as far as''
the \Marpa{} \var{Reducer} operation of Earley set \var{j},
if they are complete as far as Earley set $\var{j}\subtract 1$,
and if they contain all the visible valid confirmed EIM's
$\because$ DFPP.
}
\end{equation}

Intuitively, ``visible depth''
is depth counted in terms of
visible confirmed EIM causes.
We will write \var{Depth}(\Veim{x}) for the visible depth
of \Veim{x}.
When speaking of several EIM's,
we will say that the one with the
numerically lowest depth value
is the ``deepest''.

\begin{equation}
\label{eq:def-complete-as-far-as-20}
\myparbox{$\var{Depth}(\Veim{x}) = 1$
if $\Predot{\Veim{x}} \in \Term{\var{g}}$
$\because$ DFPP.}
\end{equation}
\begin{equation}
\label{eq:def-complete-as-far-as-22}
\myparbox{$\var{Depth}(\Veim{x}) = \var{Depth}(\Veim{bottom})+1$,
if \var{x} is the top of a Leo edge,
where
\Veim{bottom} is the bottom of the Leo edge
$\because$ DFPP.}
\end{equation}

\begin{equation}
\label{eq:def-complete-as-far-as-24}
\myparbox{$\var{Depth}(\Veim{x}) = \var{Depth}(\Veim{cuz})+1$,
where \Veim{cuz} is the ``deepest'' cause of \Veim{x},
if \var{x} is a visible confirmed EIM where
\var{x} is not the top of a Leo edge and
$\Predot{\var{x}} \in \NT{\var{g}}$
$\because$ DFPP.}
\end{equation}
\begin{equation}
\label{eq:def-complete-as-far-as-26}
\myparbox{$\var{Depth}(\Vpim{pim}) = \Lambda$
if \Vpim{pim} is a LIM,
a hidden EIM,
or a prediction.
In other words,
visible depth is
defined only for visible confirmed EIM's
$\because$ DFPP.
}
\end{equation}

\begin{equation}
\label{eq:def-complete-as-far-as-25}
\begin{gathered}
\forall \; \Veim{x} \; ( \var{Depth}(\var{x}) \ge 1 \lor
 \var{Depth}(\var{x}) = \Lambda) \\
\because
\Eref{eq:def-complete-as-far-as-20},
\Eref{eq:def-complete-as-far-as-22},
\Eref{eq:def-complete-as-far-as-24},
\Eref{eq:def-complete-as-far-as-26}.
\end{gathered}
\end{equation}

\begin{equation}
\label{eq:def-complete-as-far-as-28}
\myparbox{The Earley tables are ``complete as far as''
visible depth \var{n} in Earley set \var{j},
if they are complete as far as Earley set $\var{j}\subtract 1$,
and if they contain all the visible valid EIM's of visible depth
\var{n} or less
$\because$ DFPP.
}
\end{equation}

\begin{lemma}
\label{lem:marpa-reducer-completeness}
Let \Ves{j} be an Earley set in a \Marpa{} parse,
$\var{j} > 0$.
If the parse is complete as far as the
\Marpa{} \var{Scanner} operation of
Earley set \Ves{j},
then the parse is complete as far as the
\Marpa{} \var{Reducer} operation of
Earley set \Ves{j}.
\end{lemma}

\begin{proof}
This proof will be by induction on
visible depth.

We take for the induction hypothesis
\begin{equation}
\label{lem:marpa-reducer-completeness-28}
\myparbox{After a \Marpa{} \var{Reducer} operation at \Ves{j},
\Ves{j} is complete
with respect to the
the EIM's of visible depth \var{ix}.}
\end{equation}

\TODO{finish this proof}
\end{proof}

\begin{lemma}
\label{lem:marpa-visible-depth-completeness}
Let \Ves{j} be an Earley set in a \Marpa{} parse,
$\var{j} > 0$.
If the parse is complete as far as
visible depth \var{n}
in Earley set \Ves{j},
then the parse is complete as far as
visible depth $\var{n}+1$
in
Earley set \Ves{j}.
\end{lemma}
\begin{proof}
The proof is by a reductio.
\TODO{presence in physical Earley set (PES?) vs theoretical {ES?}.}
\begin{equation}
\label{eq:lem-marpa-visible-depth-completeness-08}
\myparbox{Let $\Vpim{rez}@\Ves{j}$ be a valid PIM where
$\var{Depth}(\var{rez}) = \var{n}+1 \land
  \neg \var{rez} \in \Ves{j}$
$\because$ AF reductio.}
\end{equation}
\begin{equation}
\label{eq:lem-marpa-visible-depth-completeness-10}
\myparbox{$\Vpim{rez} = \Veim{rez} \because$ only EIM's have
a visible depth
\Eref{eq:def-complete-as-far-as-26}.}
\end{equation}
\begin{equation}
\label{eq:lem-marpa-visible-depth-completeness-12}
\myparbox{\Veim{rez} is confirmed
$\because$ only confirmed EIM's have a visible depth
\Eref{eq:def-complete-as-far-as-26}.}
\end{equation}
\begin{equation}
\label{eq:lem-marpa-visible-depth-completeness-14}
\begin{gathered}
\var{Depth}(\Veim{rez}) > 1 \because
\Eref{eq:def-complete-as-far-as-25},
\Eref{eq:lem-marpa-visible-depth-completeness-08}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:lem-marpa-visible-depth-completeness-16}
\Predot{\Veim{rez}} \in \NT{\var{g}} \because
\Eref{eq:def-complete-as-far-as-20},
\Eref{eq:lem-marpa-visible-depth-completeness-12},
\Eref{eq:lem-marpa-visible-depth-completeness-14}.
\end{equation}
\TODO{finish proof}
\end{proof}

\TODO{Prove Marpa + Reverse Leo = Original Earley}

\begin{theorem}
In an unambiguous grammar,
in every pair of useful complete EIMs,
one is either contained contained
in the other,
or they are disjoint
\end{theorem}
\begin{proof}
\TODO{Need this proof?}
\end{proof}

\begin{theorem}
\label{th:useful-RR-count}
Let $\var{p} = [ \Vint{g}, \Vstr{w} ]$
be an unambiguous parse,
where $\var{n} = \size{\Vstr{w}}$.
The number of useful complete right recursive EIM's
in \var{p} is less than or equal to $2 \times \var{n} \subtract 1$,
or \On{}.
\end{theorem}

\begin{proof}
\TODO{Need this proof?}
\end{proof}

\section{Reversing Leo memoization}
\label{sec:reverse-leo}

\TODO{Write this section}

\begin{theorem}
The time and space to reverse the Leo memoization
in an unambiguous parse is \On{}.
\end{theorem}

\begin{proof}
\TODO{Prove this}
\end{proof}

\section{LRR complexity}
\label{sec:lrr-complexity}

The following theorem will prove useful.

\begin{theorem}\label{th:es-size}
% TODO: Is this theorem used?
The maximum size of an Earley set at \Vloc{j}
is
\order{\var{j}}.
\end{theorem}

\begin{proof}
Each EIM is of the form
$[ \Vdr{x}, \Vloc{x} ]$.
Recall that
the number of dotted rules is a \Vsize{g},
a constant depending on the grammar.
The possible values of \Vloc{x} range from
0 to \Vloc{j}.
Therefore the number of possible Earley items
in \Ves{j} is
is $\Vloc{j} \times \Oc{}$.
Recall that the size of an Earley set is the number of EIM's
that it contains.
It follows
that the maximum size of the Earley
set at \Vloc{j} is
\begin{equation*}
\Vloc{j} \times \Oc{} = \order{\var{j}} \times \Oc{} = \order{\var{j}}.
\end{equation*}
\end{proof}

\begin{theorem}\label{th:ambiguous-links-pair}
\footnote{This theorem and its proof are adapted from
Lemma 4.6 in
\cite[Vol. 1, p. 325-326]{AU1972}.
}%
Let \Vcfg{g} be a CFG.
If there is an input \Vstr{w} such that a valid
confirmed EIM has more than one link pair,
then \var{g} is ambiguous.
\end{theorem}

\begin{proof}
For the theorem, we assume without loss of generality that
\begin{gather}
\label{eq:ambiguous-links-pair-10a}
\Veim{unioned} = \left[ \left[ \var{lhs} \de \Vstr{rhs1} \Vsym{rhs2} \mydot \Vstr{rhs2} \right], \Vorig{i} \right], \\
\label{eq:ambiguous-links-pair-10b}
\text{$\Veim{unioned} @ \Ves{j}$ is valid, and} \\
\label{eq:ambiguous-links-pair-10d}
\LinkPairs{\Veim{unioned}} \ge 1.
\end{gather}

The proof proceeds by eliminating subcases within a reductio.
We assume for the reductio that
\begin{equation}
\label{eq:ambiguous-links-pair-12}
\text{\var{g} is not ambiguous.}
\end{equation}

Confirmed EIMs are only unioned by the Scanner and the Completer.

\TODO{Finish proof}

\end{proof}

\subsection{Complexity of each Earley item}

% TODO: Is this theorem used?
\begin{theorem}
\label{th:leo-right-recursion}
In a \Marpa{} internal grammar,
either
a right derivation has a step
that uses a right recursive rule,
or it has length is at most \var{c},
where \var{c} is a constant which depends
on the grammar.
\end{theorem}

\begin{proof}
Let \Vint{g} be a \Marpa{} internal grammar.
and let the constant $\var{c} = \NT{\var{g}}$.
Assume, for a reductio, that a right derivation
expands to a
Leo sequence of length
$\var{c}+1$, but that none of its steps uses a right recursive rule.

Because it is of length $\var{c}+1$,
the same symbol must appear twice as the rightmost symbol of
a derivation step.
Since we assumed for the theorem that \var{g}
is a \Marpa{} internal grammar,
there will be no nulling symbols,
and therefore
the rightmost symbol of a string will also be its rightmost
non-nulling symbol.
So part of the rightmost derivation must take the form
\begin{equation}
\label{eq:-th-leo-right-recursion-50}
\Vstr{earlier-prefix} \cat \Vsym{A} \deplus \Vstr{later-prefix} \cat \Vsym{A}.
\end{equation}
But the first step of the derivation in
\Eref{eq:-th-leo-right-recursion-50}
must use a rule of the form
\begin{equation*}
\Vsym{A} \de \Vstr{rhs-prefix} \cat \Vsym{rightmost},
\end{equation*}
where $\Vsym{rightmost} \deplus \Vsym{A}$.
Such a rule is right recursive by definition.
This is contrary to the assumption for the reductio.
We therefore conclude that the length of a right derivation
must be less than or equal to \var{c},
unless at least one step of that derivation uses a right recursive rule.
\end{proof}

\subsection{The complexity results}

\begin{lemma}
\label{lem:handle-unique}
Let \Vint{g} be a LR($\pi$) grammar where $\pi$ is
a left congruence.
Let
\begin{gather}
\label{eq:handle-unique-10a}
\begin{aligned}
& \Accept{\Vint{g}} \xderives{R\ast} \Vstr{preA}\Vsym{lhsA}\Vterm{postA}\Vterm{suffixA} \\
& \qquad \xderives{R} \Vstr{preA}\Vsym{rhsA}\Vterm{postA}\Vterm{suffixA},
\end{aligned}
\\
\label{eq:handle-unique-10b}
\begin{aligned}
& \Accept{\Vint{g}} \xderives{R\ast} \Vstr{preB}\Vsym{lhsB}\Vterm{postB}\Vterm{suffixB} \\
& \qquad \xderives{R} \Vstr{preB}\Vsym{rhsB}\Vterm{postB}\Vterm{suffixB},
\end{aligned}
\\
\label{eq:handle-unique-10c}
\Vstr{preA}\Vsym{rhsA}\Vterm{postA} =
\Vstr{preB}\Vsym{rhsB}\Vterm{postB},
\\
\label{eq:handle-unique-10d}
\text{and} \;\;
\Vterm{suffixA} \equiv \Vterm{suffixB} (\text{mod $\pi$}),
\end{gather}
where
\begin{gather*}
\set{ \Vterm{suffixA}, \Vterm{suffixB},
\Vterm{postA}, \Vterm{postB},
} \subseteq \Term{\var{g}}^\ast
\;\; \\
\text{and $\set{ \Vsym{lhsA}, \Vsym{lhsB} } \in \NT{\var{g}}$}.
\end{gather*}
Then
\begin{gather}
\label{eq:handle-unique-15a}
\Vstr{lhsA} = \Vstr{lhsB},
\\
\label{eq:handle-unique-15b}
\Vstr{preA} = \Vstr{preB},
\\
\label{eq:handle-unique-15c}
\Vstr{rhsA} = \Vstr{rhsB},
\;\; \text{and} \\
\label{eq:handle-unique-15d}
\Vterm{postA} = \Vterm{postB}.
\end{gather}
\end{lemma}

\begin{proof}
Using \Eref{eq:handle-unique-10c}
we may rewrite
\Eref{eq:handle-unique-10b}
to
\begin{equation}
\label{eq:handle-unique-20}
\begin{aligned}
& \Accept{\Vint{g}} \xderives{R\ast} \Vstr{preB}\Vsym{lhsB}\Vterm{postB}\Vterm{suffixB} \\
& \qquad \xderives{R} \Vstr{preA}\Vsym{rhsA}\Vterm{postA}\Vterm{suffixB},
\end{aligned}
\end{equation}
We assumed for the theorem that
\Vint{g} is a left congruence,
so from
\Eref{eq:handle-unique-10d}
we have
\begin{equation}
\label{eq:handle-unique-23}
\Vterm{postA}\Vterm{suffixA} \equiv \Vterm{postA}\Vterm{suffixB} (\text{mod $\pi$}).
\end{equation}
\Vint{g} is LR and from \Thref{def:LR},
\Eref{eq:handle-unique-10a},
\Eref{eq:handle-unique-20}, and
\Eref{eq:handle-unique-23}
we conclude that
\begin{gather}
\label{eq:handle-unique-26a}
\Vsym{lhsA} =
\Vsym{lhsB},
\\
\label{eq:handle-unique-26b}
\Vstr{preA} =
\Vstr{preB}, \;\; \text{and}
\\
\label{eq:handle-unique-26c}
\Vterm{postA}\Vterm{suffixB} =
\Vterm{postB}\Vterm{suffixB}.
\end{gather}
From
\Eref{eq:handle-unique-26c}, we know that
\begin{equation}
\label{eq:handle-unique-30}
\Vterm{postA} = \Vterm{postB}
\end{equation}
and from
\Eref{eq:handle-unique-10c},
\Eref{eq:handle-unique-26b}, and
\Eref{eq:handle-unique-30},
we have
\begin{equation}
\label{eq:handle-unique-33}
\Vstr{rhsA} = \Vstr{rhsB}.
\end{equation}

To show the theorem, we need
\Eref{eq:handle-unique-15a},
\Eref{eq:handle-unique-15b},
\Eref{eq:handle-unique-15c}, and
\Eref{eq:handle-unique-15d}.
We have
\Eref{eq:handle-unique-15a} from \Eref{eq:handle-unique-26a};
\Eref{eq:handle-unique-15b} from \Eref{eq:handle-unique-26b};
\Eref{eq:handle-unique-15c} from \Eref{eq:handle-unique-33};
and
\Eref{eq:handle-unique-15d} from \Eref{eq:handle-unique-30}.
\end{proof}

\begin{lemma}
\label{lem:addendum-lemma1}
This theorem and its proof are adapted from Lemma 1 of \cite{Leo1991a}.
Let
\Vint{g}
be an
$LR(\pi)$
grammar,
where the partition
$\pi$
is a left congruence
of
$\Term{\Vint{g}}^\ast$.
If for some
\[
\Vterm{suffixA}, \Vterm{suffixB} \in \Term{\var{g}}^\ast,
\]
we have
\begin{gather}
\label{eq:left-congruence1}
\begin{aligned}
\Accept{\Vint{g}} & \xderives{R\ast} \Vstr{sfA}\Vterm{suffixA} \\
                  & \destar \Vstr{prefix}\Vterm{suffixA},
\end{aligned}
\\
\label{eq:left-congruence2}
\begin{aligned}
\Accept{\Vint{g}} & \xderives{R\ast} \Vstr{sfB}\Vterm{suffixB} \\
  & \destar \Vstr{prefix}\Vterm{suffixB},
\end{aligned}
\end{gather}
and
\begin{equation}
\label{eq:left-congruence3}
\Vterm{suffixA} \equiv \Vterm{suffixB} \pmod \pi,
\end{equation}
then
\begin{equation}
\label{eq:left-congruence4}
\Vstr{sfA} \xderives{R\ast} \Vstr{sfB}
\;\; \text{or} \;\;
\Vstr{sfB} \xderives{R\ast} \Vstr{sfA}
\end{equation}
\end{lemma}

\begin{proof}
Assume that
\Eref{eq:left-congruence1},
\Eref{eq:left-congruence2}, and
\Eref{eq:left-congruence3} hold.
Expand \Eref{eq:left-congruence1} into
\begin{equation}
\label{eq:left-congruence10a}
\begin{aligned}
& \var{sfseqA}[\var{lenA}]\cat\Vterm{suffixA} = \Vstr{sfA}\cat\Vterm{suffixA}
\\
& \qquad \xderives{R} \var{sfseqA}[\var{lenA}\subtract 1]\cat\Vterm{suffixA} \\
& \qquad \xderives{R} \ldots \\
& \qquad \xderives{R} \var{sfseqA}[1]\cat\Vterm{suffixA}] = \Vstr{prefix}\Vterm{suffixA} \\
\end{aligned}
\end{equation}
and expand \Eref{eq:left-congruence2} into
\begin{equation}
\label{eq:left-congruence10b}
\begin{aligned}
& \var{sfseqB}[\var{lenB}]\cat\Vterm{suffixB} = \Vstr{sfB}\cat\Vterm{suffixB} \\
& \qquad \xderives{R} \var{sfseqB}[\var{lenB}\subtract 1]\cat\Vterm{suffixB} \\
& \qquad \xderives{R} \ldots \\
& \qquad \xderives{R} \var{sfseqB}[1]\cat\Vterm{suffixB} = \Vstr{prefix}\Vterm{suffixB} \\
\end{aligned}
\end{equation}

Let
\begin{equation}
\label{eq:left-congruence10-2}
\var{len} = \min( \var{lenA} ,\var{lenB})
\end{equation}
We seek to show
\begin{equation}
\label{eq:left-congruence11}
\forall \var{k} \; \left( \;
1 \le \var{k} \le \var{len}
\implies
\var{sfseqA}[\var{k}] = \var{sfseqB}[\var{k}]
\; \right).
\end{equation}
We assume, for a reductio, that
\var{j}, $1 \le \var{j} \le \var{len}$,
is the smallest integer such that
\begin{equation}
\label{eq:left-congruence12a}
\var{sfseqA}[\var{j}] \neq \var{sfseqB}[\var{j}]
\end{equation}
so that
\begin{equation}
\label{eq:left-congruence12b}
\forall \var{i} \left( \;
1 \le \var{i} < \var{j}
\implies
\var{sfseqA}[\var{i}] = \var{sfseqB}[\var{i}]
\; \right).
\end{equation}

From \Eref{eq:left-congruence10a}
and \Eref{eq:left-congruence10b} we know that
$\var{sfseqA}[1] = \var{sfseqB}[1] = \Vstr{prefix}$,
so that $\var{j} > 1$.
With this, we can write
for some $\Vterm{postA} \in \Term{\var{g}}^\ast$,
\begin{equation}
\label{eq:left-congruence15a}
\begin{aligned}
& \Accept{\Vint{g}} \xderives{R\ast} \var{sfseqA}[\var{j}]\cat\Vterm{suffixA} \\
& \qquad = \Vstr{preA}\Vsym{lhsA}\Vterm{postA}\Vterm{suffixA} \\
& \qquad \xderives{R} \Vstr{preA}\Vsym{rhsA}\Vterm{postA}\Vterm{suffixA} \\
\end{aligned}
\end{equation}
where
\begin{equation}
\label{eq:left-congruence15a1}
\Vstr{preA}\Vsym{rhsA}\Vterm{postA} =
\var{sfseqA}[\var{j}\subtract 1]
\end{equation}
and similarly, we can write
for some $\Vterm{postB} \in \Term{\var{g}}^\ast$,
\begin{equation}
\label{eq:left-congruence15b}
\begin{aligned}
& \Accept{\Vint{g}} \xderives{R\ast} \var{sfseqB}[\var{j}]\cat\Vterm{suffixB} \\
& \qquad = \Vstr{preB}\Vsym{lhsB}\Vterm{postB}\Vterm{suffixB} \\
& \qquad \xderives{R} \Vstr{preB}\Vsym{rhsB}\Vterm{postB}\Vterm{suffixB} \\
& \qquad = \var{sfseqB}[\var{j}\subtract 1]\cat\Vterm{suffixB}
\end{aligned}
\end{equation}
where
\begin{equation}
\label{eq:left-congruence15b1}
\Vstr{preB}\Vsym{rhsB}\Vterm{postB} =
\var{sfseqB}[\var{j}\subtract 1].
\end{equation}
Then
\begin{gather}
\label{eq:left-congruence18}
\begin{gathered}
\var{sfseqA}[\var{j}\subtract 1] = \var{sfseqB}[\var{j}\subtract 1], \\
\because \Eref{eq:left-congruence12b}, \;\; \text{and}
\end{gathered}
\\[5pt]
\label{eq:left-congruence19}
\begin{gathered}
\Vstr{preA}\Vsym{rhsA}\Vterm{postA} =
\Vstr{preB}\Vsym{rhsB}\Vterm{postB} \\
\because \Eref{eq:left-congruence15a1},
\Eref{eq:left-congruence15b1},
\Eref{eq:left-congruence18}.
\end{gathered}
\end{gather}

Using
\Eref{eq:left-congruence3},
\Eref{eq:left-congruence15a},
\Eref{eq:left-congruence15b},
\Eref{eq:left-congruence19}, and
\Lmref{lem:handle-unique} we
have
\begin{gather}
\label{eq:left-congruence25a}
\Vstr{lhsA} = \Vstr{lhsB}, \\
\label{eq:left-congruence25b}
\Vstr{preA} = \Vstr{preB}, \\
\label{eq:left-congruence25c}
\Vstr{rhsA} = \Vstr{rhsB}, \;\; \text{and} \\
\label{eq:left-congruence25d}
\Vterm{postA} = \Vterm{postB}.
\end{gather}
Therefore
\begin{gather}
\label{eq:left-congruence30a}
\begin{gathered}
\var{sfseqA}[\var{j}] = \Vstr{preA}\Vsym{lhsA}\Vterm{postA} \\
\because \Eref{eq:left-congruence15a},
\end{gathered}
\\
\label{eq:left-congruence30b}
\begin{gathered}
\var{sfseqB}[\var{j}] = \Vstr{preB}\Vsym{lhsB}\Vterm{postB} \\
\because \Eref{eq:left-congruence15b},
\;\; \text{and}
\end{gathered}
\\
\label{eq:left-congruence50}
\begin{gathered}
\var{sfseqA}[\var{j}] = \var{sfseqB}[\var{j}] \\
\because \Eref{eq:left-congruence25a},
\Eref{eq:left-congruence25b},
\Eref{eq:left-congruence25d},
\Eref{eq:left-congruence30a},
\Eref{eq:left-congruence30b}.
\end{gathered}
\end{gather}

\Eref{eq:left-congruence50}
contradicts
\Eref{eq:left-congruence12a},
our assumption for the reductio,
and allows us to conclude that
\begin{equation}
\label{eq:left-congruence52}
\begin{gathered}
\forall \var{j} \left (1 \le \var{j} \le \var{len}
\implies
\var{sfseqA}[\var{j}] = \var{sfseqB}[\var{j}] \right).
\end{gathered}
\end{equation}

Assume, without loss of generality,
\begin{equation}
\label{eq:left-congruence53}
\var{lenA} \ge \var{lenB}.
\end{equation}
Then
\begin{equation}
\label{eq:left-congruence53-2}
\var{lenB} = \var{len} \because
\Eref{eq:left-congruence10-2},
\Eref{eq:left-congruence53},
\end{equation}
\begin{equation}
\label{eq:left-congruence54}
\begin{gathered}
\forall \var{j} \left (1 \le \var{j} < \var{lenB}
\implies
\var{sfseqA}[\var{j}] = \var{sfseqB}[\var{j}] \right) \\
\because \Eref{eq:left-congruence52}, \Eref{eq:left-congruence53-2},
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:left-congruence55a}
\begin{aligned}
& \Vstr{sfA}\Vterm{suffixA} = \var{sfseqA}[\var{lenA}]\Vterm{suffixA} \because
\Eref{eq:left-congruence10a} \\
& \qquad \xderives{R\ast} \var{sfseqA}[\var{lenB}]\Vterm{suffixA}
\because
\Eref{eq:left-congruence10b},
\Eref{eq:left-congruence52},
\Eref{eq:left-congruence53-2},
\\
& \qquad = \var{sfseqB}[\var{lenB}]\Vterm{suffixA}
\because \Eref{eq:left-congruence52}, \Eref{eq:left-congruence53-2}
\\
& \qquad = \Vstr{sfB}\Vterm{suffixA}
\because \Eref{eq:left-congruence10b},
\end{aligned}
\end{equation}
\begin{equation}
\begin{gathered}
\label{eq:left-congruence60}
\Vstr{sfA} \xderives{R\ast} \Vstr{sfB}
\;\; \text{or} \;\;
\Vstr{sfB} \xderives{R\ast} \Vstr{sfA} \\
\because \Eref{eq:left-congruence55a}
\end{gathered}
\end{equation}
and from \Eref{eq:left-congruence60}
we have \Eref{eq:left-congruence4}
and the theorem.

\end{proof}

\begin{lemma}
\label{lem:addendum-lemma2}
\footnote{
This theorem and its proof are adapted from Lemma 2 of \cite{Leo1991a}.}%
Let
\Vint{g}
be an
unambiguous grammar,
and let
\begin{gather*}
[ \Vsym{lhs} \de \Vstr{rhs1}\Vstr{rhs2} ] \in \Rules{\Vcfg{g}} \\
\text{where $\Vstr{rhs2} \neq \epsilon$.}
\end{gather*}
If, for some $\Vterm{input1}, \Vterm{input2}, \Vterm{input3A}, \Vterm{input3B} \in
\Term{\var{g}}^\ast$,
we have
\begin{align}
\label{eq:equal-prefix1}
& \begin{aligned}
\Accept{\Vcfg{g}} & \xderives{R\ast} \Vstr{beforeA}\Vsym{lhs}\Vterm{input3A} \\
                  & \xderives{R} \Vstr{beforeA}\Vstr{rhs1}\Vstr{rhs2}\Vterm{input3A} \\
                  & \xderives{R\ast} \Vstr{beforeA}\Vstr{rhs1}\Vterm{input2}\Vterm{input3A} \\
                  & \xderives{R/ast} \Vterm{input1}\Vterm{input2}\Vterm{input3A} \\
\end{aligned}
\\
\intertext{and}
\label{eq:equal-prefix2}
& \begin{aligned}
\Accept{\Vcfg{g}} & \xderives{R\ast} \Vstr{beforeB}\Vsym{lhs}\Vterm{input3B} \\
                  & \xderives{R} \Vstr{beforeB}\Vstr{rhs1}\Vstr{rhs2}\Vterm{input3B} \\
                  & \xderives{R\ast} \Vstr{beforeB}\Vstr{rhs1}\Vterm{input2}\Vterm{input3B} \\
                  & \xderives{R\ast} \Vterm{input1}\Vterm{input2}\Vterm{input3B} \\
\end{aligned}
\end{align}
and
\begin{equation}
\label{eq:equal-prefix3}
\begin{gathered}
\Vstr{beforeA}\Vsym{lhs} \xderives{R\ast}
\Vstr{beforeB}\Vsym{lhs} \\
\text{or} \;\; \Vstr{beforeB}\Vsym{lhs} \xderives{R\ast}
\Vstr{beforeA}\Vsym{lhs}
\end{gathered}
\end{equation}
then
\begin{equation}
\label{eq:equal-prefix4}
\Vstr{beforeA} = \Vstr{beforeB}
\end{equation}
\end{lemma}

\begin{proof}
We assume, for a reductio, that
\Eref{eq:equal-prefix1},
\Eref{eq:equal-prefix2}, and
\Eref{eq:equal-prefix3}
are all true,
but that
\Eref{eq:equal-prefix4} is false.
By symmetry,
we may assume that
\[
\Vstr{beforeA}\Vsym{lhs} \xderives{R\ast}
\Vstr{beforeB}\Vsym{lhs},
\]
without loss of generality.
If
\begin{equation}
\label{eq:equal-prefix5}
\Vstr{beforeA}\Vsym{lhs} \xderives{(0)}
\Vstr{beforeB}\Vsym{lhs},
\end{equation}
we have
\Vstr{beforeA} = \Vstr{beforeB}
directly from
\Eref{eq:equal-prefix5},
which contradicts our assumption.
For the rest of this proof, therefore,
we assume  that
\begin{equation}
\label{eq:equal-prefix8}
\Vstr{beforeA}\Vsym{lhs} \xderives{R+}
\Vstr{beforeB}\Vsym{lhs}.
\end{equation}

In an unambiguous grammar like \Vint{g},
a right derivation is unique,
so that combining
\Eref{eq:equal-prefix1},
\Eref{eq:equal-prefix2}, and
\Eref{eq:equal-prefix8},
we see that the right derivation of
must take  the form
\begin{align}
\label{eq:equal-prefix10a}
\Accept{\Vcfg{g}} & \xderives{R\ast} \Vstr{beforeA}\Vsym{lhs}\Vterm{input3A} \\
\label{eq:equal-prefix10b}
                  & \xderives{R} \Vstr{beforeA}\Vstr{rhs1}\Vstr{rhs2}\Vterm{input3A} \\
\label{eq:equal-prefix10c}
                  & \xderives{R\ast} \Vstr{beforeB}\Vsym{lhs}\Vterm{input3A} \\
\label{eq:equal-prefix10d}
                  & \xderives{R} \Vstr{beforeB}\Vstr{rhs1}\Vstr{rhs2}\Vterm{input3A} \\
\label{eq:equal-prefix10e}
                  & \xderives{R} \Vstr{beforeB}\Vstr{rhs1}\Vterm{input2}\Vterm{input3A} \\
\label{eq:equal-prefix10f}
                  & \xderives{R\ast} \Vterm{input1}\Vterm{input2}\Vterm{input3A}
\end{align}

Assume, for an inner reductio,
that, in the derviation from \Eref{eq:equal-prefix10b}
to \Eref{eq:equal-prefix10c},
\Vstr{rhs2} does not produce the \Vsym{lhs} of
\Eref{eq:equal-prefix10c}.
Then we must have
\begin{equation}
\label{eq:equal-prefix11}
\Vstr{rhs2} \destar \epsilon.
\end{equation}
But, since \Vint{g} is a \Marpa{} internal grammar,
and there are no nullable symbols in a \Marpa{}  grammar,
we have \Eref{eq:equal-prefix11} only if
\begin{equation}
\label{eq:equal-prefix12}
\Vstr{rhs2} = \epsilon,
\end{equation}
which is contrary to assumption for the theorem.
This shows the inner reductio,
and allows us to write
\begin{equation}
\label{eq:equal-prefix15}
\Vstr{rhs2} \destar \Vstr{shim}\Vsym{lhs}.
\end{equation}

In an unambiguous grammar, a right derivation must be unique,
so we can substitute
\Eref{eq:equal-prefix15} into
\Eref{eq:equal-prefix1},
writing
\begin{align}
\label{eq:equal-prefix20a}
& \Accept{\Vcfg{g}} \xderives{R\ast} \Vstr{beforeA}\Vsym{lhs}\Vterm{input3A}
\because \Eref{eq:equal-prefix10a}
\\
\label{eq:equal-prefix20b}
&
\begin{aligned}
& \xderives{R\ast} \Vstr{beforeA}\Vstr{rhs1} \\
& \qquad \Vstr{rhs2}\Vterm{input3A}
\because \Eref{eq:equal-prefix10b}
\end{aligned}
\\
\label{eq:equal-prefix20c}
&
\begin{aligned}
& \xderives{R\ast} \Vstr{beforeA}\Vstr{rhs1} \\
& \qquad \Vstr{shim}\Vsym{lhs}\Vterm{input3A}
\because \Eref{eq:equal-prefix15}
\end{aligned}
\\
\label{eq:equal-prefix20d}
&
\begin{aligned}
& \xderives{R} \Vstr{beforeA}\Vstr{rhs1} \\
& \qquad \Vstr{shim}\Vstr{rhs1}\Vstr{rhs2}\Vterm{input3A} \\
& \qquad \qquad \because \Eref{eq:equal-prefix10d}
\end{aligned}
\\
\intertext{where \Vstr{beforeA}\Vstr{rhs1}\Vstr{shim} = \Vstr{beforeB}}
\label{eq:equal-prefix20e}
&
\begin{aligned}
& \xderives{R\ast} \Vstr{beforeA}\Vstr{rhs1} \\
& \qquad \Vstr{shim}\Vstr{rhs1}\Vterm{input2}\Vterm{input3A}
  \because \Eref{eq:equal-prefix10e}
\end{aligned}
\\
\label{eq:equal-prefix20f}
&
\begin{aligned}
& \xderives{R\ast} \Vstr{beforeA}\Vstr{rhs1} \\
& \qquad \Vterm{input2}\Vterm{input3A}
\because \Eref{eq:equal-prefix1}.
\end{aligned}
\end{align}

We have the derivation step
\Eref{eq:equal-prefix20e}--\Eref{eq:equal-prefix20f}
because
\Eref{eq:equal-prefix20f}
occurs in \Eref{eq:equal-prefix1}, and
\Eref{eq:equal-prefix20f}
must occur in the derivation after
\Eref{eq:equal-prefix20e}
because, in a right derviation,
any non-terminals in
\Eref{eq:equal-prefix20e} must be expanded
before we reach step
\Eref{eq:equal-prefix20f}.

In fact, comparing \Eref{eq:equal-prefix20e}
to \Eref{eq:equal-prefix20f},
we see that
\begin{equation}
\label{eq:equal-prefix30}
\Vstr{shim}\Vstr{rhs1} \destar \epsilon,
\end{equation}
which in a \Marpa{} internal grammar means the
\begin{equation}
\label{eq:equal-prefix31}
\Vstr{shim}\Vstr{rhs1} = \epsilon.
\end{equation}

We now proceed to show that
\Eref{eq:equal-prefix30} implies that \Vint{g}
has a cycle.
\begin{align}
\label{eq:equal-prefix35a}
& \Vstr{beforeA}\Vstr{rhs1}\Vstr{rhs2}\Vterm{input3A}
\\
\label{eq:equal-prefix35b}
& \begin{aligned}
& \xderives{\ast} \Vstr{beforeA}\Vstr{rhs1} \\
& \qquad \Vstr{shim}\Vstr{rhs1}\Vstr{rhs2}\Vterm{input3A} \\
& \qquad \quad \because \text{derivation \Eref{eq:equal-prefix20b}--\Eref{eq:equal-prefix20d}}
\end{aligned}
\\
\label{eq:equal-prefix35c}
& \begin{aligned}
& = \Vstr{beforeA}\Vstr{rhs1}\Vstr{rhs2}\Vterm{input3A}
  \because \Eref{eq:equal-prefix31}
\end{aligned}
\end{align}
The derivation \Eref{eq:equal-prefix35a}--%
\Eref{eq:equal-prefix35c}
is a cycle, which can only happen if
\Vint{g} is ambiguous.
(Note that
\Eref{eq:equal-prefix35a}--\Eref{eq:equal-prefix35c}
is not and does not need to be a right-derivation.)
It is contrary to assumption for the theorem
for \Vint{g} to be ambiguous,
and this
gives us the reductio
and the theorem.
\end{proof}

\begin{theorem}
\label{th:incompletion-bound}
\footnote{
This theorem and its proof are adapted from
\cite[``Theorem'']{Leo1991a}
and
\cite[Lemma 4.7, p. 173]{Leo1991}.
}%
Let
\Vint{g}
be an LRR
grammar.
There is a constant \var{c} such
that the number of incomplete items
in each set \Ves{j} is at most \var{c}.
\end{theorem}

\begin{proof}
Let
\begin{gather*}
[ \Vsym{lhs} \de \Vstr{rhs1}\Vstr{rhs2} ] \in \Rules{\Vint{g}} \\
\text{where $\Vstr{rhs2} \neq \epsilon$}.
\end{gather*}
Let \Vterm{input1},
\Vterm{input2},
\Vterm{input3A} and
\Vterm{input3B} be elements
of $\Term{\Vint{g}}^\ast$.

Let $\var{j} = \size{\Vterm{input1}}$.
If two distinct derivations both put incomplete EIM's into
\Ves{j}, they have the form
\begin{align}
\label{eq:incompletion-bound-1}
& \begin{aligned}
\Accept{\Vint{g}} & \xderives{R\ast} \Vstr{beforeA}\Vsym{lhs}\Vterm{input3A} \\
                  & \xderives{R} \Vstr{beforeA}\Vstr{rhs1}\Vstr{rhs2}\Vterm{input3A} \\
                  & \xderives{R\ast} \Vstr{beforeA}\Vstr{rhs1}\Vterm{input2}\Vterm{input3A} \\
                  & \destar \Vterm{input1}\Vterm{input2}\Vterm{input3A}
\end{aligned}
\\
\intertext{and}
\label{eq:incompletion-bound-2}
& \begin{aligned}
\Accept{\Vint{g}} & \xderives{R\ast} \Vstr{beforeB}\Vsym{lhs}\Vterm{input3B} \\
                  & \xderives{R} \Vstr{beforeB}\Vstr{rhs1}\Vstr{rhs2}\Vterm{input3B} \\
                  & \xderives{R\ast} \Vstr{beforeB}\Vstr{rhs1}\Vterm{input2}\Vterm{input3B} \\
                  & \destar \Vterm{input1}\Vterm{input2}\Vterm{input3B}.
\end{aligned}
\end{align}

Assume that
\Eref{eq:incompletion-bound-1} and
\Eref{eq:incompletion-bound-2}
are in the same cell of $\pi$,
so that
\begin{equation}
\label{eq:incompletion-bound-3}
\Vterm{input3A} \equiv \Vterm{input3B} ( \text{mod $\pi$} ),
\end{equation}
which, since $\pi$ is a left congruence,
is the same as
\begin{equation}
\label{eq:incompletion-bound-5}
\Vterm{input2}\Vterm{input3A} \equiv \Vterm{input2}\Vterm{input3B} ( \text{mod $\pi$} ),
\end{equation}

Since an LRR grammar is
an LR($\pi$) grammar where
$\pi$ is a left congruence of $\Term{\Vint{g}}^\ast$,
we can use
\Thref{lem:addendum-lemma1}.
We set equivalents between the variables of this
theorem and
\Thref{lem:addendum-lemma1} as follows:

\begin{align}
& \begin{aligned}
& \text{\Vstr{sfA} in \Thref{lem:addendum-lemma1}} \\
& \qquad \qquad = \text{\Vstr{beforeA}\Vsym{lhs} in this proof}
\end{aligned}
\\
& \begin{aligned}
& \text{\Vterm{suffixA} in \Thref{lem:addendum-lemma1}} \\
& \qquad \qquad = \text{\Vterm{input3A} in this proof}
\end{aligned}
\\
& \begin{aligned}
& \text{\Vstr{sfB} in \Thref{lem:addendum-lemma1}} \\
& \qquad \qquad = \text{\Vstr{beforeB}\Vsym{lhs} in this proof}
\end{aligned}
\\
& \begin{aligned}
& \text{\Vterm{suffixB} in \Thref{lem:addendum-lemma1}} \\
& \qquad \qquad = \text{\Vterm{input3B} in this proof}
\end{aligned}
\\
& \begin{aligned}
& \text{\Vterm{prefix} in \Thref{lem:addendum-lemma1}} \\
& \qquad \qquad = \text{\Vterm{input1}\Vterm{input2} in this proof}
\end{aligned}
\end{align}

\Thref{lem:addendum-lemma1} tells us that
\begin{equation}
\label{eq:incompletion-bound-10}
\begin{gathered}
\Vstr{beforeA}\Vsym{lhs} \xderives{R\ast} \Vstr{beforeB}\Vsym{lhs}
\\
\text{or} \;\;
\Vstr{beforeB}\Vsym{lhs} \xderives{R\ast} \Vstr{beforeA}\Vsym{lhs}
\end{gathered}
\end{equation}

From
\Eref{eq:incompletion-bound-1},
\Eref{eq:incompletion-bound-2},
\Eref{eq:incompletion-bound-10}, and
\Thref{lem:addendum-lemma2},
we have
\begin{equation}
\label{eq:incompletion-bound-15}
\Vstr{beforeA} = \Vstr{beforeB}
\end{equation}
From
\Eref{eq:incompletion-bound-1},
\Eref{eq:incompletion-bound-2}, and
\Eref{eq:incompletion-bound-15},
we see that every derivation which unions an incompletion
into \Ves{j} for the cell of $\pi$ containing \Vterm{input3} is
of the form
\begin{align}
\Accept{\Vint{g}} & \xderives{R\ast} \Vstr{beforeA}\Vsym{lhs}\Vterm{input3} \\
                  & \xderives{R} \Vstr{beforeA}\Vstr{rhs1}\Vstr{rhs2}\Vterm{input3} \\
                  & \xderives{R\ast} \Vstr{beforeA}\Vstr{rhs1}\Vterm{input2}\Vterm{input3} \\
                  & \destar \Vterm{input1}\Vterm{input2}\Vterm{input3}
\end{align}
and the incompletion is therefore
\begin{equation}
[ [ \Vsym{lhs} \de \Vstr{rhs1} \mydot \Vstr{rhs2 } ], \Vorig{i} ]
\end{equation}
for some \Vorig{i}.
\Vorig{i} is $\size{\Vterm{factor1}}$
where
\begin{gather*}
\Vterm{input1} = \Vterm{factor1}\Vterm{factor2}, \\
\Vstr{beforeA} \destar \Vterm{factor1}, \;\; \text{and} \\
\Vstr{rhs1} \destar \Vterm{factor2}.
\end{gather*}

% TODO: Justify in lemma that factoring must be unique.
If there were more than one factoring of
\Vterm{input1} into \Vterm{factor1}and \Vterm{factor2},
then \Vint{g} would be ambiguous.
LRR grammars are never ambiguous, so there is only one
$\Vorig{i} = \size{\Vterm{factor1}}$,
and only one
\begin{equation}
[ [ \Vsym{lhs} \de \Vstr{rhs1} \mydot \Vstr{rhs2 } ], \Vorig{i} ]
\end{equation}
for each cell of the partition.
Therefore the maximum number of incompletions at \Ves{j} is
less than
\begin{equation}
\label{eq:incompletion-bound-80}
\var{c} = \size{\Vint{g}} \times \size{\pi},
\end{equation}
where
$\size{\Vint{g}}$ is the number of dotted rules,
and $\size{\pi}$ is the cardinality of the regular partition.
$\size{\Vint{g}}$ is a constant depending on \Vint{g},
and, for any LRR grammar,
$\size{\pi}$ is a constant.
Therefore \var{c} in
\Eref{eq:incompletion-bound-80}
is the constant required by the theorem.
\end{proof}

\begin{theorem}
\label{th:completion-bound}
\footnote{
This theorem and its proof are adapted from
\cite[``Theorem'']{Leo1991a}
and
\cite[Lemma 4.7, p. 173]{Leo1991}.
}%
Let
\Vint{g}
be an unambiguous
grammar.
Let \var{complete} be the number of complete Earley items
in an arbitrary Earley set \Ves{j},
and let \var{medial} be the number of confirmed incomplete Earley items.
Then
\begin{equation*}
\var{complete} = \order{\var{medial}}.
\end{equation*}
\end{theorem}

\begin{proof}
Since \Vcfg{g} is unambiguous, no EIM will have more than one link
pair \Thref{th:unambig-g-unambig-EIM},
and therefore no EIM will have more than one cause EIM.
In this proof,
let a ``cause chain'' be a sequence \var{chain} such that
for every \var{i}, $0 \le \var{i} < \Vsize{chain}\subtract 1$,
$\var{chain}[\var{i}]$ is the cause of the link pair
of $\var{chain}[\var{i}+1]$.

EIM's may not have more than one cause,
but they may share causes.
Call a cause chain where no EIM is the cause of more than one other
EIM, a ``single-cause chain''.  We will now consider the maximum length
of a single-cause chain in a \Marpa{} parse.

In a single-cause chain, every EIM after the first has exactly one link pair
and therefore is Leo-unique.
In any single-chain longer than $\size{\NT{\var{g}}}$
an LHS will occur twice, and therefore the portion of the chain between
the duplicate occurrences will be Leo-unique and right recursive
and therefore Leo-eligible.
Any Leo-eligible stretch of a single-cause chain is memoized and reduced to
two EIM's -- the bottom and the top.
So
\begin{equation}
\label{eq:th-completion-bound-20}
\text{the longest single-cause chain is $\size{\NT{\var{g}}}+1$.}
\end{equation}

We now look at the EIM's in a single Earley set, call it \Ves{j},
seeking to establish the relationship between the count of complete and incomplete
items.
We exclude predictions and organize the rest into a ``cause chain tree''.

We call an EIM of the form,
\begin{equation}
\left[\Accept{\var{g}} \de \Vstr{top}], 0\right]
\end{equation}
the ``accept completion''.
Let the ``leaves'' of the cause tree be the confirmed incompletions,
plus the accept completion, if it exists in \Ves{j}.
These leaves have in common that they cannot be the cause of any other
EIM.
The remaining nodes are the completions, other than the accept completion.
We call these the ``interior'' nodes of the cause tree.
In this proof,
we think of a tree as having its ``leaves'' at the top,
so that the ``interior'' nodes are below the ``leaves''.

For our result, we seek to show the ``worst case''
and therefore to maximize the number of completions.
To do this we maximize the number of interior nodes.
Every leaf will have at most one cause, so we assume every leaf is at the
end of a single-cause chain.
We call the single-cause chains that end in a ``leaf'',
the first layer of the tree.

Below the first layer layer, the bottom of every single-cause chain must share
a cause, because otherwise it would not be the bottom of the single-cause chain.
To maximize the number of interior nodes, we assume that pairs of first-layer bottom
nodes share their causes, and that the causes of the first-layer
bottom notes are in turn the top of a second layer
of single-cause chains.
For the second layer, we again maximize by assuming that pairs of bottom EIM's of the second
layer share causes.

Let \var{leaves} be the number of ``leaves'' in the cause tree,
and letting \var{chains} be the number of single-cause chains in the cause tree,
we can formalize the above reasoning as
\begin{gather}
\label{eq:th-completion-bound-22}
\var{chains} \le \var{leaves} + \frac{\var{leaves}}{2} + \frac{\var{leaves}}{4} + \ldots + 1
\\
\label{eq:th-completion-bound-23}
\var{chains} = 2 \times \var{leaves} \subtract 1 \because \Eref{eq:th-completion-bound-22}.
\end{gather}
Letting \var{interior} be the number of interior nodes in the cause tree,
\begin{gather}
\label{eq:th-completion-bound-25a}
\var{interior} \le \var{chains} \times (\size{\NT{\var{g}}}+1)
\because \Eref{eq:th-completion-bound-20}
\\
\label{eq:th-completion-bound-25b}
\begin{gathered}
\var{interior} \le (2 \times \var{leaves} \subtract 1) \times (\size{\NT{\var{g}}}+1) \\
\because \Eref{eq:th-completion-bound-23},
\Eref{eq:th-completion-bound-25a}.
\end{gathered}
\end{gather}
Noting that
\size{\NT{\var{g}}} is a constant which depends on \var{g},
\Eref{eq:th-completion-bound-25b}
simplifies to
\begin{equation}
\label{eq:th-completion-bound-30}
\var{interior} = \order{\var{leaves}}.
\end{equation}

There may not be an accept completion in \Ves{j},
but to maximize the number of completions,
we assume there is one,
so that
\begin{gather}
\label{eq:th-completion-bound-32a}
\text{$\var{complete} = \var{interior}+1$ and} \\
\label{eq:th-completion-bound-32b}
\var{medial} = \var{leaves} \subtract 1.
\end{gather}
Then
\begin{equation}
\var{complete} = \order{\var{medial}} \because
\Eref{eq:th-completion-bound-30},
\Eref{eq:th-completion-bound-32a},
\Eref{eq:th-completion-bound-32b}.
\end{equation}
and we have the theorem.
\end{proof}

\begin{theorem}
For every LRR grammar,
\Marpa{} runs in $\On{}$ time and space.
\end{theorem}

\begin{proof}
\TODO{Add proof}
\end{proof}

\section{General complexity}
\label{sec:complexity}

\begin{theorem}
For every unambiguous grammar,
\Marpa{} runs in $\order{n^2}$ time and space.
\end{theorem}

\begin{proof}
\TODO{Add proof}
\end{proof}

\begin{theorem}
For every context-free grammar,
\Marpa{} runs in $\order{\var{n}^3}$ time.
\end{theorem}

\begin{proof}
\TODO{Add proof}
\end{proof}

\begin{theorem}\label{th:cfg-space}
For every context-free grammar,
\Marpa{} runs in $\order{\var{n}^2}$ space,
if it does not track links.
\end{theorem}

\begin{proof}
\TODO{Add proof}
\end{proof}

Traditionally only the space result stated for a parsing algorithm
is that
without links, as in \ref{th:cfg-space}.
This is sufficiently relevant
if the parser is only used as a recognizer.
In practice, however,
algorithms like \Marpa{}
are typically used in anticipation
of an evaluation phase,
for which links are necessary.

\begin{theorem}
For every context-free grammar,
\Marpa{} runs in $\order{\var{n}^3}$ space,
including the space for tracking links.
\end{theorem}

\begin{proof}
\TODO{Add proof}
\end{proof}

\section{The \Marpa{} Input Model}
\label{sec:input}

In this paper,
up to this point,
the traditional input stream model
has been assumed.
As implemented,
\Marpa{} generalizes the idea of
input streams beyond the traditional
model.

\Marpa{}'s generalized input model
replaces the input \Cw{}
with a set of tokens,
\var{tokens},
whose elements are triples of symbol,
start location and length:
\begin{equation*}
    [\Vsym{t}, \Vloc{start}, \var{length}]
\end{equation*}
such that
$\var{length} \ge 1$
and
$\Vloc{start} \ge 0$.
The size of the input, \size{\Cw},
is the maximum over
\var{tokens} of $\Vloc{start}+\var{length}$.

Multiple tokens can start at a single location.
(This is how \Marpa{} supports ambiguous tokens.)
The variable-length,
ambiguous and overlapping tokens
of \Marpa{}
bend the conceptual framework of ``parse location''
beyond its breaking point,
and a new term for parse location is needed.
Start and end of tokens are described in terms
of \dfn{earleme} locations,
or simply \dfn{earlemes}.
Token length is also measured in earlemes.

Like standard parse locations, earlemes start at 0,
and run up to \size{\Cw}.
Unlike standard parse locations,
there is not necessarily a token ``at'' any particular earleme.
(A token is considered to be ``at an earleme'' if it ends there,
so that there is never a token ``at'' earleme 0.)
In fact,
there may be earlemes at which no token either starts or ends,
although for the parse to succeed, such an earleme would have to be
properly inside at least one token.
Here ``properly inside'' means after the token's start earleme
and before the token's end earleme.

In the \Marpa{} input stream, tokens
may interweave and overlap freely,
but gaps are not allowed.
That is, for all \Vloc{i} such
that $0 \le \Vloc{i} < \size{\Cw}$,
there must exist
\begin{equation*}
	 \var{token} = [\Vsym{t}, \Vloc{start}, \var{length}]
\end{equation*}
such that
\begin{gather*}
	 \var{token} \in \var{tokens} \quad \text{and} \\
	 \Vloc{start} \le \Vloc{i} < \Vloc{start}+\var{length}.
\end{gather*}

The intent of \Marpa{}'s generalized input model is to allow
users to define alternative input models for special
applications.
An example that arises in current practice is natural
language, features of which are most
naturally expressed with ambiguous tokens.
The traditional input stream can be seen as the special case of
the \Marpa{} input model where
for all \Vsym{x}, \Vsym{y}, \Vloc{x}, \Vloc{y},
\var{xlength}, \var{ylength},
if we have both of
\begin{align*}
    [\Vsym{x}, \Vloc{x}, \var{xlength}] & \in \var{tokens} \quad \text{and} \\
    [\Vsym{y}, \Vloc{y}, \var{ylength}] & \in \var{tokens},
\end{align*}
then we have both of
\begin{gather*}
\var{xlength} = \var{ylength} = 1 \quad \text{and} \\
     \Vloc{x} = \Vloc{y} \implies \Vsym{x} = \Vsym{y}.
\end{gather*}

The correctness results hold for \Marpa{} input streams,
but to preserve the time complexity bounds,
restrictions must be imposed.
In stating them,
let it be understood that
\begin{equation*}
	\Vtoken{[ \Vsym{x}, \Vloc{x}, \var{length} ]} \in \var{tokens}
\end{equation*}
We require that,
for some constant \var{c},
possibly dependent on the grammar \Cg{},
that every token length be less than \var{c},
\begin{equation}
\label{eq:restriction1}
\forall \, \Vtoken{[\Vsym{x}, \Vloc{x}, \var{length}]},
\; \var{length} < \var{c},
\end{equation}
and that
the cardinality of the set of tokens starting at any
one location
be less than \var{c},
\begin{equation}
\label{eq:restriction2}
 \forall \Vloc{i}, \;
 \Bigl|
 \bigl \lbrace
	\Vtoken{[ \Vsym{x}, \Vloc{x}, \var{length} ]} \bigm|
	\Vloc{x} = \Vloc{i}
  \bigr \rbrace
  \Bigr| < \var{c}
\end{equation}
\Eref{eq:restriction1}
and \Eref{eq:restriction2}
impose little or no obstacle
to the practical use
of \Marpa{}'s generalized input model.
And with them,
the complexity results for \Marpa{} stand.

\section{Acknowledgments}
\label{sec:Acknowledgments}

Ruslan Shvedov
and
Ruslan Zakirov
made many useful suggestions
that are incorporated in this paper.
Many members of the \Marpa{} community
have helped me in many ways,
and it is risky
to single out one of them.
But Ron Savage
has been unstinting in
his support.

\bibliographystyle{plain}

\begin{thebibliography}{10}

\bibitem{AU1972}
Alfred H.~Aho and Jeffrey D.~Ullman.
\newblock {\em The Theory of Parsing, Translation, and Computing}.
\newblock Prentice-Hall, Englewood Cliff, N.J., 1972.

\bibitem{AH2002}
John~Aycock and R.~Nigel~Horspool.
\newblock Practical Earley Parsing.
\newblock {\em The Computer Journal},
    Vol. 45, No. 6, 2002, pp. 620-630.

\bibitem{Brabrand2007}
Claus~Brabrand, Robert~Giegerich and Anders~M{\"o}ller.
\newblock Analyzing ambiguity of context-free grammars.
\newblock {\em Proc. 12th International Conference
on Implementation and Application of Automata}.
\newblock CIAA ’07, Prague, Czech Republic, July 2007.
\newblock \url{http://www.brics.dk/~amoeller/papers/ambiguity/journal.pdf}

\bibitem{Culik1973}
Karel~{\v{C}}ulik, and Rina~Cohen.
\newblock LR-Regular grammars --- an extension of {LR($k$)} grammars.
\newblock {\em Journal of Computer and System Sciences},
  Vol. 7, No. 1, 1973,
  pp. 66--96.

\bibitem{Earley1968}
J.~Earley.
\newblock An Efficient Context-Free Parsing Algorithm.
\newblock Ph.D. Thesis, Carnegie Mellon University, 1968

\bibitem{Earley1970}
J.~Earley.
\newblock An efficient context-free parsing algorithm.
\newblock {\em Communications of the Association for Computing Machinery},
  13(2):94--102, 1970.

\bibitem{GJ2008}
Dirk~Grune and Ceriel~J.H Jacobs.
\newblock {\em Parsing Techniques: A Practical Guide}.
\newblock Springer, Amsterdam, 2008.

\bibitem{Harrison1978}
Michael~A.~Harrison.
\newblock {\em Introduction to Formal Language Theory}.
\newblock Addison-Wesley, 1978.

\bibitem{Irons}
Edgar~T.~Irons.
\newblock A syntax-directed compiler for ALGOL 60.
\newblock {\em Communications of the Association for Computing Machinery},
 4(1):51-55, Jan. 1961

\bibitem{Johnson}
Stephen~C. Johnson.
\newblock Yacc: Yet another compiler-compiler.
\newblock In {\em Unix Programmer's Manual Supplementary Documents 1}. 1986.

\bibitem{Kegler2019}
Jeffrey~Kegler.
\newblock Marpa, A practical general parser: the recognizer.
\newblock arXiv:1910.08129v1
\newblock \url{https://arxiv.org/abs/1910.08129v1}

\bibitem{Marpa-HTML}
Jeffrey~Kegler, 2011: Marpa-HTML.
\newblock \url{http://search.cpan.org/dist/Marpa-HTML/}.

\bibitem{Marpa-R2}
Jeffrey~Kegler, 2013: Marpa-R2.
\newblock \url{http://search.cpan.org/dist/Marpa-R2/}.

\bibitem{Marpa-XS}
Jeffrey~Kegler, 2011: Marpa-XS-1.002000.
\newblock \url{http://search.cpan.org/dist/Marpa-XS/}.

\bibitem{Timeline}
Jeffrey~Kegler.
\newblock Parsing:~a~timeline.
\newblock Version 3.1, 2019.
\newblock \url{https://jeffreykegler.github.io/personal/timeline_v3}.

\bibitem{Leo1991}
J.~M. I.~M. Leo.
\newblock A general context-free parsing algorithm running in linear time on
  every {LR($k$)} grammar without using lookahead.
\newblock {\em Theoretical Computer Science}, 82:165--176, 1991.

\bibitem{Leo1991a}
J.~M. I.~M. Leo.
\newblock Addendum.
\newblock An undated 2-page addendum to \cite{Leo1991}.

\end{thebibliography}

\clearpage
\tableofcontents

\end{document}
